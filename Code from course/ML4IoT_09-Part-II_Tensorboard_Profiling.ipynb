{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard Profiler\n",
    "\n",
    "Source: https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras\n",
    "\n",
    "In this notebook, we'll see how we can use TensorBoard to profile a training (or inference) run and optimize it for performance.\n",
    "\n",
    "Let' start by clearing the log directory, adding the TB extension, and loading the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-22 12:14:28.280069: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-22 12:14:28.280091: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Clear any logs from previous runs\n",
    "!rm -rf ./tb_log/ \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download the dataset\n",
    "\n",
    "Download the MNIST Dataset. Note that, this time, we'll use TF datasets (not Keras') because it allows us to show some more interesting stuff in the TensorBoard profiler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow_datasets\n",
      "  Downloading tensorflow_datasets-4.4.0-py3-none-any.whl (4.0 MB)\n",
      "     |████████████████████████████████| 4.0 MB 992 kB/s            \n",
      "\u001b[?25hCollecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-1.5.0-py3-none-any.whl (48 kB)\n",
      "     |████████████████████████████████| 48 kB 682 kB/s            \n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /home/matteo/.local/lib/python3.8/site-packages (from tensorflow_datasets) (2.26.0)\n",
      "Requirement already satisfied: tqdm in /home/matteo/.local/lib/python3.8/site-packages (from tensorflow_datasets) (4.62.3)\n",
      "Requirement already satisfied: absl-py in /home/matteo/.local/lib/python3.8/site-packages (from tensorflow_datasets) (1.0.0)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in /home/matteo/.local/lib/python3.8/site-packages (from tensorflow_datasets) (3.19.1)\n",
      "Requirement already satisfied: termcolor in /home/matteo/.local/lib/python3.8/site-packages (from tensorflow_datasets) (1.1.0)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /home/matteo/.local/lib/python3.8/site-packages (from tensorflow_datasets) (21.2.0)\n",
      "Requirement already satisfied: six in /home/matteo/.local/lib/python3.8/site-packages (from tensorflow_datasets) (1.16.0)\n",
      "Collecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: future in /usr/lib/python3/dist-packages (from tensorflow_datasets) (0.18.2)\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "     |████████████████████████████████| 86 kB 454 kB/s            \n",
      "\u001b[?25hRequirement already satisfied: importlib-resources in /home/matteo/.local/lib/python3.8/site-packages (from tensorflow_datasets) (5.4.0)\n",
      "Requirement already satisfied: numpy in /home/matteo/.local/lib/python3.8/site-packages (from tensorflow_datasets) (1.21.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/matteo/.local/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/matteo/.local/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/matteo/.local/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/matteo/.local/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets) (2.0.9)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/matteo/.local/lib/python3.8/site-packages (from importlib-resources->tensorflow_datasets) (3.6.0)\n",
      "Collecting googleapis-common-protos<2,>=1.52.0\n",
      "  Downloading googleapis_common_protos-1.54.0-py2.py3-none-any.whl (207 kB)\n",
      "     |████████████████████████████████| 207 kB 548 kB/s            \n",
      "\u001b[?25hCollecting absl-py\n",
      "  Downloading absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
      "     |████████████████████████████████| 129 kB 518 kB/s            \n",
      "\u001b[?25hBuilding wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21502 sha256=492c3b2f2d0756b14e3dba67f37dfd67986933486a42dc8a43624ec7c1ae9886\n",
      "  Stored in directory: /home/matteo/.cache/pip/wheels/54/aa/01/724885182f93150035a2a91bce34a12877e8067a97baaf5dc8\n",
      "Successfully built promise\n",
      "Installing collected packages: googleapis-common-protos, absl-py, tensorflow-metadata, promise, dill, tensorflow-datasets\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 1.0.0\n",
      "    Uninstalling absl-py-1.0.0:\n",
      "      Successfully uninstalled absl-py-1.0.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.7.0 requires gast<0.5.0,>=0.2.1, but you have gast 0.5.3 which is incompatible.\u001b[0m\n",
      "Successfully installed absl-py-0.12.0 dill-0.3.4 googleapis-common-protos-1.54.0 promise-2.3 tensorflow-datasets-4.4.0 tensorflow-metadata-1.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-22 12:14:41.029374: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 11.06 MiB (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /home/matteo/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d9def8a3daf4624b1309ab10a76b3dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...:   0%|          | 0/4 [00:00<?, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset mnist downloaded and prepared to /home/matteo/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-22 12:15:20.946912: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-12-22 12:15:20.946938: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-12-22 12:15:20.946958: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (matteo-Inspiron-7591-2n1): /proc/driver/nvidia/version does not exist\n",
      "2021-12-22 12:15:20.947214: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Equivalent in keras\n",
    "# mnist = keras.datasets.mnist\n",
    "# (x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "# x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n",
    "\n",
    "def normalize_img(image, label):\n",
    "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "  return tf.cast(image, tf.float32) / 255., label\n",
    "\n",
    "\n",
    "ds_train = ds_train.map(normalize_img)\n",
    "ds_train = ds_train.batch(128)\n",
    "ds_test = ds_test.map(normalize_img)\n",
    "ds_test = ds_test.batch(128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build the Model\n",
    "\n",
    "Create a simple two-layer fully-connected DNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "  keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
    "  keras.layers.Dense(128,activation='relu'),\n",
    "  keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train the Model\n",
    "\n",
    "Create a TensorBoard callback with the `profile_batch` option. In this case, let us profile batches from 500 to 520.\n",
    "\n",
    "Then, train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-22 12:15:22.261812: I tensorflow/core/profiler/lib/profiler_session.cc:110] Profiler session initializing.\n",
      "2021-12-22 12:15:22.261899: I tensorflow/core/profiler/lib/profiler_session.cc:125] Profiler session started.\n",
      "2021-12-22 12:15:22.266503: I tensorflow/core/profiler/lib/profiler_session.cc:143] Profiler session tear down.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.3590 - accuracy: 0.8996 - val_loss: 0.1948 - val_accuracy: 0.9451\n",
      "Epoch 2/5\n",
      " 42/469 [=>............................] - ETA: 1s - loss: 0.1993 - accuracy: 0.9412"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-22 12:15:24.704117: I tensorflow/core/profiler/lib/profiler_session.cc:110] Profiler session initializing.\n",
      "2021-12-22 12:15:24.704150: I tensorflow/core/profiler/lib/profiler_session.cc:125] Profiler session started.\n",
      "2021-12-22 12:15:24.757959: I tensorflow/core/profiler/lib/profiler_session.cc:67] Profiler session collecting data.\n",
      "2021-12-22 12:15:24.817609: I tensorflow/core/profiler/lib/profiler_session.cc:143] Profiler session tear down.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111/469 [======>.......................] - ETA: 1s - loss: 0.1920 - accuracy: 0.9460"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-22 12:15:24.917148: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./tb_log/plugins/profile/2021_12_22_12_15_24\n",
      "\n",
      "2021-12-22 12:15:24.967400: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to ./tb_log/plugins/profile/2021_12_22_12_15_24/matteo-Inspiron-7591-2n1.trace.json.gz\n",
      "2021-12-22 12:15:24.997290: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./tb_log/plugins/profile/2021_12_22_12_15_24\n",
      "\n",
      "2021-12-22 12:15:24.997421: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to ./tb_log/plugins/profile/2021_12_22_12_15_24/matteo-Inspiron-7591-2n1.memory_profile.json.gz\n",
      "2021-12-22 12:15:24.998105: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: ./tb_log/plugins/profile/2021_12_22_12_15_24\n",
      "Dumped tool data for xplane.pb to ./tb_log/plugins/profile/2021_12_22_12_15_24/matteo-Inspiron-7591-2n1.xplane.pb\n",
      "Dumped tool data for overview_page.pb to ./tb_log/plugins/profile/2021_12_22_12_15_24/matteo-Inspiron-7591-2n1.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to ./tb_log/plugins/profile/2021_12_22_12_15_24/matteo-Inspiron-7591-2n1.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to ./tb_log/plugins/profile/2021_12_22_12_15_24/matteo-Inspiron-7591-2n1.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to ./tb_log/plugins/profile/2021_12_22_12_15_24/matteo-Inspiron-7591-2n1.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1679 - accuracy: 0.9520 - val_loss: 0.1389 - val_accuracy: 0.9599\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1203 - accuracy: 0.9654 - val_loss: 0.1133 - val_accuracy: 0.9669\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0927 - accuracy: 0.9736 - val_loss: 0.0988 - val_accuracy: 0.9711\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0744 - accuracy: 0.9788 - val_loss: 0.0895 - val_accuracy: 0.9734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f03520a4220>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs = \"./tb_log\"\n",
    "\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(log_dir = logs, histogram_freq = 1, profile_batch = '500,520')\n",
    "\n",
    "# using test data for validation just for simplicity\n",
    "model.fit(ds_train, epochs=5, validation_data=ds_test, callbacks = [tb_callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Examine Profiling Results\n",
    "\n",
    "Open TensorBoard (in the notebook or from the command line) and examine the `PROFILE` tab from the dropdown menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1cf59a775dec861\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1cf59a775dec861\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=\"./tb_log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimize for Performance\n",
    "\n",
    "Optimize the input pipeline to speed-up the processing. In particular, cache and prefetch the data to avoid computation stalls (see dataset API lecture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n",
    "\n",
    "ds_train = ds_train.map(normalize_img)\n",
    "ds_train = ds_train.batch(128)\n",
    "ds_train = ds_train.cache()\n",
    "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "ds_test = ds_test.map(normalize_img)\n",
    "ds_test = ds_test.batch(128)\n",
    "ds_test = ds_test.cache()\n",
    "ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train the Model (v2)\n",
    "\n",
    "Train again the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0611 - accuracy: 0.9827 - val_loss: 0.0842 - val_accuracy: 0.9747\n",
      "Epoch 2/5\n",
      "131/469 [=======>......................] - ETA: 0s - loss: 0.0538 - accuracy: 0.9853"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-22 12:15:34.574677: I tensorflow/core/profiler/lib/profiler_session.cc:110] Profiler session initializing.\n",
      "2021-12-22 12:15:34.574709: I tensorflow/core/profiler/lib/profiler_session.cc:125] Profiler session started.\n",
      "2021-12-22 12:15:34.606636: I tensorflow/core/profiler/lib/profiler_session.cc:67] Profiler session collecting data.\n",
      "2021-12-22 12:15:34.609227: I tensorflow/core/profiler/lib/profiler_session.cc:143] Profiler session tear down.\n",
      "2021-12-22 12:15:34.615203: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./tb_log/plugins/profile/2021_12_22_12_15_34\n",
      "\n",
      "2021-12-22 12:15:34.619759: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to ./tb_log/plugins/profile/2021_12_22_12_15_34/matteo-Inspiron-7591-2n1.trace.json.gz\n",
      "2021-12-22 12:15:34.622695: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./tb_log/plugins/profile/2021_12_22_12_15_34\n",
      "\n",
      "2021-12-22 12:15:34.622814: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to ./tb_log/plugins/profile/2021_12_22_12_15_34/matteo-Inspiron-7591-2n1.memory_profile.json.gz\n",
      "2021-12-22 12:15:34.623262: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: ./tb_log/plugins/profile/2021_12_22_12_15_34\n",
      "Dumped tool data for xplane.pb to ./tb_log/plugins/profile/2021_12_22_12_15_34/matteo-Inspiron-7591-2n1.xplane.pb\n",
      "Dumped tool data for overview_page.pb to ./tb_log/plugins/profile/2021_12_22_12_15_34/matteo-Inspiron-7591-2n1.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to ./tb_log/plugins/profile/2021_12_22_12_15_34/matteo-Inspiron-7591-2n1.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to ./tb_log/plugins/profile/2021_12_22_12_15_34/matteo-Inspiron-7591-2n1.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to ./tb_log/plugins/profile/2021_12_22_12_15_34/matteo-Inspiron-7591-2n1.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0509 - accuracy: 0.9859 - val_loss: 0.0813 - val_accuracy: 0.9753\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0426 - accuracy: 0.9886 - val_loss: 0.0791 - val_accuracy: 0.9754\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0359 - accuracy: 0.9906 - val_loss: 0.0786 - val_accuracy: 0.9754\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0302 - accuracy: 0.9928 - val_loss: 0.0785 - val_accuracy: 0.9758\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f03526cc160>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(ds_train, epochs=5, validation_data=ds_test, callbacks = [tb_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check TensorBoard again and compare the two runs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
