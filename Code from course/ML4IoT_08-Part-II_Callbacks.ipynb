{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Callbacks in Keras\n",
    "\n",
    "In this notebook, we well see how to use pre-defined and custom callbacks in Keras for tasks such as chekpointing, learning rate scheduling, etc.\n",
    "\n",
    "We'll use the same simple dataset and linear model of the previous notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-22 12:16:08.763485: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-22 12:16:08.763507: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download the Auto-MPG dataset\n",
    " \n",
    "Let's use the Auto-MPG dataset (seen in a previous notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MPG</th>\n",
       "      <th>Cylinders</th>\n",
       "      <th>Displacement</th>\n",
       "      <th>Horsepower</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Acceleration</th>\n",
       "      <th>Model Year</th>\n",
       "      <th>Europe</th>\n",
       "      <th>Japan</th>\n",
       "      <th>USA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>27.0</td>\n",
       "      <td>4</td>\n",
       "      <td>140.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>2790.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>44.0</td>\n",
       "      <td>4</td>\n",
       "      <td>97.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2130.0</td>\n",
       "      <td>24.6</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>32.0</td>\n",
       "      <td>4</td>\n",
       "      <td>135.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>2295.0</td>\n",
       "      <td>11.6</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>28.0</td>\n",
       "      <td>4</td>\n",
       "      <td>120.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>2625.0</td>\n",
       "      <td>18.6</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>31.0</td>\n",
       "      <td>4</td>\n",
       "      <td>119.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2720.0</td>\n",
       "      <td>19.4</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      MPG  Cylinders  Displacement  Horsepower  Weight  Acceleration  \\\n",
       "393  27.0          4         140.0        86.0  2790.0          15.6   \n",
       "394  44.0          4          97.0        52.0  2130.0          24.6   \n",
       "395  32.0          4         135.0        84.0  2295.0          11.6   \n",
       "396  28.0          4         120.0        79.0  2625.0          18.6   \n",
       "397  31.0          4         119.0        82.0  2720.0          19.4   \n",
       "\n",
       "     Model Year  Europe  Japan  USA  \n",
       "393          82       0      0    1  \n",
       "394          82       1      0    0  \n",
       "395          82       0      0    1  \n",
       "396          82       0      0    1  \n",
       "397          82       0      0    1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin']\n",
    "\n",
    "dataset = pd.read_csv(url, names=column_names, na_values='?', comment='\\t', sep=' ', skipinitialspace=True)\n",
    "dataset = dataset.dropna()\n",
    "dataset['Origin'] = dataset['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})\n",
    "dataset = pd.get_dummies(dataset, prefix='', prefix_sep='')\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset.sample(frac=0.8, random_state=0)\n",
    "test_dataset = dataset.drop(train_dataset.index)\n",
    "\n",
    "train_features = train_dataset.copy()\n",
    "test_features = test_dataset.copy()\n",
    "\n",
    "train_labels = train_features.pop('MPG')\n",
    "test_labels = test_features.pop('MPG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build the model\n",
    "\n",
    "Let's build a simple linear regression model (seen in a previous notebook) to test different callbacks during its training.\n",
    "\n",
    "We use a `get_model()` function so that we can re-create the model from scratch multiple times with a single instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    normalizer = preprocessing.Normalization(input_shape=[9,])\n",
    "    normalizer.adapt(np.array(train_features))\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        normalizer,\n",
    "        layers.Dense(units=1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.optimizers.Adam(learning_rate=0.1),\n",
    "        loss='mse', metrics=['mae', 'mse']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping callback\n",
    "\n",
    "Let's use the early stopping callback to stop training when it reaches stability.\n",
    "\n",
    "The `monitor` parameter specifies the loss/metric to be monitored, and the `patience` parameters specifies the number of non-improving epochs to wait before stopping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-22 12:16:12.406346: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-12-22 12:16:12.406370: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-12-22 12:16:12.406388: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (matteo-Inspiron-7591-2n1): /proc/driver/nvidia/version does not exist\n",
      "2021-12-22 12:16:12.406572: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 585.2602 - mae: 22.9254 - mse: 585.2602 - val_loss: 559.5980 - val_mae: 22.9405 - val_mse: 559.5980\n",
      "Epoch 2/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 517.0109 - mae: 22.1718 - mse: 517.0109 - val_loss: 499.8539 - val_mae: 22.0346 - val_mse: 499.8539\n",
      "Epoch 3/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 471.5683 - mae: 21.3457 - mse: 471.5683 - val_loss: 464.2890 - val_mae: 21.2402 - val_mse: 464.2890\n",
      "Epoch 4/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 436.6931 - mae: 20.5261 - mse: 436.6931 - val_loss: 433.9763 - val_mae: 20.5267 - val_mse: 433.9763\n",
      "Epoch 5/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 404.5422 - mae: 19.7431 - mse: 404.5422 - val_loss: 405.7402 - val_mae: 19.8354 - val_mse: 405.7402\n",
      "Epoch 6/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 374.8262 - mae: 18.9781 - mse: 374.8262 - val_loss: 378.3346 - val_mae: 19.1343 - val_mse: 378.3346\n",
      "Epoch 7/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 347.2332 - mae: 18.2417 - mse: 347.2332 - val_loss: 353.5428 - val_mae: 18.4597 - val_mse: 353.5428\n",
      "Epoch 8/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 319.9728 - mae: 17.4741 - mse: 319.9728 - val_loss: 328.1574 - val_mae: 17.7658 - val_mse: 328.1574\n",
      "Epoch 9/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 294.9034 - mae: 16.7589 - mse: 294.9034 - val_loss: 303.6608 - val_mae: 17.0679 - val_mse: 303.6608\n",
      "Epoch 10/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 271.9877 - mae: 16.0607 - mse: 271.9877 - val_loss: 281.2690 - val_mae: 16.3984 - val_mse: 281.2690\n",
      "Epoch 11/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 250.5478 - mae: 15.3814 - mse: 250.5478 - val_loss: 259.8031 - val_mae: 15.7357 - val_mse: 259.8031\n",
      "Epoch 12/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 230.2511 - mae: 14.7148 - mse: 230.2511 - val_loss: 239.8384 - val_mae: 15.0880 - val_mse: 239.8384\n",
      "Epoch 13/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 211.4930 - mae: 14.0720 - mse: 211.4930 - val_loss: 221.3526 - val_mae: 14.4636 - val_mse: 221.3526\n",
      "Epoch 14/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 194.1614 - mae: 13.4437 - mse: 194.1614 - val_loss: 204.2780 - val_mae: 13.8542 - val_mse: 204.2780\n",
      "Epoch 15/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 178.1135 - mae: 12.8288 - mse: 178.1135 - val_loss: 188.8523 - val_mae: 13.2683 - val_mse: 188.8523\n",
      "Epoch 16/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 163.0119 - mae: 12.2343 - mse: 163.0119 - val_loss: 172.8778 - val_mae: 12.6720 - val_mse: 172.8778\n",
      "Epoch 17/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 149.0927 - mae: 11.6529 - mse: 149.0927 - val_loss: 157.6394 - val_mae: 12.0749 - val_mse: 157.6394\n",
      "Epoch 18/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 136.1806 - mae: 11.1010 - mse: 136.1806 - val_loss: 144.7830 - val_mae: 11.5281 - val_mse: 144.7830\n",
      "Epoch 19/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 124.3579 - mae: 10.5636 - mse: 124.3579 - val_loss: 132.5596 - val_mae: 10.9899 - val_mse: 132.5596\n",
      "Epoch 20/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 113.5468 - mae: 10.0449 - mse: 113.5468 - val_loss: 122.1541 - val_mae: 10.4912 - val_mse: 122.1541\n",
      "Epoch 21/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 103.5051 - mae: 9.5317 - mse: 103.5051 - val_loss: 111.3173 - val_mae: 9.9754 - val_mse: 111.3173\n",
      "Epoch 22/200\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 94.1475 - mae: 9.0460 - mse: 94.1475 - val_loss: 101.9210 - val_mae: 9.4944 - val_mse: 101.9210\n",
      "Epoch 23/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 85.8573 - mae: 8.5849 - mse: 85.8573 - val_loss: 92.6046 - val_mae: 9.0078 - val_mse: 92.6046\n",
      "Epoch 24/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 78.1988 - mae: 8.1382 - mse: 78.1988 - val_loss: 84.7184 - val_mae: 8.5578 - val_mse: 84.7184\n",
      "Epoch 25/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 71.3418 - mae: 7.7023 - mse: 71.3418 - val_loss: 78.0568 - val_mae: 8.1389 - val_mse: 78.0568\n",
      "Epoch 26/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 64.8469 - mae: 7.2788 - mse: 64.8469 - val_loss: 70.6086 - val_mae: 7.6974 - val_mse: 70.6086\n",
      "Epoch 27/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 58.8874 - mae: 6.8873 - mse: 58.8874 - val_loss: 64.8351 - val_mae: 7.3187 - val_mse: 64.8351\n",
      "Epoch 28/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 53.7405 - mae: 6.5145 - mse: 53.7405 - val_loss: 59.0810 - val_mae: 6.9360 - val_mse: 59.0810\n",
      "Epoch 29/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 48.9887 - mae: 6.1611 - mse: 48.9887 - val_loss: 54.0394 - val_mae: 6.5748 - val_mse: 54.0394\n",
      "Epoch 30/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 44.7578 - mae: 5.8195 - mse: 44.7578 - val_loss: 49.4527 - val_mae: 6.2318 - val_mse: 49.4527\n",
      "Epoch 31/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 40.8577 - mae: 5.5071 - mse: 40.8577 - val_loss: 45.1462 - val_mae: 5.8963 - val_mse: 45.1462\n",
      "Epoch 32/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 37.5017 - mae: 5.2147 - mse: 37.5017 - val_loss: 41.6276 - val_mae: 5.5930 - val_mse: 41.6276\n",
      "Epoch 33/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 34.3557 - mae: 4.9454 - mse: 34.3557 - val_loss: 38.0710 - val_mae: 5.2940 - val_mse: 38.0710\n",
      "Epoch 34/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 31.7057 - mae: 4.7129 - mse: 31.7057 - val_loss: 34.7641 - val_mae: 5.0004 - val_mse: 34.7641\n",
      "Epoch 35/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 29.1844 - mae: 4.4647 - mse: 29.1844 - val_loss: 32.0509 - val_mae: 4.7425 - val_mse: 32.0509\n",
      "Epoch 36/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 26.9895 - mae: 4.2374 - mse: 26.9895 - val_loss: 29.8868 - val_mae: 4.5277 - val_mse: 29.8868\n",
      "Epoch 37/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 25.0425 - mae: 4.0372 - mse: 25.0425 - val_loss: 27.5549 - val_mae: 4.2963 - val_mse: 27.5549\n",
      "Epoch 38/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 23.3285 - mae: 3.8843 - mse: 23.3285 - val_loss: 25.5043 - val_mae: 4.0811 - val_mse: 25.5043\n",
      "Epoch 39/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 21.7971 - mae: 3.7253 - mse: 21.7971 - val_loss: 23.8833 - val_mae: 3.8989 - val_mse: 23.8833\n",
      "Epoch 40/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20.5311 - mae: 3.5795 - mse: 20.5311 - val_loss: 22.4445 - val_mae: 3.7314 - val_mse: 22.4445\n",
      "Epoch 41/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19.2606 - mae: 3.4397 - mse: 19.2606 - val_loss: 21.0174 - val_mae: 3.5629 - val_mse: 21.0174\n",
      "Epoch 42/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 18.2558 - mae: 3.3270 - mse: 18.2558 - val_loss: 19.7200 - val_mae: 3.4262 - val_mse: 19.7200\n",
      "Epoch 43/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 17.3764 - mae: 3.2302 - mse: 17.3764 - val_loss: 18.6504 - val_mae: 3.3064 - val_mse: 18.6504\n",
      "Epoch 44/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 16.5483 - mae: 3.1316 - mse: 16.5483 - val_loss: 17.7452 - val_mae: 3.1988 - val_mse: 17.7452\n",
      "Epoch 45/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 15.8881 - mae: 3.0448 - mse: 15.8881 - val_loss: 16.9132 - val_mae: 3.1030 - val_mse: 16.9132\n",
      "Epoch 46/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 15.2461 - mae: 2.9681 - mse: 15.2461 - val_loss: 16.0684 - val_mae: 3.0031 - val_mse: 16.0684\n",
      "Epoch 47/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14.6887 - mae: 2.9068 - mse: 14.6887 - val_loss: 15.3503 - val_mae: 2.9245 - val_mse: 15.3503\n",
      "Epoch 48/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14.2743 - mae: 2.8628 - mse: 14.2743 - val_loss: 14.7708 - val_mae: 2.8604 - val_mse: 14.7708\n",
      "Epoch 49/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.9137 - mae: 2.8176 - mse: 13.9137 - val_loss: 14.2563 - val_mae: 2.8161 - val_mse: 14.2563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 13.5871 - mae: 2.7732 - mse: 13.5871 - val_loss: 13.8457 - val_mae: 2.7878 - val_mse: 13.8457\n",
      "Epoch 51/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.2251 - mae: 2.7333 - mse: 13.2251 - val_loss: 13.3674 - val_mae: 2.7449 - val_mse: 13.3674\n",
      "Epoch 52/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.0073 - mae: 2.7142 - mse: 13.0073 - val_loss: 13.1358 - val_mae: 2.7118 - val_mse: 13.1358\n",
      "Epoch 53/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12.8484 - mae: 2.6986 - mse: 12.8484 - val_loss: 12.7723 - val_mae: 2.6833 - val_mse: 12.7723\n",
      "Epoch 54/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12.6430 - mae: 2.6695 - mse: 12.6430 - val_loss: 12.4197 - val_mae: 2.6655 - val_mse: 12.4197\n",
      "Epoch 55/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 12.4618 - mae: 2.6381 - mse: 12.4618 - val_loss: 12.2297 - val_mae: 2.6457 - val_mse: 12.2297\n",
      "Epoch 56/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12.2940 - mae: 2.6262 - mse: 12.2940 - val_loss: 12.0300 - val_mae: 2.6256 - val_mse: 12.0300\n",
      "Epoch 57/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12.1790 - mae: 2.6194 - mse: 12.1790 - val_loss: 11.8793 - val_mae: 2.6136 - val_mse: 11.8793\n",
      "Epoch 58/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12.0947 - mae: 2.6022 - mse: 12.0947 - val_loss: 11.7029 - val_mae: 2.5925 - val_mse: 11.7029\n",
      "Epoch 59/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12.0155 - mae: 2.5960 - mse: 12.0155 - val_loss: 11.4722 - val_mae: 2.5731 - val_mse: 11.4722\n",
      "Epoch 60/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11.9201 - mae: 2.5838 - mse: 11.9201 - val_loss: 11.3822 - val_mae: 2.5632 - val_mse: 11.3822\n",
      "Epoch 61/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11.8619 - mae: 2.5856 - mse: 11.8619 - val_loss: 11.2409 - val_mae: 2.5428 - val_mse: 11.2409\n",
      "Epoch 62/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 11.8073 - mae: 2.5869 - mse: 11.8073 - val_loss: 11.0651 - val_mae: 2.5271 - val_mse: 11.0651\n",
      "Epoch 63/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 11.7550 - mae: 2.5797 - mse: 11.7550 - val_loss: 11.0145 - val_mae: 2.5295 - val_mse: 11.0145\n",
      "Epoch 64/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11.7028 - mae: 2.5702 - mse: 11.7028 - val_loss: 10.9662 - val_mae: 2.5290 - val_mse: 10.9662\n",
      "Epoch 65/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11.6779 - mae: 2.5623 - mse: 11.6779 - val_loss: 10.9254 - val_mae: 2.5232 - val_mse: 10.9254\n",
      "Epoch 66/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11.6553 - mae: 2.5640 - mse: 11.6553 - val_loss: 10.7787 - val_mae: 2.5070 - val_mse: 10.7787\n",
      "Epoch 67/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11.6147 - mae: 2.5608 - mse: 11.6147 - val_loss: 10.7548 - val_mae: 2.5034 - val_mse: 10.7548\n",
      "Epoch 68/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11.6651 - mae: 2.5782 - mse: 11.6651 - val_loss: 10.6584 - val_mae: 2.4856 - val_mse: 10.6584\n",
      "Epoch 69/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11.7414 - mae: 2.5848 - mse: 11.7414 - val_loss: 10.7041 - val_mae: 2.5031 - val_mse: 10.7041\n",
      "Epoch 70/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11.5466 - mae: 2.5579 - mse: 11.5466 - val_loss: 10.6150 - val_mae: 2.4901 - val_mse: 10.6150\n",
      "Epoch 71/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11.5762 - mae: 2.5745 - mse: 11.5762 - val_loss: 10.5159 - val_mae: 2.4810 - val_mse: 10.5159\n",
      "Epoch 72/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11.5246 - mae: 2.5715 - mse: 11.5246 - val_loss: 10.5374 - val_mae: 2.4788 - val_mse: 10.5374\n",
      "Epoch 73/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11.6089 - mae: 2.5649 - mse: 11.6089 - val_loss: 10.6761 - val_mae: 2.4920 - val_mse: 10.6761\n",
      "Epoch 00073: early stopping\n"
     ]
    }
   ],
   "source": [
    "es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n",
    "\n",
    "# re-create the model to restart training every time\n",
    "model = get_model()\n",
    "history = model.fit(train_features, train_labels, epochs=200, validation_split = 0.2, callbacks=[es_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the training stopped after about 60/70 epochs, rather than running for the entire 200 epochs specified in `fit()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint Callback\n",
    "\n",
    "Let's add a second callback to save a model checkpoint after every epoch. Notice that we can pass multiple callbacks at the same time to `fit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_callback = keras.callbacks.ModelCheckpoint(\n",
    "    './callback_test_chkp/chkp_{epoch:02d}',\n",
    "    # './callback_test_chkp/chkp_best',\n",
    "    monitor='val_loss',\n",
    "    verbose=0, \n",
    "    save_best_only=False,\n",
    "    # save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode='auto',\n",
    "    save_freq='epoch'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/8 [==>...........................] - ETA: 1s - loss: 612.8845 - mae: 23.1756 - mse: 612.8845"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-22 12:16:15.643760: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_01/assets\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 579.1166 - mae: 22.9874 - mse: 579.1166 - val_loss: 546.4377 - val_mae: 22.8021 - val_mse: 546.4377\n",
      "Epoch 2/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 638.7021 - mae: 24.4261 - mse: 638.7021INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_02/assets\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 515.4190 - mae: 22.2004 - mse: 515.4190 - val_loss: 497.4308 - val_mae: 21.9939 - val_mse: 497.4308\n",
      "Epoch 3/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 491.9919 - mae: 21.8199 - mse: 491.9919INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_03/assets\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 469.2399 - mae: 21.2954 - mse: 469.2399 - val_loss: 463.3602 - val_mae: 21.2483 - val_mse: 463.3602\n",
      "Epoch 4/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 432.2855 - mae: 20.5799 - mse: 432.2855INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_04/assets\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 435.2071 - mae: 20.5104 - mse: 435.2071 - val_loss: 433.5208 - val_mae: 20.5422 - val_mse: 433.5208\n",
      "Epoch 5/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 431.1570 - mae: 20.4707 - mse: 431.1570INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_05/assets\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 403.2576 - mae: 19.7287 - mse: 403.2576 - val_loss: 405.0819 - val_mae: 19.8416 - val_mse: 405.0819\n",
      "Epoch 6/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 410.1916 - mae: 19.7040 - mse: 410.1916INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_06/assets\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 373.3617 - mae: 18.9608 - mse: 373.3617 - val_loss: 378.8996 - val_mae: 19.1630 - val_mse: 378.8996\n",
      "Epoch 7/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 333.5110 - mae: 17.9729 - mse: 333.5110INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_07/assets\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 345.1494 - mae: 18.2062 - mse: 345.1494 - val_loss: 353.1047 - val_mae: 18.4713 - val_mse: 353.1047\n",
      "Epoch 8/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 304.5555 - mae: 17.2064 - mse: 304.5555INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_08/assets\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 321.2182 - mae: 17.5291 - mse: 321.2182 - val_loss: 330.5772 - val_mae: 17.8138 - val_mse: 330.5772\n",
      "Epoch 9/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 292.5303 - mae: 16.8930 - mse: 292.5303INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_09/assets\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 294.4363 - mae: 16.7481 - mse: 294.4363 - val_loss: 306.6337 - val_mae: 17.1352 - val_mse: 306.6337\n",
      "Epoch 10/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 299.7020 - mae: 16.7573 - mse: 299.7020INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_10/assets\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 271.6170 - mae: 16.0528 - mse: 271.6170 - val_loss: 282.1894 - val_mae: 16.4232 - val_mse: 282.1894\n",
      "Epoch 11/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 257.4498 - mae: 15.5622 - mse: 257.4498INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_11/assets\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 249.7405 - mae: 15.3655 - mse: 249.7405 - val_loss: 260.8171 - val_mae: 15.7594 - val_mse: 260.8171\n",
      "Epoch 12/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 253.1622 - mae: 15.5405 - mse: 253.1622INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_12/assets\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 229.6219 - mae: 14.6997 - mse: 229.6219 - val_loss: 241.3360 - val_mae: 15.1244 - val_mse: 241.3360\n",
      "Epoch 13/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 198.0970 - mae: 13.6349 - mse: 198.0970INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_13/assets\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 211.1610 - mae: 14.0636 - mse: 211.1610 - val_loss: 222.7539 - val_mae: 14.4936 - val_mse: 222.7539\n",
      "Epoch 14/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 197.2104 - mae: 13.5494 - mse: 197.2104INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_14/assets\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 193.7015 - mae: 13.4363 - mse: 193.7015 - val_loss: 204.5490 - val_mae: 13.8638 - val_mse: 204.5490\n",
      "Epoch 15/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 170.8827 - mae: 12.7235 - mse: 170.8827INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_15/assets\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 177.7602 - mae: 12.8289 - mse: 177.7602 - val_loss: 188.4061 - val_mae: 13.2628 - val_mse: 188.4061\n",
      "Epoch 16/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 187.1089 - mae: 13.1635 - mse: 187.1089INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_16/assets\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 162.7032 - mae: 12.2279 - mse: 162.7032 - val_loss: 173.3236 - val_mae: 12.6785 - val_mse: 173.3236\n",
      "Epoch 17/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 138.7828 - mae: 11.3237 - mse: 138.7828INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_17/assets\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 148.4865 - mae: 11.6454 - mse: 148.4865 - val_loss: 158.5211 - val_mae: 12.0985 - val_mse: 158.5211\n",
      "Epoch 18/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 146.9288 - mae: 11.5334 - mse: 146.9288INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_18/assets\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 135.8613 - mae: 11.0929 - mse: 135.8613 - val_loss: 145.0969 - val_mae: 11.5373 - val_mse: 145.0969\n",
      "Epoch 19/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 120.6937 - mae: 10.3948 - mse: 120.6937INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_19/assets\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 124.1142 - mae: 10.5582 - mse: 124.1142 - val_loss: 133.4670 - val_mae: 11.0125 - val_mse: 133.4670\n",
      "Epoch 20/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 106.4495 - mae: 9.8275 - mse: 106.4495INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_20/assets\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 113.0444 - mae: 10.0320 - mse: 113.0444 - val_loss: 121.4681 - val_mae: 10.4740 - val_mse: 121.4681\n",
      "Epoch 21/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 103.8357 - mae: 9.3342 - mse: 103.8357INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_21/assets\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 103.0613 - mae: 9.5281 - mse: 103.0613 - val_loss: 111.2241 - val_mae: 9.9738 - val_mse: 111.2241\n",
      "Epoch 22/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 103.4694 - mae: 9.6010 - mse: 103.4694INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_22/assets\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 94.0227 - mae: 9.0481 - mse: 94.0227 - val_loss: 101.6069 - val_mae: 9.4843 - val_mse: 101.6069\n",
      "Epoch 23/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 92.0535 - mae: 8.9887 - mse: 92.0535INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_23/assets\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 85.5691 - mae: 8.5720 - mse: 85.5691 - val_loss: 93.1900 - val_mae: 9.0231 - val_mse: 93.1900\n",
      "Epoch 24/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 87.3733 - mae: 8.7158 - mse: 87.3733INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_24/assets\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 78.1955 - mae: 8.1311 - mse: 78.1955 - val_loss: 85.6538 - val_mae: 8.5805 - val_mse: 85.6538\n",
      "Epoch 25/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 85.0271 - mae: 8.6683 - mse: 85.0271INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_25/assets\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 71.0384 - mae: 7.6830 - mse: 71.0384 - val_loss: 78.3199 - val_mae: 8.1469 - val_mse: 78.3199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 68.1808 - mae: 7.3463 - mse: 68.1808INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_26/assets\n",
      "8/8 [==============================] - 1s 75ms/step - loss: 64.9741 - mae: 7.2943 - mse: 64.9741 - val_loss: 70.8682 - val_mae: 7.7087 - val_mse: 70.8682\n",
      "Epoch 27/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 69.1897 - mae: 7.7204 - mse: 69.1897INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_27/assets\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 58.9196 - mae: 6.8943 - mse: 58.9196 - val_loss: 64.6649 - val_mae: 7.3153 - val_mse: 64.6649\n",
      "Epoch 28/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 49.1182 - mae: 6.3059 - mse: 49.1182INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_28/assets\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 53.7143 - mae: 6.5192 - mse: 53.7143 - val_loss: 59.5887 - val_mae: 6.9589 - val_mse: 59.5887\n",
      "Epoch 29/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 65.9604 - mae: 7.3686 - mse: 65.9604INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_29/assets\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 48.9450 - mae: 6.1474 - mse: 48.9450 - val_loss: 54.4971 - val_mae: 6.5941 - val_mse: 54.4971\n",
      "Epoch 30/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 43.6574 - mae: 5.5425 - mse: 43.6574INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_30/assets\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 44.5900 - mae: 5.8091 - mse: 44.5900 - val_loss: 49.5642 - val_mae: 6.2400 - val_mse: 49.5642\n",
      "Epoch 31/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 30.4164 - mae: 4.9916 - mse: 30.4164INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_31/assets\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 40.6464 - mae: 5.4994 - mse: 40.6464 - val_loss: 45.3325 - val_mae: 5.9104 - val_mse: 45.3325\n",
      "Epoch 32/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 38.1667 - mae: 5.5978 - mse: 38.1667INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_32/assets\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 37.3484 - mae: 5.2134 - mse: 37.3484 - val_loss: 41.6984 - val_mae: 5.6030 - val_mse: 41.6984\n",
      "Epoch 33/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 37.5879 - mae: 5.3096 - mse: 37.5879INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_33/assets\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 34.3579 - mae: 4.9512 - mse: 34.3579 - val_loss: 38.0695 - val_mae: 5.3015 - val_mse: 38.0695\n",
      "Epoch 34/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 33.1212 - mae: 4.7493 - mse: 33.1212INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_34/assets\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 31.5758 - mae: 4.6854 - mse: 31.5758 - val_loss: 35.2034 - val_mae: 5.0335 - val_mse: 35.2034\n",
      "Epoch 35/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 40.5117 - mae: 5.4919 - mse: 40.5117INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_35/assets\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 29.0862 - mae: 4.4450 - mse: 29.0862 - val_loss: 32.3287 - val_mae: 4.7652 - val_mse: 32.3287\n",
      "Epoch 36/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 18.3920 - mae: 3.6083 - mse: 18.3920INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_36/assets\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 26.8096 - mae: 4.2254 - mse: 26.8096 - val_loss: 29.9825 - val_mae: 4.5381 - val_mse: 29.9825\n",
      "Epoch 37/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 22.4175 - mae: 3.9347 - mse: 22.4175INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_37/assets\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 24.9282 - mae: 4.0362 - mse: 24.9282 - val_loss: 27.8361 - val_mae: 4.3196 - val_mse: 27.8361\n",
      "Epoch 38/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 20.5768 - mae: 3.8797 - mse: 20.5768INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_38/assets\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 23.2301 - mae: 3.8700 - mse: 23.2301 - val_loss: 25.6378 - val_mae: 4.0963 - val_mse: 25.6378\n",
      "Epoch 39/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 24.3643 - mae: 4.0132 - mse: 24.3643INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_39/assets\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 21.7547 - mae: 3.7234 - mse: 21.7547 - val_loss: 23.7759 - val_mae: 3.8948 - val_mse: 23.7759\n",
      "Epoch 40/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 12.5933 - mae: 3.0258 - mse: 12.5933INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_40/assets\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 20.2921 - mae: 3.5745 - mse: 20.2921 - val_loss: 22.2524 - val_mae: 3.7146 - val_mse: 22.2524\n",
      "Epoch 41/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 20.3480 - mae: 3.9088 - mse: 20.3480INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_41/assets\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 19.3833 - mae: 3.4475 - mse: 19.3833 - val_loss: 20.9667 - val_mae: 3.5511 - val_mse: 20.9667\n",
      "Epoch 42/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 15.4313 - mae: 2.9901 - mse: 15.4313INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_42/assets\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 18.1077 - mae: 3.3092 - mse: 18.1077 - val_loss: 19.7035 - val_mae: 3.4194 - val_mse: 19.7035\n",
      "Epoch 43/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 12.9545 - mae: 2.9779 - mse: 12.9545INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_43/assets\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 17.1945 - mae: 3.2086 - mse: 17.1945 - val_loss: 18.5292 - val_mae: 3.2953 - val_mse: 18.5292\n",
      "Epoch 44/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 11.0940 - mae: 2.7547 - mse: 11.0940INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_44/assets\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 16.4014 - mae: 3.1183 - mse: 16.4014 - val_loss: 17.5874 - val_mae: 3.1886 - val_mse: 17.5874\n",
      "Epoch 45/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 7.5649 - mae: 2.1680 - mse: 7.5649INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_45/assets\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 15.7637 - mae: 3.0434 - mse: 15.7637 - val_loss: 16.8083 - val_mae: 3.1030 - val_mse: 16.8083\n",
      "Epoch 46/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 6.9748 - mae: 2.3697 - mse: 6.9748INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_46/assets\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 15.1807 - mae: 2.9683 - mse: 15.1807 - val_loss: 15.9960 - val_mae: 3.0030 - val_mse: 15.9960\n",
      "Epoch 47/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 10.3139 - mae: 2.7247 - mse: 10.3139INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_47/assets\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 14.6389 - mae: 2.8965 - mse: 14.6389 - val_loss: 15.2993 - val_mae: 2.9289 - val_mse: 15.2993\n",
      "Epoch 48/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 23.5616 - mae: 3.6834 - mse: 23.5616INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_48/assets\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 14.2154 - mae: 2.8438 - mse: 14.2154 - val_loss: 14.7389 - val_mae: 2.8746 - val_mse: 14.7389\n",
      "Epoch 49/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 27.8660 - mae: 3.7743 - mse: 27.8660INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_49/assets\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 13.8213 - mae: 2.8017 - mse: 13.8213 - val_loss: 14.1978 - val_mae: 2.8258 - val_mse: 14.1978\n",
      "Epoch 50/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 18.3229 - mae: 3.0124 - mse: 18.3229INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_50/assets\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 13.4531 - mae: 2.7644 - mse: 13.4531 - val_loss: 13.7990 - val_mae: 2.7923 - val_mse: 13.7990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 11.5428 - mae: 2.4929 - mse: 11.5428INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_51/assets\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 13.1645 - mae: 2.7237 - mse: 13.1645 - val_loss: 13.4079 - val_mae: 2.7518 - val_mse: 13.4079\n",
      "Epoch 52/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 20.9639 - mae: 3.4833 - mse: 20.9639INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_52/assets\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 12.9103 - mae: 2.6950 - mse: 12.9103 - val_loss: 13.0089 - val_mae: 2.7114 - val_mse: 13.0089\n",
      "Epoch 53/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 19.2376 - mae: 3.2313 - mse: 19.2376INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_53/assets\n",
      "8/8 [==============================] - 0s 68ms/step - loss: 12.6711 - mae: 2.6747 - mse: 12.6711 - val_loss: 12.6923 - val_mae: 2.6858 - val_mse: 12.6923\n",
      "Epoch 54/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 10.4956 - mae: 2.5923 - mse: 10.4956INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_54/assets\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 12.5284 - mae: 2.6528 - mse: 12.5284 - val_loss: 12.4578 - val_mae: 2.6651 - val_mse: 12.4578\n",
      "Epoch 55/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 16.4567 - mae: 2.7160 - mse: 16.4567INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_55/assets\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 12.3350 - mae: 2.6266 - mse: 12.3350 - val_loss: 12.1914 - val_mae: 2.6423 - val_mse: 12.1914\n",
      "Epoch 56/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 7.8315 - mae: 2.2470 - mse: 7.8315INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_56/assets\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 12.2769 - mae: 2.6307 - mse: 12.2769 - val_loss: 11.8556 - val_mae: 2.6098 - val_mse: 11.8556\n",
      "Epoch 57/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 9.2893 - mae: 2.4946 - mse: 9.2893INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_57/assets\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 12.1270 - mae: 2.6139 - mse: 12.1270 - val_loss: 11.7581 - val_mae: 2.6100 - val_mse: 11.7581\n",
      "Epoch 58/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 6.3248 - mae: 2.0445 - mse: 6.3248INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_58/assets\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 12.0519 - mae: 2.5894 - mse: 12.0519 - val_loss: 11.6886 - val_mae: 2.6006 - val_mse: 11.6886\n",
      "Epoch 59/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 12.2505 - mae: 2.5890 - mse: 12.2505INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_59/assets\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 11.9777 - mae: 2.6014 - mse: 11.9777 - val_loss: 11.4016 - val_mae: 2.5603 - val_mse: 11.4016\n",
      "Epoch 60/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 15.4526 - mae: 2.7693 - mse: 15.4526INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_60/assets\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 11.8866 - mae: 2.5946 - mse: 11.8866 - val_loss: 11.2585 - val_mae: 2.5531 - val_mse: 11.2585\n",
      "Epoch 61/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 7.1368 - mae: 2.2307 - mse: 7.1368INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_61/assets\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 11.7883 - mae: 2.5741 - mse: 11.7883 - val_loss: 11.2101 - val_mae: 2.5535 - val_mse: 11.2101\n",
      "Epoch 62/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 15.6440 - mae: 3.2193 - mse: 15.6440INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_62/assets\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 11.7557 - mae: 2.5745 - mse: 11.7557 - val_loss: 11.1740 - val_mae: 2.5458 - val_mse: 11.1740\n",
      "Epoch 63/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 14.4948 - mae: 2.8021 - mse: 14.4948INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_63/assets\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 11.7119 - mae: 2.5794 - mse: 11.7119 - val_loss: 10.9998 - val_mae: 2.5241 - val_mse: 10.9998\n",
      "Epoch 64/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 10.5739 - mae: 2.7052 - mse: 10.5739INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_64/assets\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 11.7290 - mae: 2.5731 - mse: 11.7290 - val_loss: 10.8767 - val_mae: 2.5135 - val_mse: 10.8767\n",
      "Epoch 65/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 10.3588 - mae: 2.4327 - mse: 10.3588INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_65/assets\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 11.6284 - mae: 2.5603 - mse: 11.6284 - val_loss: 10.8979 - val_mae: 2.5179 - val_mse: 10.8979\n",
      "Epoch 66/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 8.7553 - mae: 2.3888 - mse: 8.7553INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_66/assets\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 11.5996 - mae: 2.5581 - mse: 11.5996 - val_loss: 10.7036 - val_mae: 2.4976 - val_mse: 10.7036\n",
      "Epoch 67/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 10.3350 - mae: 2.6144 - mse: 10.3350INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_67/assets\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 11.5585 - mae: 2.5594 - mse: 11.5585 - val_loss: 10.6579 - val_mae: 2.4976 - val_mse: 10.6579\n",
      "Epoch 68/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 12.3125 - mae: 2.6467 - mse: 12.3125INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_68/assets\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 11.5943 - mae: 2.5601 - mse: 11.5943 - val_loss: 10.7215 - val_mae: 2.5026 - val_mse: 10.7215\n",
      "Epoch 69/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 13.8850 - mae: 2.7795 - mse: 13.8850INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_69/assets\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 11.5727 - mae: 2.5714 - mse: 11.5727 - val_loss: 10.6386 - val_mae: 2.4882 - val_mse: 10.6386\n",
      "Epoch 70/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 13.2340 - mae: 2.6898 - mse: 13.2340INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_70/assets\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 11.5180 - mae: 2.5753 - mse: 11.5180 - val_loss: 10.5513 - val_mae: 2.4857 - val_mse: 10.5513\n",
      "Epoch 71/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 11.6475 - mae: 2.3846 - mse: 11.6475INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_71/assets\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 11.5257 - mae: 2.5633 - mse: 11.5257 - val_loss: 10.5761 - val_mae: 2.4912 - val_mse: 10.5761\n",
      "Epoch 72/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 13.4602 - mae: 2.7236 - mse: 13.4602INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_72/assets\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 11.4784 - mae: 2.5579 - mse: 11.4784 - val_loss: 10.4608 - val_mae: 2.4828 - val_mse: 10.4608\n",
      "Epoch 73/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 11.5817 - mae: 2.7027 - mse: 11.5817INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_73/assets\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 11.5051 - mae: 2.5663 - mse: 11.5051 - val_loss: 10.3543 - val_mae: 2.4707 - val_mse: 10.3543\n",
      "Epoch 74/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 8.8831 - mae: 2.4787 - mse: 8.8831INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_74/assets\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 11.5310 - mae: 2.5637 - mse: 11.5310 - val_loss: 10.4825 - val_mae: 2.4822 - val_mse: 10.4825\n",
      "Epoch 75/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 10.5067 - mae: 2.5121 - mse: 10.5067INFO:tensorflow:Assets written to: ./callback_test_chkp/chkp_75/assets\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 11.4603 - mae: 2.5441 - mse: 11.4603 - val_loss: 10.4067 - val_mae: 2.4706 - val_mse: 10.4067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00075: early stopping\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "history = model.fit(train_features, train_labels, epochs=200, validation_split = 0.2,\n",
    "                                callbacks=[es_callback, cp_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Restore a saved checkpoint\n",
    "\n",
    "Let's try loading back two different models, and let's evaluate them on training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 889us/step - loss: 534.7635 - mae: 22.5113 - mse: 534.7635\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[534.7635498046875, 22.51128578186035, 534.7635498046875]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_epoch1 = keras.models.load_model('./callback_test_chkp/chkp_01')\n",
    "model_epoch1.evaluate(train_features, train_labels,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 1ms/step - loss: 263.2537 - mae: 15.8052 - mse: 263.2537\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[263.253662109375, 15.80517578125, 263.253662109375]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_epoch10 = keras.models.load_model('./callback_test_chkp/chkp_10')\n",
    "model_epoch10.evaluate(train_features, train_labels,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Scheduling\n",
    "\n",
    "Let's try to change the learning rate by reducing it by 0.01 after every epoch. This is just to demonstrate LR scheduling, it is not a particularly useful scheduling mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_schedule(epoch, lr):\n",
    "    return max(lr - 0.01, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if the schedule works for different input LR values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04\n",
      "0.01\n"
     ]
    }
   ],
   "source": [
    "print(my_schedule(1, 0.05))\n",
    "print(my_schedule(1, 0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_callback = keras.callbacks.LearningRateScheduler(my_schedule, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.09000000149011612.\n",
      "Epoch 1/200\n",
      "8/8 [==============================] - 1s 25ms/step - loss: 568.0912 - mae: 22.9338 - mse: 568.0912 - val_loss: 561.9692 - val_mae: 23.1512 - val_mse: 561.9692 - lr: 0.0900\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.08000000357627869.\n",
      "Epoch 2/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 514.5397 - mae: 22.1865 - mse: 514.5397 - val_loss: 522.3202 - val_mae: 22.4901 - val_mse: 522.3202 - lr: 0.0800\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.07000000566244126.\n",
      "Epoch 3/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 483.5588 - mae: 21.5829 - mse: 483.5588 - val_loss: 494.6035 - val_mae: 21.9290 - val_mse: 494.6035 - lr: 0.0700\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.06000000774860382.\n",
      "Epoch 4/200\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 460.8472 - mae: 21.0891 - mse: 460.8472 - val_loss: 474.4955 - val_mae: 21.4789 - val_mse: 474.4955 - lr: 0.0600\n",
      "\n",
      "Epoch 00005: LearningRateScheduler setting learning rate to 0.05000000610947609.\n",
      "Epoch 5/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 441.8469 - mae: 20.6429 - mse: 441.8469 - val_loss: 458.9105 - val_mae: 21.1151 - val_mse: 458.9105 - lr: 0.0500\n",
      "\n",
      "Epoch 00006: LearningRateScheduler setting learning rate to 0.040000004470348356.\n",
      "Epoch 6/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 427.0905 - mae: 20.2928 - mse: 427.0905 - val_loss: 447.0638 - val_mae: 20.8328 - val_mse: 447.0638 - lr: 0.0400\n",
      "\n",
      "Epoch 00007: LearningRateScheduler setting learning rate to 0.030000002831220625.\n",
      "Epoch 7/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 414.9564 - mae: 19.9968 - mse: 414.9564 - val_loss: 437.8624 - val_mae: 20.6114 - val_mse: 437.8624 - lr: 0.0300\n",
      "\n",
      "Epoch 00008: LearningRateScheduler setting learning rate to 0.020000003054738043.\n",
      "Epoch 8/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 407.1186 - mae: 19.8010 - mse: 407.1186 - val_loss: 432.1967 - val_mae: 20.4725 - val_mse: 432.1967 - lr: 0.0200\n",
      "\n",
      "Epoch 00009: LearningRateScheduler setting learning rate to 0.010000003278255462.\n",
      "Epoch 9/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 402.4174 - mae: 19.6826 - mse: 402.4174 - val_loss: 429.4520 - val_mae: 20.4040 - val_mse: 429.4520 - lr: 0.0100\n",
      "\n",
      "Epoch 00010: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 10/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 399.0803 - mae: 19.6000 - mse: 399.0803 - val_loss: 426.2147 - val_mae: 20.3260 - val_mse: 426.2147 - lr: 0.0100\n",
      "\n",
      "Epoch 00011: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 11/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 396.0833 - mae: 19.5248 - mse: 396.0833 - val_loss: 423.2916 - val_mae: 20.2539 - val_mse: 423.2916 - lr: 0.0100\n",
      "\n",
      "Epoch 00012: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 12/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 393.0767 - mae: 19.4481 - mse: 393.0767 - val_loss: 420.3408 - val_mae: 20.1810 - val_mse: 420.3408 - lr: 0.0100\n",
      "\n",
      "Epoch 00013: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 13/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 390.0660 - mae: 19.3714 - mse: 390.0660 - val_loss: 417.3072 - val_mae: 20.1063 - val_mse: 417.3072 - lr: 0.0100\n",
      "\n",
      "Epoch 00014: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 14/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 387.0998 - mae: 19.2952 - mse: 387.0998 - val_loss: 414.3385 - val_mae: 20.0325 - val_mse: 414.3385 - lr: 0.0100\n",
      "\n",
      "Epoch 00015: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 15/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 384.0677 - mae: 19.2179 - mse: 384.0677 - val_loss: 411.3860 - val_mae: 19.9587 - val_mse: 411.3860 - lr: 0.0100\n",
      "\n",
      "Epoch 00016: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 16/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 381.1900 - mae: 19.1439 - mse: 381.1900 - val_loss: 408.3709 - val_mae: 19.8838 - val_mse: 408.3709 - lr: 0.0100\n",
      "\n",
      "Epoch 00017: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 17/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 378.1728 - mae: 19.0657 - mse: 378.1728 - val_loss: 405.5436 - val_mae: 19.8118 - val_mse: 405.5436 - lr: 0.0100\n",
      "\n",
      "Epoch 00018: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 18/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 375.2139 - mae: 18.9883 - mse: 375.2139 - val_loss: 402.6492 - val_mae: 19.7384 - val_mse: 402.6492 - lr: 0.0100\n",
      "\n",
      "Epoch 00019: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 19/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 372.3110 - mae: 18.9125 - mse: 372.3110 - val_loss: 399.7837 - val_mae: 19.6654 - val_mse: 399.7837 - lr: 0.0100\n",
      "\n",
      "Epoch 00020: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 20/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 369.4718 - mae: 18.8384 - mse: 369.4718 - val_loss: 396.9950 - val_mae: 19.5935 - val_mse: 396.9950 - lr: 0.0100\n",
      "\n",
      "Epoch 00021: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 21/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 366.5193 - mae: 18.7599 - mse: 366.5193 - val_loss: 394.0433 - val_mae: 19.5185 - val_mse: 394.0433 - lr: 0.0100\n",
      "\n",
      "Epoch 00022: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 22/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 363.7141 - mae: 18.6853 - mse: 363.7141 - val_loss: 391.2911 - val_mae: 19.4469 - val_mse: 391.2911 - lr: 0.0100\n",
      "\n",
      "Epoch 00023: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 23/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 360.7451 - mae: 18.6069 - mse: 360.7451 - val_loss: 388.4046 - val_mae: 19.3726 - val_mse: 388.4046 - lr: 0.0100\n",
      "\n",
      "Epoch 00024: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 24/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 357.9542 - mae: 18.5324 - mse: 357.9542 - val_loss: 385.4566 - val_mae: 19.2973 - val_mse: 385.4566 - lr: 0.0100\n",
      "\n",
      "Epoch 00025: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 25/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 355.2264 - mae: 18.4596 - mse: 355.2264 - val_loss: 382.4742 - val_mae: 19.2211 - val_mse: 382.4742 - lr: 0.0100\n",
      "\n",
      "Epoch 00026: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 26/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 352.2898 - mae: 18.3811 - mse: 352.2898 - val_loss: 379.8130 - val_mae: 19.1506 - val_mse: 379.8130 - lr: 0.0100\n",
      "\n",
      "Epoch 00027: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 27/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 349.4853 - mae: 18.3052 - mse: 349.4853 - val_loss: 376.9829 - val_mae: 19.0767 - val_mse: 376.9829 - lr: 0.0100\n",
      "\n",
      "Epoch 00028: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 28/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 346.7727 - mae: 18.2317 - mse: 346.7727 - val_loss: 374.2417 - val_mae: 19.0040 - val_mse: 374.2417 - lr: 0.0100\n",
      "\n",
      "Epoch 00029: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 29/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 343.9423 - mae: 18.1544 - mse: 343.9423 - val_loss: 371.4695 - val_mae: 18.9308 - val_mse: 371.4695 - lr: 0.0100\n",
      "\n",
      "Epoch 00030: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 30/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 341.2477 - mae: 18.0801 - mse: 341.2477 - val_loss: 368.5237 - val_mae: 18.8543 - val_mse: 368.5237 - lr: 0.0100\n",
      "\n",
      "Epoch 00031: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 31/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 338.5343 - mae: 18.0060 - mse: 338.5343 - val_loss: 365.8766 - val_mae: 18.7830 - val_mse: 365.8766 - lr: 0.0100\n",
      "\n",
      "Epoch 00032: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 32/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 335.7597 - mae: 17.9293 - mse: 335.7597 - val_loss: 363.2079 - val_mae: 18.7112 - val_mse: 363.2079 - lr: 0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00033: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 33/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 333.0726 - mae: 17.8545 - mse: 333.0726 - val_loss: 360.4099 - val_mae: 18.6368 - val_mse: 360.4099 - lr: 0.0100\n",
      "\n",
      "Epoch 00034: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 34/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 330.3840 - mae: 17.7800 - mse: 330.3840 - val_loss: 357.6510 - val_mae: 18.5631 - val_mse: 357.6510 - lr: 0.0100\n",
      "\n",
      "Epoch 00035: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 35/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 327.8230 - mae: 17.7091 - mse: 327.8230 - val_loss: 354.8318 - val_mae: 18.4883 - val_mse: 354.8318 - lr: 0.0100\n",
      "\n",
      "Epoch 00036: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 36/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 325.0799 - mae: 17.6323 - mse: 325.0799 - val_loss: 352.2760 - val_mae: 18.4178 - val_mse: 352.2760 - lr: 0.0100\n",
      "\n",
      "Epoch 00037: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 37/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 322.5083 - mae: 17.5585 - mse: 322.5083 - val_loss: 349.4950 - val_mae: 18.3429 - val_mse: 349.4950 - lr: 0.0100\n",
      "\n",
      "Epoch 00038: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 38/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 319.8794 - mae: 17.4849 - mse: 319.8794 - val_loss: 346.7347 - val_mae: 18.2683 - val_mse: 346.7347 - lr: 0.0100\n",
      "\n",
      "Epoch 00039: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 39/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 317.2264 - mae: 17.4096 - mse: 317.2264 - val_loss: 344.2037 - val_mae: 18.1980 - val_mse: 344.2037 - lr: 0.0100\n",
      "\n",
      "Epoch 00040: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 40/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 314.6523 - mae: 17.3358 - mse: 314.6523 - val_loss: 341.6654 - val_mae: 18.1271 - val_mse: 341.6654 - lr: 0.0100\n",
      "\n",
      "Epoch 00041: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 41/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 312.1613 - mae: 17.2642 - mse: 312.1613 - val_loss: 339.0305 - val_mae: 18.0544 - val_mse: 339.0305 - lr: 0.0100\n",
      "\n",
      "Epoch 00042: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 42/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 309.5636 - mae: 17.1894 - mse: 309.5636 - val_loss: 336.3136 - val_mae: 17.9799 - val_mse: 336.3136 - lr: 0.0100\n",
      "\n",
      "Epoch 00043: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 43/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 307.0209 - mae: 17.1163 - mse: 307.0209 - val_loss: 333.7586 - val_mae: 17.9087 - val_mse: 333.7586 - lr: 0.0100\n",
      "\n",
      "Epoch 00044: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 44/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 304.5032 - mae: 17.0427 - mse: 304.5032 - val_loss: 331.2895 - val_mae: 17.8387 - val_mse: 331.2895 - lr: 0.0100\n",
      "\n",
      "Epoch 00045: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 45/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 301.9973 - mae: 16.9691 - mse: 301.9973 - val_loss: 328.7243 - val_mae: 17.7665 - val_mse: 328.7243 - lr: 0.0100\n",
      "\n",
      "Epoch 00046: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 46/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 299.5334 - mae: 16.8965 - mse: 299.5334 - val_loss: 326.2692 - val_mae: 17.6961 - val_mse: 326.2692 - lr: 0.0100\n",
      "\n",
      "Epoch 00047: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 47/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 297.0658 - mae: 16.8237 - mse: 297.0658 - val_loss: 323.7884 - val_mae: 17.6255 - val_mse: 323.7884 - lr: 0.0100\n",
      "\n",
      "Epoch 00048: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 48/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 294.5687 - mae: 16.7503 - mse: 294.5687 - val_loss: 321.2864 - val_mae: 17.5542 - val_mse: 321.2864 - lr: 0.0100\n",
      "\n",
      "Epoch 00049: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 49/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 292.2142 - mae: 16.6796 - mse: 292.2142 - val_loss: 318.6628 - val_mae: 17.4806 - val_mse: 318.6628 - lr: 0.0100\n",
      "\n",
      "Epoch 00050: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 50/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 289.7367 - mae: 16.6067 - mse: 289.7367 - val_loss: 316.1607 - val_mae: 17.4089 - val_mse: 316.1607 - lr: 0.0100\n",
      "\n",
      "Epoch 00051: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 51/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 287.3497 - mae: 16.5346 - mse: 287.3497 - val_loss: 313.7022 - val_mae: 17.3377 - val_mse: 313.7022 - lr: 0.0100\n",
      "\n",
      "Epoch 00052: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 52/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 284.9282 - mae: 16.4618 - mse: 284.9282 - val_loss: 311.3201 - val_mae: 17.2681 - val_mse: 311.3201 - lr: 0.0100\n",
      "\n",
      "Epoch 00053: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 53/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 282.5751 - mae: 16.3894 - mse: 282.5751 - val_loss: 308.9358 - val_mae: 17.1982 - val_mse: 308.9358 - lr: 0.0100\n",
      "\n",
      "Epoch 00054: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 54/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 280.2254 - mae: 16.3190 - mse: 280.2254 - val_loss: 306.4849 - val_mae: 17.1271 - val_mse: 306.4849 - lr: 0.0100\n",
      "\n",
      "Epoch 00055: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 55/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 277.8293 - mae: 16.2463 - mse: 277.8293 - val_loss: 304.0239 - val_mae: 17.0558 - val_mse: 304.0239 - lr: 0.0100\n",
      "\n",
      "Epoch 00056: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 56/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 275.5439 - mae: 16.1758 - mse: 275.5439 - val_loss: 301.4561 - val_mae: 16.9820 - val_mse: 301.4561 - lr: 0.0100\n",
      "\n",
      "Epoch 00057: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 57/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 273.1925 - mae: 16.1041 - mse: 273.1925 - val_loss: 299.0118 - val_mae: 16.9105 - val_mse: 299.0118 - lr: 0.0100\n",
      "\n",
      "Epoch 00058: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 58/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 270.8767 - mae: 16.0330 - mse: 270.8767 - val_loss: 296.6867 - val_mae: 16.8411 - val_mse: 296.6867 - lr: 0.0100\n",
      "\n",
      "Epoch 00059: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 59/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 268.6242 - mae: 15.9627 - mse: 268.6242 - val_loss: 294.2449 - val_mae: 16.7693 - val_mse: 294.2449 - lr: 0.0100\n",
      "\n",
      "Epoch 00060: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 60/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 266.3812 - mae: 15.8926 - mse: 266.3812 - val_loss: 291.9844 - val_mae: 16.7010 - val_mse: 291.9844 - lr: 0.0100\n",
      "\n",
      "Epoch 00061: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 61/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 264.0963 - mae: 15.8205 - mse: 264.0963 - val_loss: 289.7140 - val_mae: 16.6320 - val_mse: 289.7140 - lr: 0.0100\n",
      "\n",
      "Epoch 00062: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 62/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 261.8392 - mae: 15.7492 - mse: 261.8392 - val_loss: 287.3858 - val_mae: 16.5619 - val_mse: 287.3858 - lr: 0.0100\n",
      "\n",
      "Epoch 00063: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 63/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 259.6218 - mae: 15.6795 - mse: 259.6218 - val_loss: 284.9606 - val_mae: 16.4898 - val_mse: 284.9606 - lr: 0.0100\n",
      "\n",
      "Epoch 00064: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 64/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 257.4928 - mae: 15.6109 - mse: 257.4928 - val_loss: 282.6245 - val_mae: 16.4194 - val_mse: 282.6245 - lr: 0.0100\n",
      "\n",
      "Epoch 00065: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 65/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 4ms/step - loss: 255.1996 - mae: 15.5389 - mse: 255.1996 - val_loss: 280.3805 - val_mae: 16.3506 - val_mse: 280.3805 - lr: 0.0100\n",
      "\n",
      "Epoch 00066: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 66/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 253.1167 - mae: 15.4714 - mse: 253.1167 - val_loss: 278.2611 - val_mae: 16.2838 - val_mse: 278.2611 - lr: 0.0100\n",
      "\n",
      "Epoch 00067: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 67/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 250.8825 - mae: 15.3993 - mse: 250.8825 - val_loss: 276.0200 - val_mae: 16.2145 - val_mse: 276.0200 - lr: 0.0100\n",
      "\n",
      "Epoch 00068: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 68/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 248.7883 - mae: 15.3314 - mse: 248.7883 - val_loss: 273.7058 - val_mae: 16.1442 - val_mse: 273.7058 - lr: 0.0100\n",
      "\n",
      "Epoch 00069: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 69/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 246.5897 - mae: 15.2604 - mse: 246.5897 - val_loss: 271.4380 - val_mae: 16.0742 - val_mse: 271.4380 - lr: 0.0100\n",
      "\n",
      "Epoch 00070: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 70/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 244.4900 - mae: 15.1921 - mse: 244.4900 - val_loss: 269.2667 - val_mae: 16.0060 - val_mse: 269.2667 - lr: 0.0100\n",
      "\n",
      "Epoch 00071: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 71/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 242.3685 - mae: 15.1219 - mse: 242.3685 - val_loss: 266.9942 - val_mae: 15.9358 - val_mse: 266.9942 - lr: 0.0100\n",
      "\n",
      "Epoch 00072: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 72/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 240.2561 - mae: 15.0524 - mse: 240.2561 - val_loss: 264.8430 - val_mae: 15.8678 - val_mse: 264.8430 - lr: 0.0100\n",
      "\n",
      "Epoch 00073: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 73/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 238.1643 - mae: 14.9835 - mse: 238.1643 - val_loss: 262.6186 - val_mae: 15.7982 - val_mse: 262.6186 - lr: 0.0100\n",
      "\n",
      "Epoch 00074: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 74/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 236.1009 - mae: 14.9153 - mse: 236.1009 - val_loss: 260.4886 - val_mae: 15.7304 - val_mse: 260.4886 - lr: 0.0100\n",
      "\n",
      "Epoch 00075: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 75/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 234.0561 - mae: 14.8475 - mse: 234.0561 - val_loss: 258.2813 - val_mae: 15.6610 - val_mse: 258.2813 - lr: 0.0100\n",
      "\n",
      "Epoch 00076: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 76/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 232.0245 - mae: 14.7790 - mse: 232.0245 - val_loss: 256.2057 - val_mae: 15.5941 - val_mse: 256.2057 - lr: 0.0100\n",
      "\n",
      "Epoch 00077: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 77/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 230.0062 - mae: 14.7104 - mse: 230.0062 - val_loss: 254.1335 - val_mae: 15.5268 - val_mse: 254.1335 - lr: 0.0100\n",
      "\n",
      "Epoch 00078: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 78/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 228.0159 - mae: 14.6431 - mse: 228.0159 - val_loss: 251.9193 - val_mae: 15.4567 - val_mse: 251.9193 - lr: 0.0100\n",
      "\n",
      "Epoch 00079: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 79/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 225.9962 - mae: 14.5750 - mse: 225.9962 - val_loss: 249.9146 - val_mae: 15.3909 - val_mse: 249.9146 - lr: 0.0100\n",
      "\n",
      "Epoch 00080: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 80/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 224.0484 - mae: 14.5082 - mse: 224.0484 - val_loss: 247.8826 - val_mae: 15.3245 - val_mse: 247.8826 - lr: 0.0100\n",
      "\n",
      "Epoch 00081: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 81/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 222.0529 - mae: 14.4390 - mse: 222.0529 - val_loss: 245.6991 - val_mae: 15.2543 - val_mse: 245.6991 - lr: 0.0100\n",
      "\n",
      "Epoch 00082: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 82/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 220.1055 - mae: 14.3718 - mse: 220.1055 - val_loss: 243.6961 - val_mae: 15.1878 - val_mse: 243.6961 - lr: 0.0100\n",
      "\n",
      "Epoch 00083: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 83/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 218.1496 - mae: 14.3047 - mse: 218.1496 - val_loss: 241.4902 - val_mae: 15.1171 - val_mse: 241.4902 - lr: 0.0100\n",
      "\n",
      "Epoch 00084: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 84/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 216.1969 - mae: 14.2372 - mse: 216.1969 - val_loss: 239.4553 - val_mae: 15.0499 - val_mse: 239.4553 - lr: 0.0100\n",
      "\n",
      "Epoch 00085: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 85/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 214.2925 - mae: 14.1704 - mse: 214.2925 - val_loss: 237.4827 - val_mae: 14.9837 - val_mse: 237.4827 - lr: 0.0100\n",
      "\n",
      "Epoch 00086: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 86/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 212.3686 - mae: 14.1027 - mse: 212.3686 - val_loss: 235.4851 - val_mae: 14.9172 - val_mse: 235.4851 - lr: 0.0100\n",
      "\n",
      "Epoch 00087: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 87/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 210.5087 - mae: 14.0366 - mse: 210.5087 - val_loss: 233.4216 - val_mae: 14.8490 - val_mse: 233.4216 - lr: 0.0100\n",
      "\n",
      "Epoch 00088: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 88/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 208.6693 - mae: 13.9714 - mse: 208.6693 - val_loss: 231.3183 - val_mae: 14.7799 - val_mse: 231.3183 - lr: 0.0100\n",
      "\n",
      "Epoch 00089: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 89/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 206.7632 - mae: 13.9047 - mse: 206.7632 - val_loss: 229.2991 - val_mae: 14.7124 - val_mse: 229.2991 - lr: 0.0100\n",
      "\n",
      "Epoch 00090: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 90/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 204.9399 - mae: 13.8386 - mse: 204.9399 - val_loss: 227.5218 - val_mae: 14.6496 - val_mse: 227.5218 - lr: 0.0100\n",
      "\n",
      "Epoch 00091: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 91/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 203.1540 - mae: 13.7746 - mse: 203.1540 - val_loss: 225.4533 - val_mae: 14.5809 - val_mse: 225.4533 - lr: 0.0100\n",
      "\n",
      "Epoch 00092: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 92/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 201.2664 - mae: 13.7064 - mse: 201.2664 - val_loss: 223.5672 - val_mae: 14.5155 - val_mse: 223.5672 - lr: 0.0100\n",
      "\n",
      "Epoch 00093: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 93/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 199.4391 - mae: 13.6400 - mse: 199.4391 - val_loss: 221.7290 - val_mae: 14.4513 - val_mse: 221.7290 - lr: 0.0100\n",
      "\n",
      "Epoch 00094: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 94/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 197.6302 - mae: 13.5736 - mse: 197.6302 - val_loss: 219.8928 - val_mae: 14.3869 - val_mse: 219.8928 - lr: 0.0100\n",
      "\n",
      "Epoch 00095: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 95/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 195.9028 - mae: 13.5092 - mse: 195.9028 - val_loss: 218.0472 - val_mae: 14.3220 - val_mse: 218.0472 - lr: 0.0100\n",
      "\n",
      "Epoch 00096: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 96/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 194.0902 - mae: 13.4423 - mse: 194.0902 - val_loss: 216.1485 - val_mae: 14.2559 - val_mse: 216.1485 - lr: 0.0100\n",
      "\n",
      "Epoch 00097: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 97/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 192.3265 - mae: 13.3782 - mse: 192.3265 - val_loss: 214.2048 - val_mae: 14.1891 - val_mse: 214.2048 - lr: 0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00098: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 98/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 190.6013 - mae: 13.3139 - mse: 190.6013 - val_loss: 212.3131 - val_mae: 14.1230 - val_mse: 212.3131 - lr: 0.0100\n",
      "\n",
      "Epoch 00099: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 99/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 188.8649 - mae: 13.2484 - mse: 188.8649 - val_loss: 210.5586 - val_mae: 14.0597 - val_mse: 210.5586 - lr: 0.0100\n",
      "\n",
      "Epoch 00100: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 100/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 187.1521 - mae: 13.1832 - mse: 187.1521 - val_loss: 208.6789 - val_mae: 13.9933 - val_mse: 208.6789 - lr: 0.0100\n",
      "\n",
      "Epoch 00101: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 101/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 185.4381 - mae: 13.1184 - mse: 185.4381 - val_loss: 206.9275 - val_mae: 13.9302 - val_mse: 206.9275 - lr: 0.0100\n",
      "\n",
      "Epoch 00102: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 102/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 183.7076 - mae: 13.0535 - mse: 183.7076 - val_loss: 205.0492 - val_mae: 13.8638 - val_mse: 205.0492 - lr: 0.0100\n",
      "\n",
      "Epoch 00103: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 103/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 182.0383 - mae: 12.9899 - mse: 182.0383 - val_loss: 203.1870 - val_mae: 13.7977 - val_mse: 203.1870 - lr: 0.0100\n",
      "\n",
      "Epoch 00104: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 104/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 180.3686 - mae: 12.9255 - mse: 180.3686 - val_loss: 201.3864 - val_mae: 13.7328 - val_mse: 201.3864 - lr: 0.0100\n",
      "\n",
      "Epoch 00105: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 105/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 178.7175 - mae: 12.8623 - mse: 178.7175 - val_loss: 199.5909 - val_mae: 13.6677 - val_mse: 199.5909 - lr: 0.0100\n",
      "\n",
      "Epoch 00106: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 106/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 177.0773 - mae: 12.7989 - mse: 177.0773 - val_loss: 197.8730 - val_mae: 13.6046 - val_mse: 197.8730 - lr: 0.0100\n",
      "\n",
      "Epoch 00107: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 107/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 175.4386 - mae: 12.7349 - mse: 175.4386 - val_loss: 196.2227 - val_mae: 13.5427 - val_mse: 196.2227 - lr: 0.0100\n",
      "\n",
      "Epoch 00108: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 108/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 173.8363 - mae: 12.6720 - mse: 173.8363 - val_loss: 194.3963 - val_mae: 13.4770 - val_mse: 194.3963 - lr: 0.0100\n",
      "\n",
      "Epoch 00109: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 109/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 172.1957 - mae: 12.6082 - mse: 172.1957 - val_loss: 192.6895 - val_mae: 13.4135 - val_mse: 192.6895 - lr: 0.0100\n",
      "\n",
      "Epoch 00110: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 110/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 170.6287 - mae: 12.5468 - mse: 170.6287 - val_loss: 190.9772 - val_mae: 13.3502 - val_mse: 190.9772 - lr: 0.0100\n",
      "\n",
      "Epoch 00111: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 111/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 169.0249 - mae: 12.4832 - mse: 169.0249 - val_loss: 189.2593 - val_mae: 13.2859 - val_mse: 189.2593 - lr: 0.0100\n",
      "\n",
      "Epoch 00112: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 112/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 167.4540 - mae: 12.4194 - mse: 167.4540 - val_loss: 187.5918 - val_mae: 13.2227 - val_mse: 187.5918 - lr: 0.0100\n",
      "\n",
      "Epoch 00113: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 113/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 165.9100 - mae: 12.3579 - mse: 165.9100 - val_loss: 185.9274 - val_mae: 13.1602 - val_mse: 185.9274 - lr: 0.0100\n",
      "\n",
      "Epoch 00114: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 114/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 164.3006 - mae: 12.2933 - mse: 164.3006 - val_loss: 184.3426 - val_mae: 13.0991 - val_mse: 184.3426 - lr: 0.0100\n",
      "\n",
      "Epoch 00115: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 115/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 162.8256 - mae: 12.2323 - mse: 162.8256 - val_loss: 182.7248 - val_mae: 13.0364 - val_mse: 182.7248 - lr: 0.0100\n",
      "\n",
      "Epoch 00116: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 116/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 161.2627 - mae: 12.1689 - mse: 161.2627 - val_loss: 181.0523 - val_mae: 12.9732 - val_mse: 181.0523 - lr: 0.0100\n",
      "\n",
      "Epoch 00117: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 117/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 159.7305 - mae: 12.1070 - mse: 159.7305 - val_loss: 179.3929 - val_mae: 12.9100 - val_mse: 179.3929 - lr: 0.0100\n",
      "\n",
      "Epoch 00118: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 118/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 158.2541 - mae: 12.0459 - mse: 158.2541 - val_loss: 177.7183 - val_mae: 12.8461 - val_mse: 177.7183 - lr: 0.0100\n",
      "\n",
      "Epoch 00119: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 119/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 156.7590 - mae: 11.9841 - mse: 156.7590 - val_loss: 176.1194 - val_mae: 12.7841 - val_mse: 176.1194 - lr: 0.0100\n",
      "\n",
      "Epoch 00120: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 120/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 155.2424 - mae: 11.9211 - mse: 155.2424 - val_loss: 174.6098 - val_mae: 12.7238 - val_mse: 174.6098 - lr: 0.0100\n",
      "\n",
      "Epoch 00121: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 121/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 153.7719 - mae: 11.8592 - mse: 153.7719 - val_loss: 173.0345 - val_mae: 12.6617 - val_mse: 173.0345 - lr: 0.0100\n",
      "\n",
      "Epoch 00122: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 122/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 152.3508 - mae: 11.7991 - mse: 152.3508 - val_loss: 171.5193 - val_mae: 12.6011 - val_mse: 171.5193 - lr: 0.0100\n",
      "\n",
      "Epoch 00123: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 123/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 150.8683 - mae: 11.7367 - mse: 150.8683 - val_loss: 169.9933 - val_mae: 12.5401 - val_mse: 169.9933 - lr: 0.0100\n",
      "\n",
      "Epoch 00124: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 124/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 149.4364 - mae: 11.6758 - mse: 149.4364 - val_loss: 168.3901 - val_mae: 12.4772 - val_mse: 168.3901 - lr: 0.0100\n",
      "\n",
      "Epoch 00125: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 125/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 148.0127 - mae: 11.6154 - mse: 148.0127 - val_loss: 166.7829 - val_mae: 12.4143 - val_mse: 166.7829 - lr: 0.0100\n",
      "\n",
      "Epoch 00126: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 126/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 146.6111 - mae: 11.5567 - mse: 146.6111 - val_loss: 165.1296 - val_mae: 12.3502 - val_mse: 165.1296 - lr: 0.0100\n",
      "\n",
      "Epoch 00127: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 127/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 145.1991 - mae: 11.4959 - mse: 145.1991 - val_loss: 163.6903 - val_mae: 12.2907 - val_mse: 163.6903 - lr: 0.0100\n",
      "\n",
      "Epoch 00128: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 128/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 143.8405 - mae: 11.4369 - mse: 143.8405 - val_loss: 162.1113 - val_mae: 12.2283 - val_mse: 162.1113 - lr: 0.0100\n",
      "\n",
      "Epoch 00129: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 129/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 142.3989 - mae: 11.3752 - mse: 142.3989 - val_loss: 160.6305 - val_mae: 12.1677 - val_mse: 160.6305 - lr: 0.0100\n",
      "\n",
      "Epoch 00130: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 130/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 4ms/step - loss: 141.0119 - mae: 11.3154 - mse: 141.0119 - val_loss: 159.2273 - val_mae: 12.1090 - val_mse: 159.2273 - lr: 0.0100\n",
      "\n",
      "Epoch 00131: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 131/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 139.6711 - mae: 11.2559 - mse: 139.6711 - val_loss: 157.7331 - val_mae: 12.0477 - val_mse: 157.7331 - lr: 0.0100\n",
      "\n",
      "Epoch 00132: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 132/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 138.4000 - mae: 11.2006 - mse: 138.4000 - val_loss: 156.2587 - val_mae: 11.9875 - val_mse: 156.2587 - lr: 0.0100\n",
      "\n",
      "Epoch 00133: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 133/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 136.9733 - mae: 11.1375 - mse: 136.9733 - val_loss: 154.7830 - val_mae: 11.9262 - val_mse: 154.7830 - lr: 0.0100\n",
      "\n",
      "Epoch 00134: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 134/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 135.6595 - mae: 11.0788 - mse: 135.6595 - val_loss: 153.4862 - val_mae: 11.8694 - val_mse: 153.4862 - lr: 0.0100\n",
      "\n",
      "Epoch 00135: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 135/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 134.3212 - mae: 11.0191 - mse: 134.3212 - val_loss: 151.9900 - val_mae: 11.8079 - val_mse: 151.9900 - lr: 0.0100\n",
      "\n",
      "Epoch 00136: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 136/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 133.0241 - mae: 10.9606 - mse: 133.0241 - val_loss: 150.5321 - val_mae: 11.7470 - val_mse: 150.5321 - lr: 0.0100\n",
      "\n",
      "Epoch 00137: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 137/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 131.7240 - mae: 10.9017 - mse: 131.7240 - val_loss: 149.0639 - val_mae: 11.6858 - val_mse: 149.0639 - lr: 0.0100\n",
      "\n",
      "Epoch 00138: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 138/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 130.4308 - mae: 10.8437 - mse: 130.4308 - val_loss: 147.6757 - val_mae: 11.6265 - val_mse: 147.6757 - lr: 0.0100\n",
      "\n",
      "Epoch 00139: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 139/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 129.1351 - mae: 10.7853 - mse: 129.1351 - val_loss: 146.2893 - val_mae: 11.5674 - val_mse: 146.2893 - lr: 0.0100\n",
      "\n",
      "Epoch 00140: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 140/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 127.8812 - mae: 10.7281 - mse: 127.8812 - val_loss: 144.9765 - val_mae: 11.5100 - val_mse: 144.9765 - lr: 0.0100\n",
      "\n",
      "Epoch 00141: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 141/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 126.6553 - mae: 10.6701 - mse: 126.6553 - val_loss: 143.5991 - val_mae: 11.4502 - val_mse: 143.5991 - lr: 0.0100\n",
      "\n",
      "Epoch 00142: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 142/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 125.3952 - mae: 10.6118 - mse: 125.3952 - val_loss: 142.1412 - val_mae: 11.3889 - val_mse: 142.1412 - lr: 0.0100\n",
      "\n",
      "Epoch 00143: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 143/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 124.1545 - mae: 10.5548 - mse: 124.1545 - val_loss: 140.7616 - val_mae: 11.3293 - val_mse: 140.7616 - lr: 0.0100\n",
      "\n",
      "Epoch 00144: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 144/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 122.9284 - mae: 10.4978 - mse: 122.9284 - val_loss: 139.4592 - val_mae: 11.2716 - val_mse: 139.4592 - lr: 0.0100\n",
      "\n",
      "Epoch 00145: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 145/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 121.7053 - mae: 10.4399 - mse: 121.7053 - val_loss: 138.1656 - val_mae: 11.2140 - val_mse: 138.1656 - lr: 0.0100\n",
      "\n",
      "Epoch 00146: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 146/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 120.4914 - mae: 10.3820 - mse: 120.4914 - val_loss: 136.8331 - val_mae: 11.1553 - val_mse: 136.8331 - lr: 0.0100\n",
      "\n",
      "Epoch 00147: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 147/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 119.3332 - mae: 10.3267 - mse: 119.3332 - val_loss: 135.4507 - val_mae: 11.0954 - val_mse: 135.4507 - lr: 0.0100\n",
      "\n",
      "Epoch 00148: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 148/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 118.1140 - mae: 10.2696 - mse: 118.1140 - val_loss: 134.1983 - val_mae: 11.0386 - val_mse: 134.1983 - lr: 0.0100\n",
      "\n",
      "Epoch 00149: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 149/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 116.9769 - mae: 10.2138 - mse: 116.9769 - val_loss: 132.9899 - val_mae: 10.9825 - val_mse: 132.9899 - lr: 0.0100\n",
      "\n",
      "Epoch 00150: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 150/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 115.7728 - mae: 10.1552 - mse: 115.7728 - val_loss: 131.7437 - val_mae: 10.9254 - val_mse: 131.7437 - lr: 0.0100\n",
      "\n",
      "Epoch 00151: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 151/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 114.6078 - mae: 10.0982 - mse: 114.6078 - val_loss: 130.5163 - val_mae: 10.8689 - val_mse: 130.5163 - lr: 0.0100\n",
      "\n",
      "Epoch 00152: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 152/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 113.4735 - mae: 10.0417 - mse: 113.4735 - val_loss: 129.2219 - val_mae: 10.8105 - val_mse: 129.2219 - lr: 0.0100\n",
      "\n",
      "Epoch 00153: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 153/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 112.3169 - mae: 9.9860 - mse: 112.3169 - val_loss: 127.9299 - val_mae: 10.7520 - val_mse: 127.9299 - lr: 0.0100\n",
      "\n",
      "Epoch 00154: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 154/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 111.2125 - mae: 9.9315 - mse: 111.2125 - val_loss: 126.6497 - val_mae: 10.6940 - val_mse: 126.6497 - lr: 0.0100\n",
      "\n",
      "Epoch 00155: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 155/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 110.0779 - mae: 9.8747 - mse: 110.0779 - val_loss: 125.4680 - val_mae: 10.6382 - val_mse: 125.4680 - lr: 0.0100\n",
      "\n",
      "Epoch 00156: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 156/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 108.9701 - mae: 9.8187 - mse: 108.9701 - val_loss: 124.2528 - val_mae: 10.5812 - val_mse: 124.2528 - lr: 0.0100\n",
      "\n",
      "Epoch 00157: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 157/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 107.8913 - mae: 9.7642 - mse: 107.8913 - val_loss: 123.0672 - val_mae: 10.5248 - val_mse: 123.0672 - lr: 0.0100\n",
      "\n",
      "Epoch 00158: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 158/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 106.7801 - mae: 9.7083 - mse: 106.7801 - val_loss: 121.8555 - val_mae: 10.4681 - val_mse: 121.8555 - lr: 0.0100\n",
      "\n",
      "Epoch 00159: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 159/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 105.7064 - mae: 9.6538 - mse: 105.7064 - val_loss: 120.6154 - val_mae: 10.4104 - val_mse: 120.6154 - lr: 0.0100\n",
      "\n",
      "Epoch 00160: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 160/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 104.6229 - mae: 9.5980 - mse: 104.6229 - val_loss: 119.4574 - val_mae: 10.3545 - val_mse: 119.4574 - lr: 0.0100\n",
      "\n",
      "Epoch 00161: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 161/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 103.5575 - mae: 9.5437 - mse: 103.5575 - val_loss: 118.3138 - val_mae: 10.2993 - val_mse: 118.3138 - lr: 0.0100\n",
      "\n",
      "Epoch 00162: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 162/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 102.5188 - mae: 9.4892 - mse: 102.5188 - val_loss: 117.2206 - val_mae: 10.2448 - val_mse: 117.2206 - lr: 0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00163: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 163/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 101.4803 - mae: 9.4337 - mse: 101.4803 - val_loss: 116.1239 - val_mae: 10.1902 - val_mse: 116.1239 - lr: 0.0100\n",
      "\n",
      "Epoch 00164: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 164/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 100.4178 - mae: 9.3782 - mse: 100.4178 - val_loss: 114.9232 - val_mae: 10.1334 - val_mse: 114.9232 - lr: 0.0100\n",
      "\n",
      "Epoch 00165: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 165/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 99.3914 - mae: 9.3244 - mse: 99.3914 - val_loss: 113.7689 - val_mae: 10.0775 - val_mse: 113.7689 - lr: 0.0100\n",
      "\n",
      "Epoch 00166: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 166/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 98.3831 - mae: 9.2714 - mse: 98.3831 - val_loss: 112.5898 - val_mae: 10.0208 - val_mse: 112.5898 - lr: 0.0100\n",
      "\n",
      "Epoch 00167: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 167/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 97.3858 - mae: 9.2180 - mse: 97.3858 - val_loss: 111.4261 - val_mae: 9.9640 - val_mse: 111.4261 - lr: 0.0100\n",
      "\n",
      "Epoch 00168: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 168/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 96.3669 - mae: 9.1637 - mse: 96.3669 - val_loss: 110.3818 - val_mae: 9.9107 - val_mse: 110.3818 - lr: 0.0100\n",
      "\n",
      "Epoch 00169: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 169/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 95.3613 - mae: 9.1095 - mse: 95.3613 - val_loss: 109.3217 - val_mae: 9.8570 - val_mse: 109.3217 - lr: 0.0100\n",
      "\n",
      "Epoch 00170: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 170/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 94.3782 - mae: 9.0569 - mse: 94.3782 - val_loss: 108.2705 - val_mae: 9.8034 - val_mse: 108.2705 - lr: 0.0100\n",
      "\n",
      "Epoch 00171: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 171/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 93.4250 - mae: 9.0037 - mse: 93.4250 - val_loss: 107.1893 - val_mae: 9.7485 - val_mse: 107.1893 - lr: 0.0100\n",
      "\n",
      "Epoch 00172: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 172/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 92.4513 - mae: 8.9507 - mse: 92.4513 - val_loss: 106.0944 - val_mae: 9.6934 - val_mse: 106.0944 - lr: 0.0100\n",
      "\n",
      "Epoch 00173: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 173/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 91.5009 - mae: 8.8981 - mse: 91.5009 - val_loss: 105.0129 - val_mae: 9.6388 - val_mse: 105.0129 - lr: 0.0100\n",
      "\n",
      "Epoch 00174: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 174/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 90.5524 - mae: 8.8464 - mse: 90.5524 - val_loss: 104.0150 - val_mae: 9.5860 - val_mse: 104.0150 - lr: 0.0100\n",
      "\n",
      "Epoch 00175: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 175/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 89.6569 - mae: 8.7950 - mse: 89.6569 - val_loss: 103.0085 - val_mae: 9.5328 - val_mse: 103.0085 - lr: 0.0100\n",
      "\n",
      "Epoch 00176: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 176/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 88.6617 - mae: 8.7402 - mse: 88.6617 - val_loss: 101.9224 - val_mae: 9.4779 - val_mse: 101.9224 - lr: 0.0100\n",
      "\n",
      "Epoch 00177: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 177/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 87.7573 - mae: 8.6893 - mse: 87.7573 - val_loss: 100.9865 - val_mae: 9.4273 - val_mse: 100.9865 - lr: 0.0100\n",
      "\n",
      "Epoch 00178: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 178/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 86.8392 - mae: 8.6373 - mse: 86.8392 - val_loss: 99.9046 - val_mae: 9.3718 - val_mse: 99.9046 - lr: 0.0100\n",
      "\n",
      "Epoch 00179: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 179/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 85.9141 - mae: 8.5851 - mse: 85.9141 - val_loss: 98.8918 - val_mae: 9.3185 - val_mse: 98.8918 - lr: 0.0100\n",
      "\n",
      "Epoch 00180: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 180/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 85.0213 - mae: 8.5345 - mse: 85.0213 - val_loss: 97.8995 - val_mae: 9.2658 - val_mse: 97.8995 - lr: 0.0100\n",
      "\n",
      "Epoch 00181: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 181/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 84.1509 - mae: 8.4843 - mse: 84.1509 - val_loss: 96.8911 - val_mae: 9.2125 - val_mse: 96.8911 - lr: 0.0100\n",
      "\n",
      "Epoch 00182: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 182/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 83.2727 - mae: 8.4333 - mse: 83.2727 - val_loss: 95.9931 - val_mae: 9.1622 - val_mse: 95.9931 - lr: 0.0100\n",
      "\n",
      "Epoch 00183: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 183/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 82.4138 - mae: 8.3828 - mse: 82.4138 - val_loss: 94.9063 - val_mae: 9.1061 - val_mse: 94.9063 - lr: 0.0100\n",
      "\n",
      "Epoch 00184: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 184/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 81.5195 - mae: 8.3314 - mse: 81.5195 - val_loss: 93.9481 - val_mae: 9.0538 - val_mse: 93.9481 - lr: 0.0100\n",
      "\n",
      "Epoch 00185: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 185/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 80.6788 - mae: 8.2813 - mse: 80.6788 - val_loss: 93.0134 - val_mae: 9.0019 - val_mse: 93.0134 - lr: 0.0100\n",
      "\n",
      "Epoch 00186: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 186/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 79.8268 - mae: 8.2308 - mse: 79.8268 - val_loss: 92.0623 - val_mae: 8.9499 - val_mse: 92.0623 - lr: 0.0100\n",
      "\n",
      "Epoch 00187: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 187/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 78.9767 - mae: 8.1812 - mse: 78.9767 - val_loss: 91.1039 - val_mae: 8.8973 - val_mse: 91.1039 - lr: 0.0100\n",
      "\n",
      "Epoch 00188: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 188/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 78.1326 - mae: 8.1308 - mse: 78.1326 - val_loss: 90.2202 - val_mae: 8.8470 - val_mse: 90.2202 - lr: 0.0100\n",
      "\n",
      "Epoch 00189: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 189/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 77.3121 - mae: 8.0803 - mse: 77.3121 - val_loss: 89.3437 - val_mae: 8.7967 - val_mse: 89.3437 - lr: 0.0100\n",
      "\n",
      "Epoch 00190: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 190/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 76.4895 - mae: 8.0305 - mse: 76.4895 - val_loss: 88.4586 - val_mae: 8.7460 - val_mse: 88.4586 - lr: 0.0100\n",
      "\n",
      "Epoch 00191: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 191/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 75.7365 - mae: 7.9837 - mse: 75.7365 - val_loss: 87.5264 - val_mae: 8.6942 - val_mse: 87.5264 - lr: 0.0100\n",
      "\n",
      "Epoch 00192: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 192/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 74.8932 - mae: 7.9328 - mse: 74.8932 - val_loss: 86.6211 - val_mae: 8.6425 - val_mse: 86.6211 - lr: 0.0100\n",
      "\n",
      "Epoch 00193: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 193/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 74.0920 - mae: 7.8830 - mse: 74.0920 - val_loss: 85.7138 - val_mae: 8.5909 - val_mse: 85.7138 - lr: 0.0100\n",
      "\n",
      "Epoch 00194: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 194/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 73.3082 - mae: 7.8341 - mse: 73.3082 - val_loss: 84.8679 - val_mae: 8.5411 - val_mse: 84.8679 - lr: 0.0100\n",
      "\n",
      "Epoch 00195: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 195/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 72.5229 - mae: 7.7854 - mse: 72.5229 - val_loss: 84.0422 - val_mae: 8.4922 - val_mse: 84.0422 - lr: 0.0100\n",
      "\n",
      "Epoch 00196: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 196/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 4ms/step - loss: 71.7445 - mae: 7.7358 - mse: 71.7445 - val_loss: 83.1704 - val_mae: 8.4415 - val_mse: 83.1704 - lr: 0.0100\n",
      "\n",
      "Epoch 00197: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 197/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 70.9843 - mae: 7.6883 - mse: 70.9843 - val_loss: 82.3652 - val_mae: 8.3930 - val_mse: 82.3652 - lr: 0.0100\n",
      "\n",
      "Epoch 00198: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 198/200\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 70.2509 - mae: 7.6414 - mse: 70.2509 - val_loss: 81.4628 - val_mae: 8.3413 - val_mse: 81.4628 - lr: 0.0100\n",
      "\n",
      "Epoch 00199: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 199/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 69.4702 - mae: 7.5929 - mse: 69.4702 - val_loss: 80.6324 - val_mae: 8.2918 - val_mse: 80.6324 - lr: 0.0100\n",
      "\n",
      "Epoch 00200: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 200/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 68.7355 - mae: 7.5466 - mse: 68.7355 - val_loss: 79.7967 - val_mae: 8.2422 - val_mse: 79.7967 - lr: 0.0100\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "history = model.fit(train_features, train_labels, epochs=200, validation_split = 0.2,\n",
    "                                callbacks=[lr_callback, es_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Callback N.1\n",
    "\n",
    "Let's write a simple custom callback that logs the loss and metrics values after every batch, epoch, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLogger(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        print(\"Starting training; log content: {}\".format(logs))\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        print(\"Stop training; got log content: {}\".format(logs))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(\"End epoch {} of training; log content: {}\".format(epoch, logs))\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        print(\"...Training: end of batch {}; log content: {}\".format(batch, logs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training; log content: {}\n",
      "...Training: end of batch 0; log content: {'loss': 640.1168212890625, 'mae': 23.924179077148438, 'mse': 640.1168212890625}\n",
      "...Training: end of batch 1; log content: {'loss': 601.4976196289062, 'mae': 23.367958068847656, 'mse': 601.4976196289062}\n",
      "...Training: end of batch 2; log content: {'loss': 603.247802734375, 'mae': 23.63275718688965, 'mse': 603.247802734375}\n",
      "...Training: end of batch 3; log content: {'loss': 586.8490600585938, 'mae': 23.314189910888672, 'mse': 586.8490600585938}\n",
      "...Training: end of batch 4; log content: {'loss': 568.4512939453125, 'mae': 22.980281829833984, 'mse': 568.4512939453125}\n",
      "...Training: end of batch 5; log content: {'loss': 558.9595336914062, 'mae': 22.837554931640625, 'mse': 558.9595336914062}\n",
      "...Training: end of batch 6; log content: {'loss': 556.0956420898438, 'mae': 22.81576919555664, 'mse': 556.0956420898438}\n",
      "...Training: end of batch 7; log content: {'loss': 561.2073974609375, 'mae': 22.94468879699707, 'mse': 561.2073974609375}\n",
      "End epoch 0 of training; log content: {'loss': 561.2073974609375, 'mae': 22.94468879699707, 'mse': 561.2073974609375, 'val_loss': 546.5420532226562, 'val_mae': 22.921232223510742, 'val_mse': 546.5420532226562}\n",
      "...Training: end of batch 0; log content: {'loss': 521.28759765625, 'mae': 22.493099212646484, 'mse': 521.28759765625}\n",
      "...Training: end of batch 1; log content: {'loss': 529.5286865234375, 'mae': 22.552419662475586, 'mse': 529.5286865234375}\n",
      "...Training: end of batch 2; log content: {'loss': 532.2929077148438, 'mae': 22.535682678222656, 'mse': 532.2929077148438}\n",
      "...Training: end of batch 3; log content: {'loss': 531.7648315429688, 'mae': 22.519756317138672, 'mse': 531.7648315429688}\n",
      "...Training: end of batch 4; log content: {'loss': 525.2618408203125, 'mae': 22.39572525024414, 'mse': 525.2618408203125}\n",
      "...Training: end of batch 5; log content: {'loss': 520.2916870117188, 'mae': 22.31141471862793, 'mse': 520.2916870117188}\n",
      "...Training: end of batch 6; log content: {'loss': 512.8848266601562, 'mae': 22.193161010742188, 'mse': 512.8848266601562}\n",
      "...Training: end of batch 7; log content: {'loss': 509.8860168457031, 'mae': 22.152082443237305, 'mse': 509.8860168457031}\n",
      "End epoch 1 of training; log content: {'loss': 509.8860168457031, 'mae': 22.152082443237305, 'mse': 509.8860168457031, 'val_loss': 501.9075012207031, 'val_mae': 22.11319351196289, 'val_mse': 501.9075012207031}\n",
      "...Training: end of batch 0; log content: {'loss': 473.5583801269531, 'mae': 21.547122955322266, 'mse': 473.5583801269531}\n",
      "...Training: end of batch 1; log content: {'loss': 468.87353515625, 'mae': 21.357219696044922, 'mse': 468.87353515625}\n",
      "...Training: end of batch 2; log content: {'loss': 476.8494567871094, 'mae': 21.525392532348633, 'mse': 476.8494567871094}\n",
      "...Training: end of batch 3; log content: {'loss': 472.84124755859375, 'mae': 21.442136764526367, 'mse': 472.84124755859375}\n",
      "...Training: end of batch 4; log content: {'loss': 462.2977600097656, 'mae': 21.169605255126953, 'mse': 462.2977600097656}\n",
      "...Training: end of batch 5; log content: {'loss': 469.1890563964844, 'mae': 21.301002502441406, 'mse': 469.1890563964844}\n",
      "...Training: end of batch 6; log content: {'loss': 471.11944580078125, 'mae': 21.33453941345215, 'mse': 471.11944580078125}\n",
      "...Training: end of batch 7; log content: {'loss': 469.2757568359375, 'mae': 21.29889488220215, 'mse': 469.2757568359375}\n",
      "End epoch 2 of training; log content: {'loss': 469.2757568359375, 'mae': 21.29889488220215, 'mse': 469.2757568359375, 'val_loss': 467.602294921875, 'val_mae': 21.333988189697266, 'val_mse': 467.602294921875}\n",
      "...Training: end of batch 0; log content: {'loss': 478.9859313964844, 'mae': 21.740802764892578, 'mse': 478.9859313964844}\n",
      "...Training: end of batch 1; log content: {'loss': 446.43389892578125, 'mae': 20.83553695678711, 'mse': 446.43389892578125}\n",
      "...Training: end of batch 2; log content: {'loss': 438.3507385253906, 'mae': 20.6197452545166, 'mse': 438.3507385253906}\n",
      "...Training: end of batch 3; log content: {'loss': 441.6624755859375, 'mae': 20.674232482910156, 'mse': 441.6624755859375}\n",
      "...Training: end of batch 4; log content: {'loss': 438.1527404785156, 'mae': 20.591571807861328, 'mse': 438.1527404785156}\n",
      "...Training: end of batch 5; log content: {'loss': 438.8896484375, 'mae': 20.614362716674805, 'mse': 438.8896484375}\n",
      "...Training: end of batch 6; log content: {'loss': 436.70367431640625, 'mae': 20.548946380615234, 'mse': 436.70367431640625}\n",
      "...Training: end of batch 7; log content: {'loss': 436.2064208984375, 'mae': 20.50989532470703, 'mse': 436.2064208984375}\n",
      "End epoch 3 of training; log content: {'loss': 436.2064208984375, 'mae': 20.50989532470703, 'mse': 436.2064208984375, 'val_loss': 436.0570983886719, 'val_mae': 20.5855655670166, 'val_mse': 436.0570983886719}\n",
      "...Training: end of batch 0; log content: {'loss': 410.7190246582031, 'mae': 19.892242431640625, 'mse': 410.7190246582031}\n",
      "...Training: end of batch 1; log content: {'loss': 421.773193359375, 'mae': 20.24920082092285, 'mse': 421.773193359375}\n",
      "...Training: end of batch 2; log content: {'loss': 411.0187072753906, 'mae': 19.99785041809082, 'mse': 411.0187072753906}\n",
      "...Training: end of batch 3; log content: {'loss': 405.6856689453125, 'mae': 19.817834854125977, 'mse': 405.6856689453125}\n",
      "...Training: end of batch 4; log content: {'loss': 408.99822998046875, 'mae': 19.872539520263672, 'mse': 408.99822998046875}\n",
      "...Training: end of batch 5; log content: {'loss': 405.9210510253906, 'mae': 19.804832458496094, 'mse': 405.9210510253906}\n",
      "...Training: end of batch 6; log content: {'loss': 405.309326171875, 'mae': 19.73988151550293, 'mse': 405.309326171875}\n",
      "...Training: end of batch 7; log content: {'loss': 404.9720153808594, 'mae': 19.745107650756836, 'mse': 404.9720153808594}\n",
      "End epoch 4 of training; log content: {'loss': 404.9720153808594, 'mae': 19.745107650756836, 'mse': 404.9720153808594, 'val_loss': 408.247314453125, 'val_mae': 19.898902893066406, 'val_mse': 408.247314453125}\n",
      "...Training: end of batch 0; log content: {'loss': 407.2008361816406, 'mae': 19.783370971679688, 'mse': 407.2008361816406}\n",
      "...Training: end of batch 1; log content: {'loss': 384.3619384765625, 'mae': 19.227222442626953, 'mse': 384.3619384765625}\n",
      "...Training: end of batch 2; log content: {'loss': 383.66357421875, 'mae': 19.270681381225586, 'mse': 383.66357421875}\n",
      "...Training: end of batch 3; log content: {'loss': 380.4130859375, 'mae': 19.137168884277344, 'mse': 380.4130859375}\n",
      "...Training: end of batch 4; log content: {'loss': 382.589111328125, 'mae': 19.15304946899414, 'mse': 382.589111328125}\n",
      "...Training: end of batch 5; log content: {'loss': 382.4768981933594, 'mae': 19.16260528564453, 'mse': 382.4768981933594}\n",
      "...Training: end of batch 6; log content: {'loss': 376.85150146484375, 'mae': 19.028390884399414, 'mse': 376.85150146484375}\n",
      "...Training: end of batch 7; log content: {'loss': 373.7081604003906, 'mae': 18.95045280456543, 'mse': 373.7081604003906}\n",
      "End epoch 5 of training; log content: {'loss': 373.7081604003906, 'mae': 18.95045280456543, 'mse': 373.7081604003906, 'val_loss': 379.5339050292969, 'val_mae': 19.167282104492188, 'val_mse': 379.5339050292969}\n",
      "...Training: end of batch 0; log content: {'loss': 355.40447998046875, 'mae': 18.570924758911133, 'mse': 355.40447998046875}\n",
      "...Training: end of batch 1; log content: {'loss': 364.49560546875, 'mae': 18.81317138671875, 'mse': 364.49560546875}\n",
      "...Training: end of batch 2; log content: {'loss': 355.979248046875, 'mae': 18.603586196899414, 'mse': 355.979248046875}\n",
      "...Training: end of batch 3; log content: {'loss': 358.1134338378906, 'mae': 18.58596420288086, 'mse': 358.1134338378906}\n",
      "...Training: end of batch 4; log content: {'loss': 354.03045654296875, 'mae': 18.489208221435547, 'mse': 354.03045654296875}\n",
      "...Training: end of batch 5; log content: {'loss': 348.5636291503906, 'mae': 18.31413459777832, 'mse': 348.5636291503906}\n",
      "...Training: end of batch 6; log content: {'loss': 343.38232421875, 'mae': 18.15395164489746, 'mse': 343.38232421875}\n",
      "...Training: end of batch 7; log content: {'loss': 345.8172302246094, 'mae': 18.200672149658203, 'mse': 345.8172302246094}\n",
      "End epoch 6 of training; log content: {'loss': 345.8172302246094, 'mae': 18.200672149658203, 'mse': 345.8172302246094, 'val_loss': 354.3877258300781, 'val_mae': 18.48524284362793, 'val_mse': 354.3877258300781}\n",
      "...Training: end of batch 0; log content: {'loss': 321.98968505859375, 'mae': 17.501235961914062, 'mse': 321.98968505859375}\n",
      "...Training: end of batch 1; log content: {'loss': 319.00067138671875, 'mae': 17.524503707885742, 'mse': 319.00067138671875}\n",
      "...Training: end of batch 2; log content: {'loss': 332.3965148925781, 'mae': 17.840866088867188, 'mse': 332.3965148925781}\n",
      "...Training: end of batch 3; log content: {'loss': 332.90875244140625, 'mae': 17.849716186523438, 'mse': 332.90875244140625}\n",
      "...Training: end of batch 4; log content: {'loss': 324.26470947265625, 'mae': 17.60668182373047, 'mse': 324.26470947265625}\n",
      "...Training: end of batch 5; log content: {'loss': 322.6199951171875, 'mae': 17.53495979309082, 'mse': 322.6199951171875}\n",
      "...Training: end of batch 6; log content: {'loss': 319.1618957519531, 'mae': 17.466289520263672, 'mse': 319.1618957519531}\n",
      "...Training: end of batch 7; log content: {'loss': 319.1371154785156, 'mae': 17.45562744140625, 'mse': 319.1371154785156}\n",
      "End epoch 7 of training; log content: {'loss': 319.1371154785156, 'mae': 17.45562744140625, 'mse': 319.1371154785156, 'val_loss': 328.5968017578125, 'val_mae': 17.77509307861328, 'val_mse': 328.5968017578125}\n",
      "...Training: end of batch 0; log content: {'loss': 288.041748046875, 'mae': 16.690738677978516, 'mse': 288.041748046875}\n",
      "...Training: end of batch 1; log content: {'loss': 312.8978576660156, 'mae': 17.263654708862305, 'mse': 312.8978576660156}\n",
      "...Training: end of batch 2; log content: {'loss': 309.0263366699219, 'mae': 17.183807373046875, 'mse': 309.0263366699219}\n",
      "...Training: end of batch 3; log content: {'loss': 298.49420166015625, 'mae': 16.85144805908203, 'mse': 298.49420166015625}\n",
      "...Training: end of batch 4; log content: {'loss': 295.2081604003906, 'mae': 16.734527587890625, 'mse': 295.2081604003906}\n",
      "...Training: end of batch 5; log content: {'loss': 294.7914733886719, 'mae': 16.75594139099121, 'mse': 294.7914733886719}\n",
      "...Training: end of batch 6; log content: {'loss': 294.72869873046875, 'mae': 16.742069244384766, 'mse': 294.72869873046875}\n",
      "...Training: end of batch 7; log content: {'loss': 294.6585693359375, 'mae': 16.74576187133789, 'mse': 294.6585693359375}\n",
      "End epoch 8 of training; log content: {'loss': 294.6585693359375, 'mae': 16.74576187133789, 'mse': 294.6585693359375, 'val_loss': 304.6367492675781, 'val_mae': 17.087387084960938, 'val_mse': 304.6367492675781}\n",
      "...Training: end of batch 0; log content: {'loss': 283.4262390136719, 'mae': 16.51761245727539, 'mse': 283.4262390136719}\n",
      "...Training: end of batch 1; log content: {'loss': 269.3650817871094, 'mae': 16.052562713623047, 'mse': 269.3650817871094}\n",
      "...Training: end of batch 2; log content: {'loss': 263.87548828125, 'mae': 15.8455228805542, 'mse': 263.87548828125}\n",
      "...Training: end of batch 3; log content: {'loss': 269.1131286621094, 'mae': 15.989986419677734, 'mse': 269.1131286621094}\n",
      "...Training: end of batch 4; log content: {'loss': 270.64886474609375, 'mae': 16.043594360351562, 'mse': 270.64886474609375}\n",
      "...Training: end of batch 5; log content: {'loss': 268.7678527832031, 'mae': 15.962635040283203, 'mse': 268.7678527832031}\n",
      "...Training: end of batch 6; log content: {'loss': 271.9452209472656, 'mae': 16.065616607666016, 'mse': 271.9452209472656}\n",
      "...Training: end of batch 7; log content: {'loss': 271.7099914550781, 'mae': 16.05546760559082, 'mse': 271.7099914550781}\n",
      "End epoch 9 of training; log content: {'loss': 271.7099914550781, 'mae': 16.05546760559082, 'mse': 271.7099914550781, 'val_loss': 281.7728271484375, 'val_mae': 16.40814781188965, 'val_mse': 281.7728271484375}\n",
      "...Training: end of batch 0; log content: {'loss': 267.15557861328125, 'mae': 15.939409255981445, 'mse': 267.15557861328125}\n",
      "...Training: end of batch 1; log content: {'loss': 256.8780517578125, 'mae': 15.728631973266602, 'mse': 256.8780517578125}\n",
      "...Training: end of batch 2; log content: {'loss': 257.48193359375, 'mae': 15.683863639831543, 'mse': 257.48193359375}\n",
      "...Training: end of batch 3; log content: {'loss': 255.27395629882812, 'mae': 15.576242446899414, 'mse': 255.27395629882812}\n",
      "...Training: end of batch 4; log content: {'loss': 253.3936309814453, 'mae': 15.469106674194336, 'mse': 253.3936309814453}\n",
      "...Training: end of batch 5; log content: {'loss': 250.5079803466797, 'mae': 15.389054298400879, 'mse': 250.5079803466797}\n",
      "...Training: end of batch 6; log content: {'loss': 251.29331970214844, 'mae': 15.409355163574219, 'mse': 251.29331970214844}\n",
      "...Training: end of batch 7; log content: {'loss': 250.2487030029297, 'mae': 15.372559547424316, 'mse': 250.2487030029297}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End epoch 10 of training; log content: {'loss': 250.2487030029297, 'mae': 15.372559547424316, 'mse': 250.2487030029297, 'val_loss': 259.7608642578125, 'val_mae': 15.729279518127441, 'val_mse': 259.7608642578125}\n",
      "...Training: end of batch 0; log content: {'loss': 219.69247436523438, 'mae': 14.310229301452637, 'mse': 219.69247436523438}\n",
      "...Training: end of batch 1; log content: {'loss': 249.23289489746094, 'mae': 15.20467758178711, 'mse': 249.23289489746094}\n",
      "...Training: end of batch 2; log content: {'loss': 241.60801696777344, 'mae': 14.985664367675781, 'mse': 241.60801696777344}\n",
      "...Training: end of batch 3; log content: {'loss': 240.0593719482422, 'mae': 14.980049133300781, 'mse': 240.0593719482422}\n",
      "...Training: end of batch 4; log content: {'loss': 239.7289581298828, 'mae': 14.99125862121582, 'mse': 239.7289581298828}\n",
      "...Training: end of batch 5; log content: {'loss': 230.0460662841797, 'mae': 14.690673828125, 'mse': 230.0460662841797}\n",
      "...Training: end of batch 6; log content: {'loss': 232.619384765625, 'mae': 14.792834281921387, 'mse': 232.619384765625}\n",
      "...Training: end of batch 7; log content: {'loss': 230.154052734375, 'mae': 14.709092140197754, 'mse': 230.154052734375}\n",
      "End epoch 11 of training; log content: {'loss': 230.154052734375, 'mae': 14.709092140197754, 'mse': 230.154052734375, 'val_loss': 239.61575317382812, 'val_mae': 15.077425956726074, 'val_mse': 239.61575317382812}\n",
      "...Training: end of batch 0; log content: {'loss': 255.44129943847656, 'mae': 15.324597358703613, 'mse': 255.44129943847656}\n",
      "...Training: end of batch 1; log content: {'loss': 226.19752502441406, 'mae': 14.458417892456055, 'mse': 226.19752502441406}\n",
      "...Training: end of batch 2; log content: {'loss': 219.5752716064453, 'mae': 14.327709197998047, 'mse': 219.5752716064453}\n",
      "...Training: end of batch 3; log content: {'loss': 216.91900634765625, 'mae': 14.265390396118164, 'mse': 216.91900634765625}\n",
      "...Training: end of batch 4; log content: {'loss': 213.8046417236328, 'mae': 14.098840713500977, 'mse': 213.8046417236328}\n",
      "...Training: end of batch 5; log content: {'loss': 209.2711639404297, 'mae': 13.974631309509277, 'mse': 209.2711639404297}\n",
      "...Training: end of batch 6; log content: {'loss': 210.15538024902344, 'mae': 13.997983932495117, 'mse': 210.15538024902344}\n",
      "...Training: end of batch 7; log content: {'loss': 211.29971313476562, 'mae': 14.05717945098877, 'mse': 211.29971313476562}\n",
      "End epoch 12 of training; log content: {'loss': 211.29971313476562, 'mae': 14.05717945098877, 'mse': 211.29971313476562, 'val_loss': 221.73744201660156, 'val_mae': 14.463626861572266, 'val_mse': 221.73744201660156}\n",
      "...Training: end of batch 0; log content: {'loss': 182.84451293945312, 'mae': 13.202795028686523, 'mse': 182.84451293945312}\n",
      "...Training: end of batch 1; log content: {'loss': 215.68482971191406, 'mae': 14.225244522094727, 'mse': 215.68482971191406}\n",
      "...Training: end of batch 2; log content: {'loss': 196.9503173828125, 'mae': 13.606337547302246, 'mse': 196.9503173828125}\n",
      "...Training: end of batch 3; log content: {'loss': 202.52725219726562, 'mae': 13.723098754882812, 'mse': 202.52725219726562}\n",
      "...Training: end of batch 4; log content: {'loss': 201.27728271484375, 'mae': 13.656875610351562, 'mse': 201.27728271484375}\n",
      "...Training: end of batch 5; log content: {'loss': 196.3245391845703, 'mae': 13.508613586425781, 'mse': 196.3245391845703}\n",
      "...Training: end of batch 6; log content: {'loss': 194.95028686523438, 'mae': 13.454543113708496, 'mse': 194.95028686523438}\n",
      "...Training: end of batch 7; log content: {'loss': 194.17576599121094, 'mae': 13.430800437927246, 'mse': 194.17576599121094}\n",
      "End epoch 13 of training; log content: {'loss': 194.17576599121094, 'mae': 13.430800437927246, 'mse': 194.17576599121094, 'val_loss': 204.53562927246094, 'val_mae': 13.851820945739746, 'val_mse': 204.53562927246094}\n",
      "...Training: end of batch 0; log content: {'loss': 195.36993408203125, 'mae': 13.472597122192383, 'mse': 195.36993408203125}\n",
      "...Training: end of batch 1; log content: {'loss': 197.65406799316406, 'mae': 13.512669563293457, 'mse': 197.65406799316406}\n",
      "...Training: end of batch 2; log content: {'loss': 205.8296661376953, 'mae': 13.787554740905762, 'mse': 205.8296661376953}\n",
      "...Training: end of batch 3; log content: {'loss': 195.5362548828125, 'mae': 13.445040702819824, 'mse': 195.5362548828125}\n",
      "...Training: end of batch 4; log content: {'loss': 188.0430450439453, 'mae': 13.229217529296875, 'mse': 188.0430450439453}\n",
      "...Training: end of batch 5; log content: {'loss': 182.59495544433594, 'mae': 13.013482093811035, 'mse': 182.59495544433594}\n",
      "...Training: end of batch 6; log content: {'loss': 177.79005432128906, 'mae': 12.802713394165039, 'mse': 177.79005432128906}\n",
      "...Training: end of batch 7; log content: {'loss': 178.2477264404297, 'mae': 12.825504302978516, 'mse': 178.2477264404297}\n",
      "End epoch 14 of training; log content: {'loss': 178.2477264404297, 'mae': 12.825504302978516, 'mse': 178.2477264404297, 'val_loss': 187.60955810546875, 'val_mae': 13.236557006835938, 'val_mse': 187.60955810546875}\n",
      "...Training: end of batch 0; log content: {'loss': 156.96194458007812, 'mae': 12.121326446533203, 'mse': 156.96194458007812}\n",
      "...Training: end of batch 1; log content: {'loss': 171.2829132080078, 'mae': 12.566973686218262, 'mse': 171.2829132080078}\n",
      "...Training: end of batch 2; log content: {'loss': 169.14515686035156, 'mae': 12.431540489196777, 'mse': 169.14515686035156}\n",
      "...Training: end of batch 3; log content: {'loss': 154.9426727294922, 'mae': 11.898212432861328, 'mse': 154.9426727294922}\n",
      "...Training: end of batch 4; log content: {'loss': 153.35873413085938, 'mae': 11.886141777038574, 'mse': 153.35873413085938}\n",
      "...Training: end of batch 5; log content: {'loss': 156.6110382080078, 'mae': 12.021588325500488, 'mse': 156.6110382080078}\n",
      "...Training: end of batch 6; log content: {'loss': 160.58871459960938, 'mae': 12.141596794128418, 'mse': 160.58871459960938}\n",
      "...Training: end of batch 7; log content: {'loss': 162.80752563476562, 'mae': 12.225014686584473, 'mse': 162.80752563476562}\n",
      "End epoch 15 of training; log content: {'loss': 162.80752563476562, 'mae': 12.225014686584473, 'mse': 162.80752563476562, 'val_loss': 172.4579315185547, 'val_mae': 12.651473045349121, 'val_mse': 172.4579315185547}\n",
      "...Training: end of batch 0; log content: {'loss': 181.54934692382812, 'mae': 12.718343734741211, 'mse': 181.54934692382812}\n",
      "...Training: end of batch 1; log content: {'loss': 156.72080993652344, 'mae': 11.911284446716309, 'mse': 156.72080993652344}\n",
      "...Training: end of batch 2; log content: {'loss': 159.66952514648438, 'mae': 12.07351016998291, 'mse': 159.66952514648438}\n",
      "...Training: end of batch 3; log content: {'loss': 155.40731811523438, 'mae': 11.945877075195312, 'mse': 155.40731811523438}\n",
      "...Training: end of batch 4; log content: {'loss': 153.52841186523438, 'mae': 11.931585311889648, 'mse': 153.52841186523438}\n",
      "...Training: end of batch 5; log content: {'loss': 152.4300079345703, 'mae': 11.855036735534668, 'mse': 152.4300079345703}\n",
      "...Training: end of batch 6; log content: {'loss': 144.5894317626953, 'mae': 11.511835098266602, 'mse': 144.5894317626953}\n",
      "...Training: end of batch 7; log content: {'loss': 149.0323486328125, 'mae': 11.647234916687012, 'mse': 149.0323486328125}\n",
      "End epoch 16 of training; log content: {'loss': 149.0323486328125, 'mae': 11.647234916687012, 'mse': 149.0323486328125, 'val_loss': 158.6109161376953, 'val_mae': 12.088574409484863, 'val_mse': 158.6109161376953}\n",
      "...Training: end of batch 0; log content: {'loss': 148.41893005371094, 'mae': 11.282219886779785, 'mse': 148.41893005371094}\n",
      "...Training: end of batch 1; log content: {'loss': 145.9764404296875, 'mae': 11.451850891113281, 'mse': 145.9764404296875}\n",
      "...Training: end of batch 2; log content: {'loss': 141.75408935546875, 'mae': 11.378807067871094, 'mse': 141.75408935546875}\n",
      "...Training: end of batch 3; log content: {'loss': 138.7211456298828, 'mae': 11.226198196411133, 'mse': 138.7211456298828}\n",
      "...Training: end of batch 4; log content: {'loss': 135.42251586914062, 'mae': 11.082844734191895, 'mse': 135.42251586914062}\n",
      "...Training: end of batch 5; log content: {'loss': 135.55615234375, 'mae': 11.065503120422363, 'mse': 135.55615234375}\n",
      "...Training: end of batch 6; log content: {'loss': 134.1217041015625, 'mae': 11.030588150024414, 'mse': 134.1217041015625}\n",
      "...Training: end of batch 7; log content: {'loss': 136.249267578125, 'mae': 11.09259033203125, 'mse': 136.249267578125}\n",
      "End epoch 17 of training; log content: {'loss': 136.249267578125, 'mae': 11.09259033203125, 'mse': 136.249267578125, 'val_loss': 145.75096130371094, 'val_mae': 11.542436599731445, 'val_mse': 145.75096130371094}\n",
      "...Training: end of batch 0; log content: {'loss': 133.8323974609375, 'mae': 10.997974395751953, 'mse': 133.8323974609375}\n",
      "...Training: end of batch 1; log content: {'loss': 136.2080078125, 'mae': 11.041240692138672, 'mse': 136.2080078125}\n",
      "...Training: end of batch 2; log content: {'loss': 131.06114196777344, 'mae': 10.69069766998291, 'mse': 131.06114196777344}\n",
      "...Training: end of batch 3; log content: {'loss': 131.70217895507812, 'mae': 10.771380424499512, 'mse': 131.70217895507812}\n",
      "...Training: end of batch 4; log content: {'loss': 130.02520751953125, 'mae': 10.768470764160156, 'mse': 130.02520751953125}\n",
      "...Training: end of batch 5; log content: {'loss': 129.71116638183594, 'mae': 10.789657592773438, 'mse': 129.71116638183594}\n",
      "...Training: end of batch 6; log content: {'loss': 126.18819427490234, 'mae': 10.599236488342285, 'mse': 126.18819427490234}\n",
      "...Training: end of batch 7; log content: {'loss': 124.58759307861328, 'mae': 10.566314697265625, 'mse': 124.58759307861328}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End epoch 18 of training; log content: {'loss': 124.58759307861328, 'mae': 10.566314697265625, 'mse': 124.58759307861328, 'val_loss': 131.84349060058594, 'val_mae': 10.96011734008789, 'val_mse': 131.84349060058594}\n",
      "...Training: end of batch 0; log content: {'loss': 116.57064819335938, 'mae': 10.363588333129883, 'mse': 116.57064819335938}\n",
      "...Training: end of batch 1; log content: {'loss': 119.81245422363281, 'mae': 10.290020942687988, 'mse': 119.81245422363281}\n",
      "...Training: end of batch 2; log content: {'loss': 112.76578521728516, 'mae': 10.046494483947754, 'mse': 112.76578521728516}\n",
      "...Training: end of batch 3; log content: {'loss': 115.21142578125, 'mae': 10.063802719116211, 'mse': 115.21142578125}\n",
      "...Training: end of batch 4; log content: {'loss': 114.1043930053711, 'mae': 10.024415969848633, 'mse': 114.1043930053711}\n",
      "...Training: end of batch 5; log content: {'loss': 113.20852661132812, 'mae': 9.982121467590332, 'mse': 113.20852661132812}\n",
      "...Training: end of batch 6; log content: {'loss': 111.11211395263672, 'mae': 9.922098159790039, 'mse': 111.11211395263672}\n",
      "...Training: end of batch 7; log content: {'loss': 113.56945037841797, 'mae': 10.042705535888672, 'mse': 113.56945037841797}\n",
      "End epoch 19 of training; log content: {'loss': 113.56945037841797, 'mae': 10.042705535888672, 'mse': 113.56945037841797, 'val_loss': 121.43515014648438, 'val_mae': 10.4644136428833, 'val_mse': 121.4351577758789}\n",
      "...Training: end of batch 0; log content: {'loss': 105.12869262695312, 'mae': 9.555288314819336, 'mse': 105.12869262695312}\n",
      "...Training: end of batch 1; log content: {'loss': 110.22015380859375, 'mae': 9.683014869689941, 'mse': 110.22015380859375}\n",
      "...Training: end of batch 2; log content: {'loss': 102.8720474243164, 'mae': 9.403653144836426, 'mse': 102.8720474243164}\n",
      "...Training: end of batch 3; log content: {'loss': 101.25464630126953, 'mae': 9.384111404418945, 'mse': 101.25464630126953}\n",
      "...Training: end of batch 4; log content: {'loss': 102.83668518066406, 'mae': 9.50607681274414, 'mse': 102.83668518066406}\n",
      "...Training: end of batch 5; log content: {'loss': 101.93448638916016, 'mae': 9.473710060119629, 'mse': 101.93448638916016}\n",
      "...Training: end of batch 6; log content: {'loss': 101.8628921508789, 'mae': 9.491134643554688, 'mse': 101.8628921508789}\n",
      "...Training: end of batch 7; log content: {'loss': 103.2221908569336, 'mae': 9.527326583862305, 'mse': 103.2221908569336}\n",
      "End epoch 20 of training; log content: {'loss': 103.2221908569336, 'mae': 9.527326583862305, 'mse': 103.2221908569336, 'val_loss': 110.49357604980469, 'val_mae': 9.94260025024414, 'val_mse': 110.49357604980469}\n",
      "...Training: end of batch 0; log content: {'loss': 85.73414611816406, 'mae': 8.490856170654297, 'mse': 85.73414611816406}\n",
      "...Training: end of batch 1; log content: {'loss': 97.9444580078125, 'mae': 8.99808406829834, 'mse': 97.9444580078125}\n",
      "...Training: end of batch 2; log content: {'loss': 98.11771392822266, 'mae': 9.153565406799316, 'mse': 98.11771392822266}\n",
      "...Training: end of batch 3; log content: {'loss': 95.0865478515625, 'mae': 9.111917495727539, 'mse': 95.0865478515625}\n",
      "...Training: end of batch 4; log content: {'loss': 94.25924682617188, 'mae': 9.086708068847656, 'mse': 94.25924682617188}\n",
      "...Training: end of batch 5; log content: {'loss': 95.21408081054688, 'mae': 9.093592643737793, 'mse': 95.21408081054688}\n",
      "...Training: end of batch 6; log content: {'loss': 93.38580322265625, 'mae': 9.025708198547363, 'mse': 93.38580322265625}\n",
      "...Training: end of batch 7; log content: {'loss': 94.1776351928711, 'mae': 9.042608261108398, 'mse': 94.1776351928711}\n",
      "End epoch 21 of training; log content: {'loss': 94.1776351928711, 'mae': 9.042608261108398, 'mse': 94.1776351928711, 'val_loss': 101.1422348022461, 'val_mae': 9.456265449523926, 'val_mse': 101.1422348022461}\n",
      "...Training: end of batch 0; log content: {'loss': 78.87012481689453, 'mae': 8.651640892028809, 'mse': 78.87012481689453}\n",
      "...Training: end of batch 1; log content: {'loss': 89.97002410888672, 'mae': 8.97543716430664, 'mse': 89.97002410888672}\n",
      "...Training: end of batch 2; log content: {'loss': 84.34297943115234, 'mae': 8.628835678100586, 'mse': 84.34297943115234}\n",
      "...Training: end of batch 3; log content: {'loss': 84.7352066040039, 'mae': 8.618457794189453, 'mse': 84.7352066040039}\n",
      "...Training: end of batch 4; log content: {'loss': 87.7343521118164, 'mae': 8.714798927307129, 'mse': 87.7343521118164}\n",
      "...Training: end of batch 5; log content: {'loss': 86.24849700927734, 'mae': 8.625346183776855, 'mse': 86.24849700927734}\n",
      "...Training: end of batch 6; log content: {'loss': 84.39781188964844, 'mae': 8.521029472351074, 'mse': 84.39781188964844}\n",
      "...Training: end of batch 7; log content: {'loss': 85.73597717285156, 'mae': 8.571211814880371, 'mse': 85.73597717285156}\n",
      "End epoch 22 of training; log content: {'loss': 85.73597717285156, 'mae': 8.571211814880371, 'mse': 85.73597717285156, 'val_loss': 92.66974639892578, 'val_mae': 8.999483108520508, 'val_mse': 92.66974639892578}\n",
      "...Training: end of batch 0; log content: {'loss': 80.47071838378906, 'mae': 8.179668426513672, 'mse': 80.47071838378906}\n",
      "...Training: end of batch 1; log content: {'loss': 75.33190155029297, 'mae': 7.954807281494141, 'mse': 75.33190155029297}\n",
      "...Training: end of batch 2; log content: {'loss': 80.74324035644531, 'mae': 8.299162864685059, 'mse': 80.74324035644531}\n",
      "...Training: end of batch 3; log content: {'loss': 84.72369384765625, 'mae': 8.526710510253906, 'mse': 84.72369384765625}\n",
      "...Training: end of batch 4; log content: {'loss': 82.47467041015625, 'mae': 8.40020751953125, 'mse': 82.47467041015625}\n",
      "...Training: end of batch 5; log content: {'loss': 79.63755798339844, 'mae': 8.25197696685791, 'mse': 79.63755798339844}\n",
      "...Training: end of batch 6; log content: {'loss': 78.63847351074219, 'mae': 8.188019752502441, 'mse': 78.63847351074219}\n",
      "...Training: end of batch 7; log content: {'loss': 78.0450668334961, 'mae': 8.126936912536621, 'mse': 78.0450668334961}\n",
      "End epoch 23 of training; log content: {'loss': 78.0450668334961, 'mae': 8.126936912536621, 'mse': 78.0450668334961, 'val_loss': 84.24604797363281, 'val_mae': 8.533573150634766, 'val_mse': 84.24604797363281}\n",
      "...Training: end of batch 0; log content: {'loss': 84.2523193359375, 'mae': 8.400816917419434, 'mse': 84.2523193359375}\n",
      "...Training: end of batch 1; log content: {'loss': 70.04435729980469, 'mae': 7.692208290100098, 'mse': 70.04435729980469}\n",
      "...Training: end of batch 2; log content: {'loss': 70.52167510986328, 'mae': 7.684769153594971, 'mse': 70.52167510986328}\n",
      "...Training: end of batch 3; log content: {'loss': 70.15750885009766, 'mae': 7.73895263671875, 'mse': 70.15750885009766}\n",
      "...Training: end of batch 4; log content: {'loss': 72.42411041259766, 'mae': 7.793421745300293, 'mse': 72.42411041259766}\n",
      "...Training: end of batch 5; log content: {'loss': 69.23978424072266, 'mae': 7.607841491699219, 'mse': 69.23978424072266}\n",
      "...Training: end of batch 6; log content: {'loss': 70.75031280517578, 'mae': 7.680053234100342, 'mse': 70.75031280517578}\n",
      "...Training: end of batch 7; log content: {'loss': 71.16384887695312, 'mae': 7.711913585662842, 'mse': 71.16384887695312}\n",
      "End epoch 24 of training; log content: {'loss': 71.16384887695312, 'mae': 7.711913585662842, 'mse': 71.16384887695312, 'val_loss': 76.53712463378906, 'val_mae': 8.083147048950195, 'val_mse': 76.53712463378906}\n",
      "...Training: end of batch 0; log content: {'loss': 58.320281982421875, 'mae': 7.381993293762207, 'mse': 58.320281982421875}\n",
      "...Training: end of batch 1; log content: {'loss': 60.469627380371094, 'mae': 7.300628662109375, 'mse': 60.469627380371094}\n",
      "...Training: end of batch 2; log content: {'loss': 72.51532745361328, 'mae': 7.819036483764648, 'mse': 72.51532745361328}\n",
      "...Training: end of batch 3; log content: {'loss': 68.63168334960938, 'mae': 7.674901008605957, 'mse': 68.63168334960938}\n",
      "...Training: end of batch 4; log content: {'loss': 69.03926086425781, 'mae': 7.666583061218262, 'mse': 69.03926086425781}\n",
      "...Training: end of batch 5; log content: {'loss': 65.545166015625, 'mae': 7.40744161605835, 'mse': 65.545166015625}\n",
      "...Training: end of batch 6; log content: {'loss': 66.81192779541016, 'mae': 7.434853553771973, 'mse': 66.81192779541016}\n",
      "...Training: end of batch 7; log content: {'loss': 64.70088195800781, 'mae': 7.276759147644043, 'mse': 64.70088195800781}\n",
      "End epoch 25 of training; log content: {'loss': 64.70088195800781, 'mae': 7.276759147644043, 'mse': 64.70088195800781, 'val_loss': 70.12905883789062, 'val_mae': 7.668913841247559, 'val_mse': 70.12905883789062}\n",
      "...Training: end of batch 0; log content: {'loss': 66.84671020507812, 'mae': 7.416415214538574, 'mse': 66.84671020507812}\n",
      "...Training: end of batch 1; log content: {'loss': 62.50434494018555, 'mae': 7.102437973022461, 'mse': 62.50434494018555}\n",
      "...Training: end of batch 2; log content: {'loss': 57.30488967895508, 'mae': 6.876012802124023, 'mse': 57.30488967895508}\n",
      "...Training: end of batch 3; log content: {'loss': 56.385345458984375, 'mae': 6.887160778045654, 'mse': 56.385345458984375}\n",
      "...Training: end of batch 4; log content: {'loss': 57.9586067199707, 'mae': 6.892287254333496, 'mse': 57.9586067199707}\n",
      "...Training: end of batch 5; log content: {'loss': 60.47660827636719, 'mae': 7.019455432891846, 'mse': 60.47660827636719}\n",
      "...Training: end of batch 6; log content: {'loss': 61.08771514892578, 'mae': 7.0164618492126465, 'mse': 61.08771514892578}\n",
      "...Training: end of batch 7; log content: {'loss': 58.7831916809082, 'mae': 6.8795647621154785, 'mse': 58.7831916809082}\n",
      "End epoch 26 of training; log content: {'loss': 58.7831916809082, 'mae': 6.8795647621154785, 'mse': 58.7831916809082, 'val_loss': 64.23085021972656, 'val_mae': 7.286190032958984, 'val_mse': 64.23085021972656}\n",
      "...Training: end of batch 0; log content: {'loss': 34.707420349121094, 'mae': 5.306642532348633, 'mse': 34.707420349121094}\n",
      "...Training: end of batch 1; log content: {'loss': 54.93429183959961, 'mae': 6.54270076751709, 'mse': 54.93429183959961}\n",
      "...Training: end of batch 2; log content: {'loss': 54.91990280151367, 'mae': 6.584684371948242, 'mse': 54.91990280151367}\n",
      "...Training: end of batch 3; log content: {'loss': 54.075416564941406, 'mae': 6.5485944747924805, 'mse': 54.075416564941406}\n",
      "...Training: end of batch 4; log content: {'loss': 54.17889404296875, 'mae': 6.573544502258301, 'mse': 54.17889404296875}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Training: end of batch 5; log content: {'loss': 54.75857925415039, 'mae': 6.580890655517578, 'mse': 54.75857925415039}\n",
      "...Training: end of batch 6; log content: {'loss': 54.31869888305664, 'mae': 6.525269985198975, 'mse': 54.31869888305664}\n",
      "...Training: end of batch 7; log content: {'loss': 53.55525207519531, 'mae': 6.506133556365967, 'mse': 53.55525207519531}\n",
      "End epoch 27 of training; log content: {'loss': 53.55525207519531, 'mae': 6.506133556365967, 'mse': 53.55525207519531, 'val_loss': 58.736698150634766, 'val_mae': 6.916505813598633, 'val_mse': 58.736698150634766}\n",
      "...Training: end of batch 0; log content: {'loss': 61.73976135253906, 'mae': 6.933187484741211, 'mse': 61.73976135253906}\n",
      "...Training: end of batch 1; log content: {'loss': 47.07685089111328, 'mae': 6.088812828063965, 'mse': 47.07685089111328}\n",
      "...Training: end of batch 2; log content: {'loss': 48.54850387573242, 'mae': 6.21239709854126, 'mse': 48.54850387573242}\n",
      "...Training: end of batch 3; log content: {'loss': 47.83827209472656, 'mae': 6.110176086425781, 'mse': 47.83827209472656}\n",
      "...Training: end of batch 4; log content: {'loss': 47.277923583984375, 'mae': 6.097744464874268, 'mse': 47.277923583984375}\n",
      "...Training: end of batch 5; log content: {'loss': 48.951480865478516, 'mae': 6.175378322601318, 'mse': 48.951480865478516}\n",
      "...Training: end of batch 6; log content: {'loss': 48.16220474243164, 'mae': 6.134608745574951, 'mse': 48.16220474243164}\n",
      "...Training: end of batch 7; log content: {'loss': 48.96063232421875, 'mae': 6.166076183319092, 'mse': 48.96063232421875}\n",
      "End epoch 28 of training; log content: {'loss': 48.96063232421875, 'mae': 6.166076183319092, 'mse': 48.96063232421875, 'val_loss': 53.36365509033203, 'val_mae': 6.5442633628845215, 'val_mse': 53.36365509033203}\n",
      "...Training: end of batch 0; log content: {'loss': 59.35303497314453, 'mae': 6.518993854522705, 'mse': 59.35303497314453}\n",
      "...Training: end of batch 1; log content: {'loss': 49.134376525878906, 'mae': 6.131951332092285, 'mse': 49.134376525878906}\n",
      "...Training: end of batch 2; log content: {'loss': 42.93026351928711, 'mae': 5.824438571929932, 'mse': 42.93026351928711}\n",
      "...Training: end of batch 3; log content: {'loss': 42.52263641357422, 'mae': 5.780265808105469, 'mse': 42.52263641357422}\n",
      "...Training: end of batch 4; log content: {'loss': 43.42163848876953, 'mae': 5.902630805969238, 'mse': 43.42163848876953}\n",
      "...Training: end of batch 5; log content: {'loss': 43.308624267578125, 'mae': 5.870188236236572, 'mse': 43.308624267578125}\n",
      "...Training: end of batch 6; log content: {'loss': 44.59502410888672, 'mae': 5.878890037536621, 'mse': 44.59502410888672}\n",
      "...Training: end of batch 7; log content: {'loss': 44.74043273925781, 'mae': 5.817495822906494, 'mse': 44.74043273925781}\n",
      "End epoch 29 of training; log content: {'loss': 44.74043273925781, 'mae': 5.817495822906494, 'mse': 44.74043273925781, 'val_loss': 49.275821685791016, 'val_mae': 6.221005439758301, 'val_mse': 49.275821685791016}\n",
      "...Training: end of batch 0; log content: {'loss': 41.297908782958984, 'mae': 5.656214714050293, 'mse': 41.297908782958984}\n",
      "...Training: end of batch 1; log content: {'loss': 46.282867431640625, 'mae': 5.8179216384887695, 'mse': 46.282867431640625}\n",
      "...Training: end of batch 2; log content: {'loss': 45.3795051574707, 'mae': 5.721477031707764, 'mse': 45.3795051574707}\n",
      "...Training: end of batch 3; log content: {'loss': 44.41986083984375, 'mae': 5.683362007141113, 'mse': 44.41986083984375}\n",
      "...Training: end of batch 4; log content: {'loss': 43.73323059082031, 'mae': 5.622153282165527, 'mse': 43.73323059082031}\n",
      "...Training: end of batch 5; log content: {'loss': 40.219261169433594, 'mae': 5.406148910522461, 'mse': 40.219261169433594}\n",
      "...Training: end of batch 6; log content: {'loss': 40.775917053222656, 'mae': 5.485371112823486, 'mse': 40.775917053222656}\n",
      "...Training: end of batch 7; log content: {'loss': 40.881465911865234, 'mae': 5.49697732925415, 'mse': 40.881465911865234}\n",
      "End epoch 30 of training; log content: {'loss': 40.881465911865234, 'mae': 5.49697732925415, 'mse': 40.881465911865234, 'val_loss': 45.29133224487305, 'val_mae': 5.89756965637207, 'val_mse': 45.29133224487305}\n",
      "...Training: end of batch 0; log content: {'loss': 39.23514175415039, 'mae': 5.800128936767578, 'mse': 39.23514175415039}\n",
      "...Training: end of batch 1; log content: {'loss': 39.909873962402344, 'mae': 5.719675064086914, 'mse': 39.909873962402344}\n",
      "...Training: end of batch 2; log content: {'loss': 41.82497024536133, 'mae': 5.731974124908447, 'mse': 41.82497024536133}\n",
      "...Training: end of batch 3; log content: {'loss': 39.245277404785156, 'mae': 5.480713844299316, 'mse': 39.245277404785156}\n",
      "...Training: end of batch 4; log content: {'loss': 38.577293395996094, 'mae': 5.396719932556152, 'mse': 38.577293395996094}\n",
      "...Training: end of batch 5; log content: {'loss': 37.412601470947266, 'mae': 5.309113502502441, 'mse': 37.412601470947266}\n",
      "...Training: end of batch 6; log content: {'loss': 38.716819763183594, 'mae': 5.3013224601745605, 'mse': 38.716819763183594}\n",
      "...Training: end of batch 7; log content: {'loss': 37.39976119995117, 'mae': 5.200077533721924, 'mse': 37.39976119995117}\n",
      "End epoch 31 of training; log content: {'loss': 37.39976119995117, 'mae': 5.200077533721924, 'mse': 37.39976119995117, 'val_loss': 41.608001708984375, 'val_mae': 5.5866923332214355, 'val_mse': 41.608001708984375}\n",
      "...Training: end of batch 0; log content: {'loss': 29.52545928955078, 'mae': 4.798462390899658, 'mse': 29.52545928955078}\n",
      "...Training: end of batch 1; log content: {'loss': 25.388599395751953, 'mae': 4.407809257507324, 'mse': 25.388599395751953}\n",
      "...Training: end of batch 2; log content: {'loss': 26.424619674682617, 'mae': 4.415608882904053, 'mse': 26.424619674682617}\n",
      "...Training: end of batch 3; log content: {'loss': 30.931556701660156, 'mae': 4.709690093994141, 'mse': 30.931556701660156}\n",
      "...Training: end of batch 4; log content: {'loss': 31.599273681640625, 'mae': 4.801148414611816, 'mse': 31.599273681640625}\n",
      "...Training: end of batch 5; log content: {'loss': 32.858760833740234, 'mae': 4.883398056030273, 'mse': 32.858760833740234}\n",
      "...Training: end of batch 6; log content: {'loss': 32.611812591552734, 'mae': 4.873897075653076, 'mse': 32.611812591552734}\n",
      "...Training: end of batch 7; log content: {'loss': 34.223350524902344, 'mae': 4.941536903381348, 'mse': 34.223350524902344}\n",
      "End epoch 32 of training; log content: {'loss': 34.223350524902344, 'mae': 4.941536903381348, 'mse': 34.223350524902344, 'val_loss': 37.942996978759766, 'val_mae': 5.2860822677612305, 'val_mse': 37.942996978759766}\n",
      "...Training: end of batch 0; log content: {'loss': 20.31998062133789, 'mae': 3.959240674972534, 'mse': 20.31998062133789}\n",
      "...Training: end of batch 1; log content: {'loss': 23.1448917388916, 'mae': 4.113453388214111, 'mse': 23.1448917388916}\n",
      "...Training: end of batch 2; log content: {'loss': 33.29613494873047, 'mae': 4.667175769805908, 'mse': 33.29613494873047}\n",
      "...Training: end of batch 3; log content: {'loss': 34.1708984375, 'mae': 4.670149803161621, 'mse': 34.1708984375}\n",
      "...Training: end of batch 4; log content: {'loss': 33.928443908691406, 'mae': 4.726381301879883, 'mse': 33.928443908691406}\n",
      "...Training: end of batch 5; log content: {'loss': 33.2402229309082, 'mae': 4.744304656982422, 'mse': 33.2402229309082}\n",
      "...Training: end of batch 6; log content: {'loss': 32.4710693359375, 'mae': 4.712314605712891, 'mse': 32.4710693359375}\n",
      "...Training: end of batch 7; log content: {'loss': 31.600383758544922, 'mae': 4.6899943351745605, 'mse': 31.600383758544922}\n",
      "End epoch 33 of training; log content: {'loss': 31.600383758544922, 'mae': 4.6899943351745605, 'mse': 31.600383758544922, 'val_loss': 34.784271240234375, 'val_mae': 5.004475116729736, 'val_mse': 34.784271240234375}\n",
      "...Training: end of batch 0; log content: {'loss': 21.085582733154297, 'mae': 4.149670600891113, 'mse': 21.085582733154297}\n",
      "...Training: end of batch 1; log content: {'loss': 21.485197067260742, 'mae': 4.003276824951172, 'mse': 21.485197067260742}\n",
      "...Training: end of batch 2; log content: {'loss': 21.92591094970703, 'mae': 3.9977197647094727, 'mse': 21.92591094970703}\n",
      "...Training: end of batch 3; log content: {'loss': 24.643218994140625, 'mae': 4.17233419418335, 'mse': 24.643218994140625}\n",
      "...Training: end of batch 4; log content: {'loss': 25.633438110351562, 'mae': 4.208058834075928, 'mse': 25.633438110351562}\n",
      "...Training: end of batch 5; log content: {'loss': 26.32474708557129, 'mae': 4.295172691345215, 'mse': 26.32474708557129}\n",
      "...Training: end of batch 6; log content: {'loss': 27.15816879272461, 'mae': 4.36937952041626, 'mse': 27.15816879272461}\n",
      "...Training: end of batch 7; log content: {'loss': 29.056724548339844, 'mae': 4.451137065887451, 'mse': 29.056724548339844}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End epoch 34 of training; log content: {'loss': 29.056724548339844, 'mae': 4.451137065887451, 'mse': 29.056724548339844, 'val_loss': 32.411991119384766, 'val_mae': 4.766209125518799, 'val_mse': 32.411991119384766}\n",
      "...Training: end of batch 0; log content: {'loss': 21.57598114013672, 'mae': 3.6450138092041016, 'mse': 21.57598114013672}\n",
      "...Training: end of batch 1; log content: {'loss': 19.100364685058594, 'mae': 3.505295753479004, 'mse': 19.100364685058594}\n",
      "...Training: end of batch 2; log content: {'loss': 25.434341430664062, 'mae': 3.9524612426757812, 'mse': 25.434341430664062}\n",
      "...Training: end of batch 3; log content: {'loss': 27.3979549407959, 'mae': 4.21675968170166, 'mse': 27.3979549407959}\n",
      "...Training: end of batch 4; log content: {'loss': 26.446508407592773, 'mae': 4.189908027648926, 'mse': 26.446508407592773}\n",
      "...Training: end of batch 5; log content: {'loss': 27.263036727905273, 'mae': 4.226528167724609, 'mse': 27.263036727905273}\n",
      "...Training: end of batch 6; log content: {'loss': 26.49639320373535, 'mae': 4.2322916984558105, 'mse': 26.49639320373535}\n",
      "...Training: end of batch 7; log content: {'loss': 26.876646041870117, 'mae': 4.231285572052002, 'mse': 26.876646041870117}\n",
      "End epoch 35 of training; log content: {'loss': 26.876646041870117, 'mae': 4.231285572052002, 'mse': 26.876646041870117, 'val_loss': 29.703367233276367, 'val_mae': 4.510553359985352, 'val_mse': 29.703367233276367}\n",
      "...Training: end of batch 0; log content: {'loss': 23.046524047851562, 'mae': 3.937157392501831, 'mse': 23.046524047851562}\n",
      "...Training: end of batch 1; log content: {'loss': 33.205562591552734, 'mae': 4.637399196624756, 'mse': 33.205562591552734}\n",
      "...Training: end of batch 2; log content: {'loss': 31.883209228515625, 'mae': 4.521618366241455, 'mse': 31.883209228515625}\n",
      "...Training: end of batch 3; log content: {'loss': 30.59588623046875, 'mae': 4.396895885467529, 'mse': 30.59588623046875}\n",
      "...Training: end of batch 4; log content: {'loss': 28.137470245361328, 'mae': 4.213094711303711, 'mse': 28.137470245361328}\n",
      "...Training: end of batch 5; log content: {'loss': 26.753084182739258, 'mae': 4.1017045974731445, 'mse': 26.753084182739258}\n",
      "...Training: end of batch 6; log content: {'loss': 25.84694480895996, 'mae': 4.0644402503967285, 'mse': 25.84694480895996}\n",
      "...Training: end of batch 7; log content: {'loss': 24.981889724731445, 'mae': 4.0383501052856445, 'mse': 24.981889724731445}\n",
      "End epoch 36 of training; log content: {'loss': 24.981889724731445, 'mae': 4.0383501052856445, 'mse': 24.981889724731445, 'val_loss': 27.334693908691406, 'val_mae': 4.279271125793457, 'val_mse': 27.334693908691406}\n",
      "...Training: end of batch 0; log content: {'loss': 21.529415130615234, 'mae': 3.8092193603515625, 'mse': 21.529415130615234}\n",
      "...Training: end of batch 1; log content: {'loss': 24.149415969848633, 'mae': 3.919050455093384, 'mse': 24.149415969848633}\n",
      "...Training: end of batch 2; log content: {'loss': 26.84039878845215, 'mae': 4.066072940826416, 'mse': 26.84039878845215}\n",
      "...Training: end of batch 3; log content: {'loss': 25.4443359375, 'mae': 4.044634819030762, 'mse': 25.4443359375}\n",
      "...Training: end of batch 4; log content: {'loss': 26.012969970703125, 'mae': 4.071351528167725, 'mse': 26.012969970703125}\n",
      "...Training: end of batch 5; log content: {'loss': 24.37572479248047, 'mae': 3.9306843280792236, 'mse': 24.37572479248047}\n",
      "...Training: end of batch 6; log content: {'loss': 23.616607666015625, 'mae': 3.8865408897399902, 'mse': 23.616607666015625}\n",
      "...Training: end of batch 7; log content: {'loss': 23.262950897216797, 'mae': 3.876267433166504, 'mse': 23.262950897216797}\n",
      "End epoch 37 of training; log content: {'loss': 23.262950897216797, 'mae': 3.876267433166504, 'mse': 23.262950897216797, 'val_loss': 25.411043167114258, 'val_mae': 4.074682712554932, 'val_mse': 25.411043167114258}\n",
      "...Training: end of batch 0; log content: {'loss': 21.47498321533203, 'mae': 3.6238369941711426, 'mse': 21.47498321533203}\n",
      "...Training: end of batch 1; log content: {'loss': 20.012495040893555, 'mae': 3.4197299480438232, 'mse': 20.012495040893555}\n",
      "...Training: end of batch 2; log content: {'loss': 19.677268981933594, 'mae': 3.4421470165252686, 'mse': 19.677268981933594}\n",
      "...Training: end of batch 3; log content: {'loss': 21.49161148071289, 'mae': 3.6182608604431152, 'mse': 21.49161148071289}\n",
      "...Training: end of batch 4; log content: {'loss': 22.71211814880371, 'mae': 3.7370712757110596, 'mse': 22.71211814880371}\n",
      "...Training: end of batch 5; log content: {'loss': 22.960927963256836, 'mae': 3.813481092453003, 'mse': 22.960927963256836}\n",
      "...Training: end of batch 6; log content: {'loss': 22.121164321899414, 'mae': 3.761828660964966, 'mse': 22.121164321899414}\n",
      "...Training: end of batch 7; log content: {'loss': 21.729494094848633, 'mae': 3.7151455879211426, 'mse': 21.729494094848633}\n",
      "End epoch 38 of training; log content: {'loss': 21.729494094848633, 'mae': 3.7151455879211426, 'mse': 21.729494094848633, 'val_loss': 23.56021499633789, 'val_mae': 3.864229202270508, 'val_mse': 23.56021499633789}\n",
      "...Training: end of batch 0; log content: {'loss': 31.094532012939453, 'mae': 4.20921516418457, 'mse': 31.094532012939453}\n",
      "...Training: end of batch 1; log content: {'loss': 30.5592041015625, 'mae': 4.239092826843262, 'mse': 30.5592041015625}\n",
      "...Training: end of batch 2; log content: {'loss': 24.9940185546875, 'mae': 3.9071216583251953, 'mse': 24.9940185546875}\n",
      "...Training: end of batch 3; log content: {'loss': 23.097776412963867, 'mae': 3.8291423320770264, 'mse': 23.097776412963867}\n",
      "...Training: end of batch 4; log content: {'loss': 22.607370376586914, 'mae': 3.7920608520507812, 'mse': 22.607370376586914}\n",
      "...Training: end of batch 5; log content: {'loss': 22.812902450561523, 'mae': 3.788667917251587, 'mse': 22.812902450561523}\n",
      "...Training: end of batch 6; log content: {'loss': 21.209489822387695, 'mae': 3.650592088699341, 'mse': 21.209489822387695}\n",
      "...Training: end of batch 7; log content: {'loss': 20.396881103515625, 'mae': 3.560835361480713, 'mse': 20.396881103515625}\n",
      "End epoch 39 of training; log content: {'loss': 20.396881103515625, 'mae': 3.560835361480713, 'mse': 20.396881103515625, 'val_loss': 22.158090591430664, 'val_mae': 3.6953518390655518, 'val_mse': 22.158090591430664}\n",
      "...Training: end of batch 0; log content: {'loss': 16.34657096862793, 'mae': 3.3699769973754883, 'mse': 16.34657096862793}\n",
      "...Training: end of batch 1; log content: {'loss': 17.160785675048828, 'mae': 3.407402276992798, 'mse': 17.160785675048828}\n",
      "...Training: end of batch 2; log content: {'loss': 21.250638961791992, 'mae': 3.7380502223968506, 'mse': 21.250638961791992}\n",
      "...Training: end of batch 3; log content: {'loss': 22.353012084960938, 'mae': 3.6727511882781982, 'mse': 22.353012084960938}\n",
      "...Training: end of batch 4; log content: {'loss': 19.902997970581055, 'mae': 3.4507040977478027, 'mse': 19.902997970581055}\n",
      "...Training: end of batch 5; log content: {'loss': 19.638460159301758, 'mae': 3.45766544342041, 'mse': 19.638460159301758}\n",
      "...Training: end of batch 6; log content: {'loss': 19.99966049194336, 'mae': 3.4943671226501465, 'mse': 19.99966049194336}\n",
      "...Training: end of batch 7; log content: {'loss': 19.219242095947266, 'mae': 3.430840492248535, 'mse': 19.219242095947266}\n",
      "End epoch 40 of training; log content: {'loss': 19.219242095947266, 'mae': 3.430840492248535, 'mse': 19.219242095947266, 'val_loss': 20.884862899780273, 'val_mae': 3.5513877868652344, 'val_mse': 20.884862899780273}\n",
      "...Training: end of batch 0; log content: {'loss': 17.574954986572266, 'mae': 3.535384178161621, 'mse': 17.574954986572266}\n",
      "...Training: end of batch 1; log content: {'loss': 16.9361515045166, 'mae': 3.354609251022339, 'mse': 16.9361515045166}\n",
      "...Training: end of batch 2; log content: {'loss': 17.291349411010742, 'mae': 3.357379674911499, 'mse': 17.291349411010742}\n",
      "...Training: end of batch 3; log content: {'loss': 15.898383140563965, 'mae': 3.2315170764923096, 'mse': 15.898383140563965}\n",
      "...Training: end of batch 4; log content: {'loss': 16.806432723999023, 'mae': 3.2779297828674316, 'mse': 16.806432723999023}\n",
      "...Training: end of batch 5; log content: {'loss': 15.474356651306152, 'mae': 3.1443498134613037, 'mse': 15.474356651306152}\n",
      "...Training: end of batch 6; log content: {'loss': 16.59667205810547, 'mae': 3.2286899089813232, 'mse': 16.59667205810547}\n",
      "...Training: end of batch 7; log content: {'loss': 18.165224075317383, 'mae': 3.3134822845458984, 'mse': 18.165224075317383}\n",
      "End epoch 41 of training; log content: {'loss': 18.165224075317383, 'mae': 3.3134822845458984, 'mse': 18.165224075317383, 'val_loss': 19.719226837158203, 'val_mae': 3.416217803955078, 'val_mse': 19.719226837158203}\n",
      "...Training: end of batch 0; log content: {'loss': 8.589508056640625, 'mae': 2.4297423362731934, 'mse': 8.589508056640625}\n",
      "...Training: end of batch 1; log content: {'loss': 13.68461799621582, 'mae': 2.9526801109313965, 'mse': 13.68461799621582}\n",
      "...Training: end of batch 2; log content: {'loss': 18.172941207885742, 'mae': 3.1643993854522705, 'mse': 18.172941207885742}\n",
      "...Training: end of batch 3; log content: {'loss': 16.912385940551758, 'mae': 3.091459274291992, 'mse': 16.912385940551758}\n",
      "...Training: end of batch 4; log content: {'loss': 16.65555191040039, 'mae': 3.143648624420166, 'mse': 16.65555191040039}\n",
      "...Training: end of batch 5; log content: {'loss': 16.3644962310791, 'mae': 3.15701961517334, 'mse': 16.3644962310791}\n",
      "...Training: end of batch 6; log content: {'loss': 16.36945152282715, 'mae': 3.153083324432373, 'mse': 16.36945152282715}\n",
      "...Training: end of batch 7; log content: {'loss': 17.297029495239258, 'mae': 3.2213900089263916, 'mse': 17.297029495239258}\n",
      "End epoch 42 of training; log content: {'loss': 17.297029495239258, 'mae': 3.2213900089263916, 'mse': 17.297029495239258, 'val_loss': 18.547388076782227, 'val_mae': 3.297886848449707, 'val_mse': 18.547388076782227}\n",
      "...Training: end of batch 0; log content: {'loss': 14.421594619750977, 'mae': 3.086348295211792, 'mse': 14.421594619750977}\n",
      "...Training: end of batch 1; log content: {'loss': 16.891498565673828, 'mae': 3.320746421813965, 'mse': 16.891498565673828}\n",
      "...Training: end of batch 2; log content: {'loss': 17.118019104003906, 'mae': 3.3303298950195312, 'mse': 17.118019104003906}\n",
      "...Training: end of batch 3; log content: {'loss': 19.979034423828125, 'mae': 3.4289400577545166, 'mse': 19.979034423828125}\n",
      "...Training: end of batch 4; log content: {'loss': 18.666378021240234, 'mae': 3.3500964641571045, 'mse': 18.666378021240234}\n",
      "...Training: end of batch 5; log content: {'loss': 17.870134353637695, 'mae': 3.2705533504486084, 'mse': 17.870134353637695}\n",
      "...Training: end of batch 6; log content: {'loss': 17.737110137939453, 'mae': 3.2593040466308594, 'mse': 17.737110137939453}\n",
      "...Training: end of batch 7; log content: {'loss': 16.5654296875, 'mae': 3.128066062927246, 'mse': 16.5654296875}\n",
      "End epoch 43 of training; log content: {'loss': 16.5654296875, 'mae': 3.128066062927246, 'mse': 16.5654296875, 'val_loss': 17.60850715637207, 'val_mae': 3.1847548484802246, 'val_mse': 17.60850715637207}\n",
      "...Training: end of batch 0; log content: {'loss': 19.217998504638672, 'mae': 3.4534459114074707, 'mse': 19.217998504638672}\n",
      "...Training: end of batch 1; log content: {'loss': 16.48981475830078, 'mae': 3.2356696128845215, 'mse': 16.48981475830078}\n",
      "...Training: end of batch 2; log content: {'loss': 16.4389705657959, 'mae': 3.161130905151367, 'mse': 16.4389705657959}\n",
      "...Training: end of batch 3; log content: {'loss': 15.804486274719238, 'mae': 3.1762073040008545, 'mse': 15.804486274719238}\n",
      "...Training: end of batch 4; log content: {'loss': 14.671493530273438, 'mae': 3.0318236351013184, 'mse': 14.671493530273438}\n",
      "...Training: end of batch 5; log content: {'loss': 14.58597469329834, 'mae': 3.0078155994415283, 'mse': 14.58597469329834}\n",
      "...Training: end of batch 6; log content: {'loss': 15.496350288391113, 'mae': 3.0153615474700928, 'mse': 15.496350288391113}\n",
      "...Training: end of batch 7; log content: {'loss': 15.798630714416504, 'mae': 3.032130002975464, 'mse': 15.798630714416504}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End epoch 44 of training; log content: {'loss': 15.798630714416504, 'mae': 3.032130002975464, 'mse': 15.798630714416504, 'val_loss': 16.72330093383789, 'val_mae': 3.077984571456909, 'val_mse': 16.72330093383789}\n",
      "...Training: end of batch 0; log content: {'loss': 11.792201042175293, 'mae': 2.804126024246216, 'mse': 11.792201042175293}\n",
      "...Training: end of batch 1; log content: {'loss': 11.98503303527832, 'mae': 2.845733165740967, 'mse': 11.98503303527832}\n",
      "...Training: end of batch 2; log content: {'loss': 16.22673988342285, 'mae': 2.99578857421875, 'mse': 16.22673988342285}\n",
      "...Training: end of batch 3; log content: {'loss': 17.248491287231445, 'mae': 3.1394951343536377, 'mse': 17.248491287231445}\n",
      "...Training: end of batch 4; log content: {'loss': 16.820524215698242, 'mae': 3.121908187866211, 'mse': 16.820524215698242}\n",
      "...Training: end of batch 5; log content: {'loss': 15.829340934753418, 'mae': 3.031987190246582, 'mse': 15.829340934753418}\n",
      "...Training: end of batch 6; log content: {'loss': 14.535832405090332, 'mae': 2.903369426727295, 'mse': 14.535832405090332}\n",
      "...Training: end of batch 7; log content: {'loss': 15.198016166687012, 'mae': 2.9645841121673584, 'mse': 15.198016166687012}\n",
      "End epoch 45 of training; log content: {'loss': 15.198016166687012, 'mae': 2.9645841121673584, 'mse': 15.198016166687012, 'val_loss': 15.923418998718262, 'val_mae': 2.9948086738586426, 'val_mse': 15.923418998718262}\n",
      "...Training: end of batch 0; log content: {'loss': 15.8660888671875, 'mae': 2.9389162063598633, 'mse': 15.8660888671875}\n",
      "...Training: end of batch 1; log content: {'loss': 14.807859420776367, 'mae': 2.927185297012329, 'mse': 14.807859420776367}\n",
      "...Training: end of batch 2; log content: {'loss': 16.681236267089844, 'mae': 3.121913194656372, 'mse': 16.681236267089844}\n",
      "...Training: end of batch 3; log content: {'loss': 17.106889724731445, 'mae': 3.1733767986297607, 'mse': 17.106889724731445}\n",
      "...Training: end of batch 4; log content: {'loss': 16.44496726989746, 'mae': 3.123283863067627, 'mse': 16.44496726989746}\n",
      "...Training: end of batch 5; log content: {'loss': 15.037891387939453, 'mae': 2.9566166400909424, 'mse': 15.037891387939453}\n",
      "...Training: end of batch 6; log content: {'loss': 14.648595809936523, 'mae': 2.9107532501220703, 'mse': 14.648595809936523}\n",
      "...Training: end of batch 7; log content: {'loss': 14.675057411193848, 'mae': 2.9096450805664062, 'mse': 14.675057411193848}\n",
      "End epoch 46 of training; log content: {'loss': 14.675057411193848, 'mae': 2.9096450805664062, 'mse': 14.675057411193848, 'val_loss': 15.184245109558105, 'val_mae': 2.9132792949676514, 'val_mse': 15.184245109558105}\n",
      "...Training: end of batch 0; log content: {'loss': 26.498851776123047, 'mae': 3.907179832458496, 'mse': 26.498851776123047}\n",
      "...Training: end of batch 1; log content: {'loss': 17.892337799072266, 'mae': 3.1292834281921387, 'mse': 17.892337799072266}\n",
      "...Training: end of batch 2; log content: {'loss': 19.16596794128418, 'mae': 3.334637403488159, 'mse': 19.16596794128418}\n",
      "...Training: end of batch 3; log content: {'loss': 17.553783416748047, 'mae': 3.2774124145507812, 'mse': 17.553783416748047}\n",
      "...Training: end of batch 4; log content: {'loss': 16.148418426513672, 'mae': 3.105839967727661, 'mse': 16.148418426513672}\n",
      "...Training: end of batch 5; log content: {'loss': 15.684866905212402, 'mae': 3.0318257808685303, 'mse': 15.684866905212402}\n",
      "...Training: end of batch 6; log content: {'loss': 15.134196281433105, 'mae': 2.960918426513672, 'mse': 15.134196281433105}\n",
      "...Training: end of batch 7; log content: {'loss': 14.242791175842285, 'mae': 2.85917592048645, 'mse': 14.242791175842285}\n",
      "End epoch 47 of training; log content: {'loss': 14.242791175842285, 'mae': 2.85917592048645, 'mse': 14.242791175842285, 'val_loss': 14.606598854064941, 'val_mae': 2.8488545417785645, 'val_mse': 14.606598854064941}\n",
      "...Training: end of batch 0; log content: {'loss': 12.873814582824707, 'mae': 2.532942771911621, 'mse': 12.873814582824707}\n",
      "...Training: end of batch 1; log content: {'loss': 14.380402565002441, 'mae': 2.652568817138672, 'mse': 14.380402565002441}\n",
      "...Training: end of batch 2; log content: {'loss': 14.163012504577637, 'mae': 2.77685809135437, 'mse': 14.163012504577637}\n",
      "...Training: end of batch 3; log content: {'loss': 13.079959869384766, 'mae': 2.7255032062530518, 'mse': 13.079959869384766}\n",
      "...Training: end of batch 4; log content: {'loss': 14.179952621459961, 'mae': 2.797363758087158, 'mse': 14.179952621459961}\n",
      "...Training: end of batch 5; log content: {'loss': 14.275703430175781, 'mae': 2.800272226333618, 'mse': 14.275703430175781}\n",
      "...Training: end of batch 6; log content: {'loss': 14.052931785583496, 'mae': 2.8189265727996826, 'mse': 14.052931785583496}\n",
      "...Training: end of batch 7; log content: {'loss': 13.849021911621094, 'mae': 2.811868190765381, 'mse': 13.849021911621094}\n",
      "End epoch 48 of training; log content: {'loss': 13.849021911621094, 'mae': 2.811868190765381, 'mse': 13.849021911621094, 'val_loss': 14.066259384155273, 'val_mae': 2.8091421127319336, 'val_mse': 14.066259384155273}\n",
      "...Training: end of batch 0; log content: {'loss': 13.828900337219238, 'mae': 2.8516130447387695, 'mse': 13.828900337219238}\n",
      "...Training: end of batch 1; log content: {'loss': 13.382469177246094, 'mae': 2.851780414581299, 'mse': 13.382469177246094}\n",
      "...Training: end of batch 2; log content: {'loss': 11.947821617126465, 'mae': 2.733989715576172, 'mse': 11.947821617126465}\n",
      "...Training: end of batch 3; log content: {'loss': 11.787457466125488, 'mae': 2.7334091663360596, 'mse': 11.787457466125488}\n",
      "...Training: end of batch 4; log content: {'loss': 11.151910781860352, 'mae': 2.6133627891540527, 'mse': 11.151910781860352}\n",
      "...Training: end of batch 5; log content: {'loss': 12.995925903320312, 'mae': 2.735579490661621, 'mse': 12.995925903320312}\n",
      "...Training: end of batch 6; log content: {'loss': 13.712103843688965, 'mae': 2.7821247577667236, 'mse': 13.712103843688965}\n",
      "...Training: end of batch 7; log content: {'loss': 13.498939514160156, 'mae': 2.749533176422119, 'mse': 13.498939514160156}\n",
      "End epoch 49 of training; log content: {'loss': 13.498939514160156, 'mae': 2.749533176422119, 'mse': 13.498939514160156, 'val_loss': 13.810370445251465, 'val_mae': 2.7828080654144287, 'val_mse': 13.810370445251465}\n",
      "...Training: end of batch 0; log content: {'loss': 10.361690521240234, 'mae': 2.5984554290771484, 'mse': 10.361690521240234}\n",
      "...Training: end of batch 1; log content: {'loss': 11.506553649902344, 'mae': 2.6739444732666016, 'mse': 11.506553649902344}\n",
      "...Training: end of batch 2; log content: {'loss': 15.316573143005371, 'mae': 3.092113733291626, 'mse': 15.316573143005371}\n",
      "...Training: end of batch 3; log content: {'loss': 13.677973747253418, 'mae': 2.8747453689575195, 'mse': 13.677973747253418}\n",
      "...Training: end of batch 4; log content: {'loss': 13.913592338562012, 'mae': 2.8531343936920166, 'mse': 13.913592338562012}\n",
      "...Training: end of batch 5; log content: {'loss': 14.124407768249512, 'mae': 2.8120362758636475, 'mse': 14.124407768249512}\n",
      "...Training: end of batch 6; log content: {'loss': 12.987168312072754, 'mae': 2.700573444366455, 'mse': 12.987168312072754}\n",
      "...Training: end of batch 7; log content: {'loss': 13.249703407287598, 'mae': 2.721142292022705, 'mse': 13.249703407287598}\n",
      "End epoch 50 of training; log content: {'loss': 13.249703407287598, 'mae': 2.721142292022705, 'mse': 13.249703407287598, 'val_loss': 13.388016700744629, 'val_mae': 2.748676061630249, 'val_mse': 13.388016700744629}\n",
      "...Training: end of batch 0; log content: {'loss': 5.86165714263916, 'mae': 1.8673624992370605, 'mse': 5.86165714263916}\n",
      "...Training: end of batch 1; log content: {'loss': 9.580564498901367, 'mae': 2.4405624866485596, 'mse': 9.580564498901367}\n",
      "...Training: end of batch 2; log content: {'loss': 13.065640449523926, 'mae': 2.6854794025421143, 'mse': 13.065640449523926}\n",
      "...Training: end of batch 3; log content: {'loss': 13.592123031616211, 'mae': 2.6910552978515625, 'mse': 13.592123031616211}\n",
      "...Training: end of batch 4; log content: {'loss': 13.440434455871582, 'mae': 2.726086378097534, 'mse': 13.440434455871582}\n",
      "...Training: end of batch 5; log content: {'loss': 13.466029167175293, 'mae': 2.663755178451538, 'mse': 13.466029167175293}\n",
      "...Training: end of batch 6; log content: {'loss': 13.128568649291992, 'mae': 2.6802656650543213, 'mse': 13.128568649291992}\n",
      "...Training: end of batch 7; log content: {'loss': 12.91627311706543, 'mae': 2.6944782733917236, 'mse': 12.91627311706543}\n",
      "End epoch 51 of training; log content: {'loss': 12.91627311706543, 'mae': 2.6944782733917236, 'mse': 12.91627311706543, 'val_loss': 12.97103214263916, 'val_mae': 2.7107415199279785, 'val_mse': 12.97103214263916}\n",
      "...Training: end of batch 0; log content: {'loss': 12.455036163330078, 'mae': 2.8260045051574707, 'mse': 12.455036163330078}\n",
      "...Training: end of batch 1; log content: {'loss': 11.690147399902344, 'mae': 2.796170234680176, 'mse': 11.690147399902344}\n",
      "...Training: end of batch 2; log content: {'loss': 12.901698112487793, 'mae': 2.921339273452759, 'mse': 12.901698112487793}\n",
      "...Training: end of batch 3; log content: {'loss': 12.642255783081055, 'mae': 2.847527503967285, 'mse': 12.642255783081055}\n",
      "...Training: end of batch 4; log content: {'loss': 13.595674514770508, 'mae': 2.8576810359954834, 'mse': 13.595674514770508}\n",
      "...Training: end of batch 5; log content: {'loss': 13.593029975891113, 'mae': 2.7827951908111572, 'mse': 13.593029975891113}\n",
      "...Training: end of batch 6; log content: {'loss': 13.612915992736816, 'mae': 2.7817859649658203, 'mse': 13.612915992736816}\n",
      "...Training: end of batch 7; log content: {'loss': 12.742076873779297, 'mae': 2.688840866088867, 'mse': 12.742076873779297}\n",
      "End epoch 52 of training; log content: {'loss': 12.742076873779297, 'mae': 2.688840866088867, 'mse': 12.742076873779297, 'val_loss': 12.613245964050293, 'val_mae': 2.67602801322937, 'val_mse': 12.613245964050293}\n",
      "...Training: end of batch 0; log content: {'loss': 9.578301429748535, 'mae': 2.5402579307556152, 'mse': 9.578301429748535}\n",
      "...Training: end of batch 1; log content: {'loss': 8.528392791748047, 'mae': 2.4103379249572754, 'mse': 8.528392791748047}\n",
      "...Training: end of batch 2; log content: {'loss': 10.03101634979248, 'mae': 2.537597417831421, 'mse': 10.03101634979248}\n",
      "...Training: end of batch 3; log content: {'loss': 9.066190719604492, 'mae': 2.377079963684082, 'mse': 9.066190719604492}\n",
      "...Training: end of batch 4; log content: {'loss': 10.32860279083252, 'mae': 2.483806610107422, 'mse': 10.32860279083252}\n",
      "...Training: end of batch 5; log content: {'loss': 10.433510780334473, 'mae': 2.4929301738739014, 'mse': 10.433510780334473}\n",
      "...Training: end of batch 6; log content: {'loss': 12.25841236114502, 'mae': 2.6297667026519775, 'mse': 12.25841236114502}\n",
      "...Training: end of batch 7; log content: {'loss': 12.536865234375, 'mae': 2.664072036743164, 'mse': 12.536865234375}\n",
      "End epoch 53 of training; log content: {'loss': 12.536865234375, 'mae': 2.664072036743164, 'mse': 12.536865234375, 'val_loss': 12.39417552947998, 'val_mae': 2.657228708267212, 'val_mse': 12.39417552947998}\n",
      "...Training: end of batch 0; log content: {'loss': 10.523324966430664, 'mae': 2.3165717124938965, 'mse': 10.523324966430664}\n",
      "...Training: end of batch 1; log content: {'loss': 12.187540054321289, 'mae': 2.5650651454925537, 'mse': 12.187540054321289}\n",
      "...Training: end of batch 2; log content: {'loss': 11.789759635925293, 'mae': 2.5381808280944824, 'mse': 11.789759635925293}\n",
      "...Training: end of batch 3; log content: {'loss': 12.619372367858887, 'mae': 2.6657800674438477, 'mse': 12.619372367858887}\n",
      "...Training: end of batch 4; log content: {'loss': 11.481695175170898, 'mae': 2.5635387897491455, 'mse': 11.481695175170898}\n",
      "...Training: end of batch 5; log content: {'loss': 12.92665958404541, 'mae': 2.6660635471343994, 'mse': 12.92665958404541}\n",
      "...Training: end of batch 6; log content: {'loss': 12.10954475402832, 'mae': 2.617401599884033, 'mse': 12.10954475402832}\n",
      "...Training: end of batch 7; log content: {'loss': 12.426100730895996, 'mae': 2.6393463611602783, 'mse': 12.426100730895996}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End epoch 54 of training; log content: {'loss': 12.426100730895996, 'mae': 2.6393463611602783, 'mse': 12.426100730895996, 'val_loss': 12.23901653289795, 'val_mae': 2.651272773742676, 'val_mse': 12.23901653289795}\n",
      "...Training: end of batch 0; log content: {'loss': 19.046146392822266, 'mae': 2.969933271408081, 'mse': 19.046146392822266}\n",
      "...Training: end of batch 1; log content: {'loss': 13.875015258789062, 'mae': 2.702345848083496, 'mse': 13.875015258789062}\n",
      "...Training: end of batch 2; log content: {'loss': 12.090901374816895, 'mae': 2.5573532581329346, 'mse': 12.090901374816895}\n",
      "...Training: end of batch 3; log content: {'loss': 10.916969299316406, 'mae': 2.389885902404785, 'mse': 10.916969299316406}\n",
      "...Training: end of batch 4; log content: {'loss': 12.513747215270996, 'mae': 2.5495100021362305, 'mse': 12.513747215270996}\n",
      "...Training: end of batch 5; log content: {'loss': 11.698369026184082, 'mae': 2.5066885948181152, 'mse': 11.698369026184082}\n",
      "...Training: end of batch 6; log content: {'loss': 12.280428886413574, 'mae': 2.6266391277313232, 'mse': 12.280428886413574}\n",
      "...Training: end of batch 7; log content: {'loss': 12.390485763549805, 'mae': 2.6289846897125244, 'mse': 12.390485763549805}\n",
      "End epoch 55 of training; log content: {'loss': 12.390485763549805, 'mae': 2.6289846897125244, 'mse': 12.390485763549805, 'val_loss': 11.82129192352295, 'val_mae': 2.6064233779907227, 'val_mse': 11.82129192352295}\n",
      "...Training: end of batch 0; log content: {'loss': 12.247481346130371, 'mae': 2.7343645095825195, 'mse': 12.247481346130371}\n",
      "...Training: end of batch 1; log content: {'loss': 15.705848693847656, 'mae': 2.843505620956421, 'mse': 15.705848693847656}\n",
      "...Training: end of batch 2; log content: {'loss': 14.275676727294922, 'mae': 2.7391014099121094, 'mse': 14.275676727294922}\n",
      "...Training: end of batch 3; log content: {'loss': 13.786608695983887, 'mae': 2.761540412902832, 'mse': 13.786608695983887}\n",
      "...Training: end of batch 4; log content: {'loss': 14.289077758789062, 'mae': 2.8817861080169678, 'mse': 14.289077758789062}\n",
      "...Training: end of batch 5; log content: {'loss': 12.763794898986816, 'mae': 2.704561233520508, 'mse': 12.763794898986816}\n",
      "...Training: end of batch 6; log content: {'loss': 12.064461708068848, 'mae': 2.6161580085754395, 'mse': 12.064461708068848}\n",
      "...Training: end of batch 7; log content: {'loss': 12.211498260498047, 'mae': 2.636875629425049, 'mse': 12.211498260498047}\n",
      "End epoch 56 of training; log content: {'loss': 12.211498260498047, 'mae': 2.636875629425049, 'mse': 12.211498260498047, 'val_loss': 11.620856285095215, 'val_mae': 2.5864040851593018, 'val_mse': 11.620856285095215}\n",
      "...Training: end of batch 0; log content: {'loss': 13.293052673339844, 'mae': 2.569345474243164, 'mse': 13.293052673339844}\n",
      "...Training: end of batch 1; log content: {'loss': 12.945037841796875, 'mae': 2.6013925075531006, 'mse': 12.945037841796875}\n",
      "...Training: end of batch 2; log content: {'loss': 11.718994140625, 'mae': 2.568450689315796, 'mse': 11.718994140625}\n",
      "...Training: end of batch 3; log content: {'loss': 12.756088256835938, 'mae': 2.6591238975524902, 'mse': 12.756088256835938}\n",
      "...Training: end of batch 4; log content: {'loss': 12.279595375061035, 'mae': 2.5821566581726074, 'mse': 12.279595375061035}\n",
      "...Training: end of batch 5; log content: {'loss': 12.629572868347168, 'mae': 2.650331497192383, 'mse': 12.629572868347168}\n",
      "...Training: end of batch 6; log content: {'loss': 12.692090034484863, 'mae': 2.661445140838623, 'mse': 12.692090034484863}\n",
      "...Training: end of batch 7; log content: {'loss': 12.029736518859863, 'mae': 2.6015005111694336, 'mse': 12.029736518859863}\n",
      "End epoch 57 of training; log content: {'loss': 12.029736518859863, 'mae': 2.6015005111694336, 'mse': 12.029736518859863, 'val_loss': 11.635848045349121, 'val_mae': 2.6043131351470947, 'val_mse': 11.635847091674805}\n",
      "...Training: end of batch 0; log content: {'loss': 10.2518310546875, 'mae': 2.035461187362671, 'mse': 10.2518310546875}\n",
      "...Training: end of batch 1; log content: {'loss': 11.25977611541748, 'mae': 2.5229837894439697, 'mse': 11.25977611541748}\n",
      "...Training: end of batch 2; log content: {'loss': 10.165886878967285, 'mae': 2.4592840671539307, 'mse': 10.165886878967285}\n",
      "...Training: end of batch 3; log content: {'loss': 10.301911354064941, 'mae': 2.4775991439819336, 'mse': 10.301911354064941}\n",
      "...Training: end of batch 4; log content: {'loss': 10.79793643951416, 'mae': 2.5282034873962402, 'mse': 10.79793643951416}\n",
      "...Training: end of batch 5; log content: {'loss': 11.077385902404785, 'mae': 2.505952835083008, 'mse': 11.077385902404785}\n",
      "...Training: end of batch 6; log content: {'loss': 10.403645515441895, 'mae': 2.448941707611084, 'mse': 10.403645515441895}\n",
      "...Training: end of batch 7; log content: {'loss': 12.05229377746582, 'mae': 2.5859477519989014, 'mse': 12.05229377746582}\n",
      "End epoch 58 of training; log content: {'loss': 12.05229377746582, 'mae': 2.5859477519989014, 'mse': 12.05229377746582, 'val_loss': 11.516109466552734, 'val_mae': 2.588885545730591, 'val_mse': 11.516109466552734}\n",
      "...Training: end of batch 0; log content: {'loss': 9.288463592529297, 'mae': 2.2118258476257324, 'mse': 9.288463592529297}\n",
      "...Training: end of batch 1; log content: {'loss': 9.788247108459473, 'mae': 2.2883706092834473, 'mse': 9.788247108459473}\n",
      "...Training: end of batch 2; log content: {'loss': 9.871269226074219, 'mae': 2.3549208641052246, 'mse': 9.871269226074219}\n",
      "...Training: end of batch 3; log content: {'loss': 9.379983901977539, 'mae': 2.3337388038635254, 'mse': 9.379983901977539}\n",
      "...Training: end of batch 4; log content: {'loss': 11.490214347839355, 'mae': 2.4532361030578613, 'mse': 11.490214347839355}\n",
      "...Training: end of batch 5; log content: {'loss': 12.347088813781738, 'mae': 2.5678021907806396, 'mse': 12.347088813781738}\n",
      "...Training: end of batch 6; log content: {'loss': 12.202719688415527, 'mae': 2.5872251987457275, 'mse': 12.202719688415527}\n",
      "...Training: end of batch 7; log content: {'loss': 11.849961280822754, 'mae': 2.5694990158081055, 'mse': 11.849961280822754}\n",
      "End epoch 59 of training; log content: {'loss': 11.849961280822754, 'mae': 2.5694990158081055, 'mse': 11.849961280822754, 'val_loss': 11.279048919677734, 'val_mae': 2.552657127380371, 'val_mse': 11.279048919677734}\n",
      "...Training: end of batch 0; log content: {'loss': 20.79757308959961, 'mae': 3.518120527267456, 'mse': 20.79757308959961}\n",
      "...Training: end of batch 1; log content: {'loss': 15.694048881530762, 'mae': 3.0230069160461426, 'mse': 15.694048881530762}\n",
      "...Training: end of batch 2; log content: {'loss': 14.094673156738281, 'mae': 2.843501091003418, 'mse': 14.094673156738281}\n",
      "...Training: end of batch 3; log content: {'loss': 14.879053115844727, 'mae': 2.9134392738342285, 'mse': 14.879053115844727}\n",
      "...Training: end of batch 4; log content: {'loss': 13.693742752075195, 'mae': 2.806891679763794, 'mse': 13.693742752075195}\n",
      "...Training: end of batch 5; log content: {'loss': 13.333407402038574, 'mae': 2.772348403930664, 'mse': 13.333407402038574}\n",
      "...Training: end of batch 6; log content: {'loss': 12.177565574645996, 'mae': 2.6504154205322266, 'mse': 12.177565574645996}\n",
      "...Training: end of batch 7; log content: {'loss': 11.87105655670166, 'mae': 2.60174298286438, 'mse': 11.87105655670166}\n",
      "End epoch 60 of training; log content: {'loss': 11.87105655670166, 'mae': 2.60174298286438, 'mse': 11.87105655670166, 'val_loss': 11.129563331604004, 'val_mae': 2.528792142868042, 'val_mse': 11.129563331604004}\n",
      "...Training: end of batch 0; log content: {'loss': 10.472732543945312, 'mae': 2.507490634918213, 'mse': 10.472732543945312}\n",
      "...Training: end of batch 1; log content: {'loss': 13.559657096862793, 'mae': 2.820068359375, 'mse': 13.559657096862793}\n",
      "...Training: end of batch 2; log content: {'loss': 12.621129035949707, 'mae': 2.7647387981414795, 'mse': 12.621129035949707}\n",
      "...Training: end of batch 3; log content: {'loss': 12.80937385559082, 'mae': 2.7845447063446045, 'mse': 12.80937385559082}\n",
      "...Training: end of batch 4; log content: {'loss': 11.808417320251465, 'mae': 2.6384730339050293, 'mse': 11.808417320251465}\n",
      "...Training: end of batch 5; log content: {'loss': 11.218104362487793, 'mae': 2.5975453853607178, 'mse': 11.218104362487793}\n",
      "...Training: end of batch 6; log content: {'loss': 11.456618309020996, 'mae': 2.5519492626190186, 'mse': 11.456618309020996}\n",
      "...Training: end of batch 7; log content: {'loss': 11.884821891784668, 'mae': 2.594102621078491, 'mse': 11.884821891784668}\n",
      "End epoch 61 of training; log content: {'loss': 11.884821891784668, 'mae': 2.594102621078491, 'mse': 11.884821891784668, 'val_loss': 11.234107971191406, 'val_mae': 2.552109956741333, 'val_mse': 11.234107971191406}\n",
      "...Training: end of batch 0; log content: {'loss': 13.509160995483398, 'mae': 2.666252851486206, 'mse': 13.509160995483398}\n",
      "...Training: end of batch 1; log content: {'loss': 10.427979469299316, 'mae': 2.433047294616699, 'mse': 10.427979469299316}\n",
      "...Training: end of batch 2; log content: {'loss': 9.33720874786377, 'mae': 2.3044090270996094, 'mse': 9.33720874786377}\n",
      "...Training: end of batch 3; log content: {'loss': 9.648225784301758, 'mae': 2.358428955078125, 'mse': 9.648225784301758}\n",
      "...Training: end of batch 4; log content: {'loss': 9.416810989379883, 'mae': 2.3223888874053955, 'mse': 9.416810989379883}\n",
      "...Training: end of batch 5; log content: {'loss': 10.731793403625488, 'mae': 2.4513800144195557, 'mse': 10.731793403625488}\n",
      "...Training: end of batch 6; log content: {'loss': 11.625120162963867, 'mae': 2.524806499481201, 'mse': 11.625120162963867}\n",
      "...Training: end of batch 7; log content: {'loss': 11.7333345413208, 'mae': 2.5636186599731445, 'mse': 11.7333345413208}\n",
      "End epoch 62 of training; log content: {'loss': 11.7333345413208, 'mae': 2.5636186599731445, 'mse': 11.7333345413208, 'val_loss': 11.099431991577148, 'val_mae': 2.533219575881958, 'val_mse': 11.099431991577148}\n",
      "...Training: end of batch 0; log content: {'loss': 12.374773025512695, 'mae': 2.656158447265625, 'mse': 12.374773025512695}\n",
      "...Training: end of batch 1; log content: {'loss': 11.333038330078125, 'mae': 2.6349382400512695, 'mse': 11.333038330078125}\n",
      "...Training: end of batch 2; log content: {'loss': 12.722956657409668, 'mae': 2.725828170776367, 'mse': 12.722956657409668}\n",
      "...Training: end of batch 3; log content: {'loss': 11.038901329040527, 'mae': 2.5336978435516357, 'mse': 11.038901329040527}\n",
      "...Training: end of batch 4; log content: {'loss': 12.083176612854004, 'mae': 2.622680902481079, 'mse': 12.083176612854004}\n",
      "...Training: end of batch 5; log content: {'loss': 11.943889617919922, 'mae': 2.596712827682495, 'mse': 11.943889617919922}\n",
      "...Training: end of batch 6; log content: {'loss': 11.824395179748535, 'mae': 2.6117892265319824, 'mse': 11.824395179748535}\n",
      "...Training: end of batch 7; log content: {'loss': 11.661696434020996, 'mae': 2.5663747787475586, 'mse': 11.661696434020996}\n",
      "End epoch 63 of training; log content: {'loss': 11.661696434020996, 'mae': 2.5663747787475586, 'mse': 11.661696434020996, 'val_loss': 10.921278953552246, 'val_mae': 2.5107369422912598, 'val_mse': 10.921278953552246}\n",
      "...Training: end of batch 0; log content: {'loss': 8.03656005859375, 'mae': 2.1757240295410156, 'mse': 8.03656005859375}\n",
      "...Training: end of batch 1; log content: {'loss': 9.50609302520752, 'mae': 2.44380521774292, 'mse': 9.50609302520752}\n",
      "...Training: end of batch 2; log content: {'loss': 10.24034595489502, 'mae': 2.554591178894043, 'mse': 10.24034595489502}\n",
      "...Training: end of batch 3; log content: {'loss': 10.680591583251953, 'mae': 2.51187801361084, 'mse': 10.680591583251953}\n",
      "...Training: end of batch 4; log content: {'loss': 11.15131950378418, 'mae': 2.588575839996338, 'mse': 11.15131950378418}\n",
      "...Training: end of batch 5; log content: {'loss': 11.102465629577637, 'mae': 2.573437452316284, 'mse': 11.102465629577637}\n",
      "...Training: end of batch 6; log content: {'loss': 12.21699047088623, 'mae': 2.64897084236145, 'mse': 12.21699047088623}\n",
      "...Training: end of batch 7; log content: {'loss': 11.636542320251465, 'mae': 2.5599405765533447, 'mse': 11.636542320251465}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End epoch 64 of training; log content: {'loss': 11.636542320251465, 'mae': 2.5599405765533447, 'mse': 11.636542320251465, 'val_loss': 10.813929557800293, 'val_mae': 2.5090341567993164, 'val_mse': 10.813929557800293}\n",
      "...Training: end of batch 0; log content: {'loss': 14.56159782409668, 'mae': 2.4341213703155518, 'mse': 14.56159782409668}\n",
      "...Training: end of batch 1; log content: {'loss': 12.815101623535156, 'mae': 2.378309726715088, 'mse': 12.815101623535156}\n",
      "...Training: end of batch 2; log content: {'loss': 12.389732360839844, 'mae': 2.429386615753174, 'mse': 12.389732360839844}\n",
      "...Training: end of batch 3; log content: {'loss': 12.077654838562012, 'mae': 2.418034076690674, 'mse': 12.077654838562012}\n",
      "...Training: end of batch 4; log content: {'loss': 11.399609565734863, 'mae': 2.4300196170806885, 'mse': 11.399609565734863}\n",
      "...Training: end of batch 5; log content: {'loss': 11.196110725402832, 'mae': 2.479538917541504, 'mse': 11.196110725402832}\n",
      "...Training: end of batch 6; log content: {'loss': 11.00472354888916, 'mae': 2.4669089317321777, 'mse': 11.00472354888916}\n",
      "...Training: end of batch 7; log content: {'loss': 11.656336784362793, 'mae': 2.5749409198760986, 'mse': 11.656336784362793}\n",
      "End epoch 65 of training; log content: {'loss': 11.656336784362793, 'mae': 2.5749409198760986, 'mse': 11.656336784362793, 'val_loss': 10.700631141662598, 'val_mae': 2.496633529663086, 'val_mse': 10.700631141662598}\n",
      "...Training: end of batch 0; log content: {'loss': 6.4821906089782715, 'mae': 2.145249366760254, 'mse': 6.4821906089782715}\n",
      "...Training: end of batch 1; log content: {'loss': 11.110891342163086, 'mae': 2.4848642349243164, 'mse': 11.110891342163086}\n",
      "...Training: end of batch 2; log content: {'loss': 11.039029121398926, 'mae': 2.492971181869507, 'mse': 11.039029121398926}\n",
      "...Training: end of batch 3; log content: {'loss': 10.853352546691895, 'mae': 2.43206787109375, 'mse': 10.853352546691895}\n",
      "...Training: end of batch 4; log content: {'loss': 12.049556732177734, 'mae': 2.582636594772339, 'mse': 12.049556732177734}\n",
      "...Training: end of batch 5; log content: {'loss': 11.335755348205566, 'mae': 2.5224106311798096, 'mse': 11.335755348205566}\n",
      "...Training: end of batch 6; log content: {'loss': 10.959455490112305, 'mae': 2.4956538677215576, 'mse': 10.959455490112305}\n",
      "...Training: end of batch 7; log content: {'loss': 11.590902328491211, 'mae': 2.56363582611084, 'mse': 11.590902328491211}\n",
      "End epoch 66 of training; log content: {'loss': 11.590902328491211, 'mae': 2.56363582611084, 'mse': 11.590902328491211, 'val_loss': 10.684487342834473, 'val_mae': 2.501605749130249, 'val_mse': 10.684487342834473}\n",
      "...Training: end of batch 0; log content: {'loss': 11.489992141723633, 'mae': 2.264345169067383, 'mse': 11.489992141723633}\n",
      "...Training: end of batch 1; log content: {'loss': 12.449737548828125, 'mae': 2.4965362548828125, 'mse': 12.449737548828125}\n",
      "...Training: end of batch 2; log content: {'loss': 12.779037475585938, 'mae': 2.689124345779419, 'mse': 12.779037475585938}\n",
      "...Training: end of batch 3; log content: {'loss': 10.96410083770752, 'mae': 2.5019478797912598, 'mse': 10.96410083770752}\n",
      "...Training: end of batch 4; log content: {'loss': 13.118108749389648, 'mae': 2.6888744831085205, 'mse': 13.118108749389648}\n",
      "...Training: end of batch 5; log content: {'loss': 12.872260093688965, 'mae': 2.695650339126587, 'mse': 12.872260093688965}\n",
      "...Training: end of batch 6; log content: {'loss': 12.101693153381348, 'mae': 2.6133522987365723, 'mse': 12.101693153381348}\n",
      "...Training: end of batch 7; log content: {'loss': 11.63077163696289, 'mae': 2.5707106590270996, 'mse': 11.63077163696289}\n",
      "End epoch 67 of training; log content: {'loss': 11.63077163696289, 'mae': 2.5707106590270996, 'mse': 11.63077163696289, 'val_loss': 10.58410930633545, 'val_mae': 2.4884634017944336, 'val_mse': 10.58410930633545}\n",
      "...Training: end of batch 0; log content: {'loss': 5.948660850524902, 'mae': 1.9195221662521362, 'mse': 5.948660850524902}\n",
      "...Training: end of batch 1; log content: {'loss': 7.062459468841553, 'mae': 2.1020758152008057, 'mse': 7.062459468841553}\n",
      "...Training: end of batch 2; log content: {'loss': 8.509377479553223, 'mae': 2.2747912406921387, 'mse': 8.509377479553223}\n",
      "...Training: end of batch 3; log content: {'loss': 10.438758850097656, 'mae': 2.4502158164978027, 'mse': 10.438758850097656}\n",
      "...Training: end of batch 4; log content: {'loss': 9.690763473510742, 'mae': 2.387890577316284, 'mse': 9.690763473510742}\n",
      "...Training: end of batch 5; log content: {'loss': 10.60602855682373, 'mae': 2.483285665512085, 'mse': 10.60602855682373}\n",
      "...Training: end of batch 6; log content: {'loss': 10.660704612731934, 'mae': 2.492502450942993, 'mse': 10.660704612731934}\n",
      "...Training: end of batch 7; log content: {'loss': 11.572198867797852, 'mae': 2.5645368099212646, 'mse': 11.572198867797852}\n",
      "End epoch 68 of training; log content: {'loss': 11.572198867797852, 'mae': 2.5645368099212646, 'mse': 11.572198867797852, 'val_loss': 10.603096961975098, 'val_mae': 2.4943034648895264, 'val_mse': 10.603096961975098}\n",
      "...Training: end of batch 0; log content: {'loss': 11.944193840026855, 'mae': 2.6454854011535645, 'mse': 11.944193840026855}\n",
      "...Training: end of batch 1; log content: {'loss': 10.383478164672852, 'mae': 2.534130573272705, 'mse': 10.383478164672852}\n",
      "...Training: end of batch 2; log content: {'loss': 10.271695137023926, 'mae': 2.5142428874969482, 'mse': 10.271695137023926}\n",
      "...Training: end of batch 3; log content: {'loss': 10.633193969726562, 'mae': 2.4918174743652344, 'mse': 10.633193969726562}\n",
      "...Training: end of batch 4; log content: {'loss': 11.10957145690918, 'mae': 2.4855637550354004, 'mse': 11.10957145690918}\n",
      "...Training: end of batch 5; log content: {'loss': 11.816468238830566, 'mae': 2.560511589050293, 'mse': 11.816468238830566}\n",
      "...Training: end of batch 6; log content: {'loss': 11.42005729675293, 'mae': 2.5302255153656006, 'mse': 11.42005729675293}\n",
      "...Training: end of batch 7; log content: {'loss': 11.52169418334961, 'mae': 2.5574023723602295, 'mse': 11.52169418334961}\n",
      "End epoch 69 of training; log content: {'loss': 11.52169418334961, 'mae': 2.5574023723602295, 'mse': 11.52169418334961, 'val_loss': 10.462836265563965, 'val_mae': 2.480038642883301, 'val_mse': 10.462836265563965}\n",
      "...Training: end of batch 0; log content: {'loss': 9.539037704467773, 'mae': 2.5436182022094727, 'mse': 9.539037704467773}\n",
      "...Training: end of batch 1; log content: {'loss': 13.742904663085938, 'mae': 2.7586164474487305, 'mse': 13.742904663085938}\n",
      "...Training: end of batch 2; log content: {'loss': 12.329643249511719, 'mae': 2.618656635284424, 'mse': 12.329643249511719}\n",
      "...Training: end of batch 3; log content: {'loss': 12.057778358459473, 'mae': 2.6065917015075684, 'mse': 12.057778358459473}\n",
      "...Training: end of batch 4; log content: {'loss': 12.289166450500488, 'mae': 2.6443793773651123, 'mse': 12.289166450500488}\n",
      "...Training: end of batch 5; log content: {'loss': 11.437230110168457, 'mae': 2.5632293224334717, 'mse': 11.437230110168457}\n",
      "...Training: end of batch 6; log content: {'loss': 11.865839958190918, 'mae': 2.6171905994415283, 'mse': 11.865839958190918}\n",
      "...Training: end of batch 7; log content: {'loss': 11.552986145019531, 'mae': 2.581244468688965, 'mse': 11.552987098693848}\n",
      "End epoch 70 of training; log content: {'loss': 11.552986145019531, 'mae': 2.581244468688965, 'mse': 11.552987098693848, 'val_loss': 10.504691123962402, 'val_mae': 2.476362705230713, 'val_mse': 10.504691123962402}\n",
      "...Training: end of batch 0; log content: {'loss': 13.322811126708984, 'mae': 2.6410346031188965, 'mse': 13.322811126708984}\n",
      "...Training: end of batch 1; log content: {'loss': 11.069458961486816, 'mae': 2.4999513626098633, 'mse': 11.069458961486816}\n",
      "...Training: end of batch 2; log content: {'loss': 9.769200325012207, 'mae': 2.3754827976226807, 'mse': 9.769200325012207}\n",
      "...Training: end of batch 3; log content: {'loss': 9.516447067260742, 'mae': 2.355027437210083, 'mse': 9.516447067260742}\n",
      "...Training: end of batch 4; log content: {'loss': 10.39498233795166, 'mae': 2.4548044204711914, 'mse': 10.39498233795166}\n",
      "...Training: end of batch 5; log content: {'loss': 11.990788459777832, 'mae': 2.5770435333251953, 'mse': 11.990788459777832}\n",
      "...Training: end of batch 6; log content: {'loss': 11.595780372619629, 'mae': 2.559422016143799, 'mse': 11.595780372619629}\n",
      "...Training: end of batch 7; log content: {'loss': 11.556272506713867, 'mae': 2.5681376457214355, 'mse': 11.556272506713867}\n",
      "End epoch 71 of training; log content: {'loss': 11.556272506713867, 'mae': 2.5681376457214355, 'mse': 11.556272506713867, 'val_loss': 10.588900566101074, 'val_mae': 2.491580009460449, 'val_mse': 10.588900566101074}\n",
      "...Training: end of batch 0; log content: {'loss': 12.334914207458496, 'mae': 2.6798861026763916, 'mse': 12.334914207458496}\n",
      "...Training: end of batch 1; log content: {'loss': 9.832155227661133, 'mae': 2.3732690811157227, 'mse': 9.832155227661133}\n",
      "...Training: end of batch 2; log content: {'loss': 12.15838623046875, 'mae': 2.6119132041931152, 'mse': 12.15838623046875}\n",
      "...Training: end of batch 3; log content: {'loss': 11.666939735412598, 'mae': 2.592874526977539, 'mse': 11.666939735412598}\n",
      "...Training: end of batch 4; log content: {'loss': 10.89952278137207, 'mae': 2.5658347606658936, 'mse': 10.89952278137207}\n",
      "...Training: end of batch 5; log content: {'loss': 10.957061767578125, 'mae': 2.5685813426971436, 'mse': 10.957061767578125}\n",
      "...Training: end of batch 6; log content: {'loss': 10.839315414428711, 'mae': 2.529694080352783, 'mse': 10.839315414428711}\n",
      "...Training: end of batch 7; log content: {'loss': 11.534668922424316, 'mae': 2.561161518096924, 'mse': 11.534668922424316}\n",
      "End epoch 72 of training; log content: {'loss': 11.534668922424316, 'mae': 2.561161518096924, 'mse': 11.534668922424316, 'val_loss': 10.42940616607666, 'val_mae': 2.4754183292388916, 'val_mse': 10.42940616607666}\n",
      "...Training: end of batch 0; log content: {'loss': 12.80443000793457, 'mae': 2.717510223388672, 'mse': 12.80443000793457}\n",
      "...Training: end of batch 1; log content: {'loss': 13.440788269042969, 'mae': 2.903238296508789, 'mse': 13.440788269042969}\n",
      "...Training: end of batch 2; log content: {'loss': 13.33730697631836, 'mae': 2.786182165145874, 'mse': 13.33730697631836}\n",
      "...Training: end of batch 3; log content: {'loss': 12.069826126098633, 'mae': 2.659355640411377, 'mse': 12.069826126098633}\n",
      "...Training: end of batch 4; log content: {'loss': 11.350028991699219, 'mae': 2.578885078430176, 'mse': 11.350028991699219}\n",
      "...Training: end of batch 5; log content: {'loss': 10.849579811096191, 'mae': 2.532421350479126, 'mse': 10.849579811096191}\n",
      "...Training: end of batch 6; log content: {'loss': 12.191521644592285, 'mae': 2.6391489505767822, 'mse': 12.191521644592285}\n",
      "...Training: end of batch 7; log content: {'loss': 11.458609580993652, 'mae': 2.5580568313598633, 'mse': 11.458609580993652}\n",
      "End epoch 73 of training; log content: {'loss': 11.458609580993652, 'mae': 2.5580568313598633, 'mse': 11.458609580993652, 'val_loss': 10.480610847473145, 'val_mae': 2.4756054878234863, 'val_mse': 10.480610847473145}\n",
      "...Training: end of batch 0; log content: {'loss': 11.000956535339355, 'mae': 2.5016777515411377, 'mse': 11.000956535339355}\n",
      "...Training: end of batch 1; log content: {'loss': 11.135103225708008, 'mae': 2.6208691596984863, 'mse': 11.135103225708008}\n",
      "...Training: end of batch 2; log content: {'loss': 11.269637107849121, 'mae': 2.6230099201202393, 'mse': 11.269637107849121}\n",
      "...Training: end of batch 3; log content: {'loss': 12.223163604736328, 'mae': 2.677433967590332, 'mse': 12.223163604736328}\n",
      "...Training: end of batch 4; log content: {'loss': 11.646052360534668, 'mae': 2.596214771270752, 'mse': 11.646052360534668}\n",
      "...Training: end of batch 5; log content: {'loss': 11.58099365234375, 'mae': 2.5920586585998535, 'mse': 11.58099365234375}\n",
      "...Training: end of batch 6; log content: {'loss': 11.76691722869873, 'mae': 2.583615779876709, 'mse': 11.76691722869873}\n",
      "...Training: end of batch 7; log content: {'loss': 11.479355812072754, 'mae': 2.5637998580932617, 'mse': 11.479355812072754}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End epoch 74 of training; log content: {'loss': 11.479355812072754, 'mae': 2.5637998580932617, 'mse': 11.479355812072754, 'val_loss': 10.369196891784668, 'val_mae': 2.4663989543914795, 'val_mse': 10.369196891784668}\n",
      "...Training: end of batch 0; log content: {'loss': 9.048422813415527, 'mae': 2.208756446838379, 'mse': 9.048422813415527}\n",
      "...Training: end of batch 1; log content: {'loss': 9.464361190795898, 'mae': 2.3993887901306152, 'mse': 9.464361190795898}\n",
      "...Training: end of batch 2; log content: {'loss': 9.09804630279541, 'mae': 2.347954034805298, 'mse': 9.09804630279541}\n",
      "...Training: end of batch 3; log content: {'loss': 10.923328399658203, 'mae': 2.6119256019592285, 'mse': 10.923328399658203}\n",
      "...Training: end of batch 4; log content: {'loss': 10.52726936340332, 'mae': 2.564666986465454, 'mse': 10.52726936340332}\n",
      "...Training: end of batch 5; log content: {'loss': 11.826176643371582, 'mae': 2.681410074234009, 'mse': 11.826176643371582}\n",
      "...Training: end of batch 6; log content: {'loss': 11.47154426574707, 'mae': 2.5663015842437744, 'mse': 11.47154426574707}\n",
      "...Training: end of batch 7; log content: {'loss': 11.456597328186035, 'mae': 2.545546054840088, 'mse': 11.456597328186035}\n",
      "End epoch 75 of training; log content: {'loss': 11.456597328186035, 'mae': 2.545546054840088, 'mse': 11.456597328186035, 'val_loss': 10.4456787109375, 'val_mae': 2.473925828933716, 'val_mse': 10.4456787109375}\n",
      "...Training: end of batch 0; log content: {'loss': 10.476288795471191, 'mae': 2.5022926330566406, 'mse': 10.476288795471191}\n",
      "...Training: end of batch 1; log content: {'loss': 12.154777526855469, 'mae': 2.627450466156006, 'mse': 12.154777526855469}\n",
      "...Training: end of batch 2; log content: {'loss': 11.294528007507324, 'mae': 2.556941509246826, 'mse': 11.294528007507324}\n",
      "...Training: end of batch 3; log content: {'loss': 10.693536758422852, 'mae': 2.4983863830566406, 'mse': 10.693536758422852}\n",
      "...Training: end of batch 4; log content: {'loss': 11.381319046020508, 'mae': 2.552420139312744, 'mse': 11.381319046020508}\n",
      "...Training: end of batch 5; log content: {'loss': 11.53758716583252, 'mae': 2.5770111083984375, 'mse': 11.53758716583252}\n",
      "...Training: end of batch 6; log content: {'loss': 10.794356346130371, 'mae': 2.5061237812042236, 'mse': 10.794356346130371}\n",
      "...Training: end of batch 7; log content: {'loss': 11.449345588684082, 'mae': 2.550375461578369, 'mse': 11.449345588684082}\n",
      "End epoch 76 of training; log content: {'loss': 11.449345588684082, 'mae': 2.550375461578369, 'mse': 11.449345588684082, 'val_loss': 10.391976356506348, 'val_mae': 2.467409610748291, 'val_mse': 10.391976356506348}\n",
      "...Training: end of batch 0; log content: {'loss': 8.848529815673828, 'mae': 2.4384279251098633, 'mse': 8.848529815673828}\n",
      "...Training: end of batch 1; log content: {'loss': 8.613357543945312, 'mae': 2.392744541168213, 'mse': 8.613357543945312}\n",
      "...Training: end of batch 2; log content: {'loss': 9.695144653320312, 'mae': 2.447324514389038, 'mse': 9.695144653320312}\n",
      "...Training: end of batch 3; log content: {'loss': 11.684808731079102, 'mae': 2.5599403381347656, 'mse': 11.684808731079102}\n",
      "...Training: end of batch 4; log content: {'loss': 11.662405014038086, 'mae': 2.5806846618652344, 'mse': 11.662405014038086}\n",
      "...Training: end of batch 5; log content: {'loss': 10.387032508850098, 'mae': 2.4354190826416016, 'mse': 10.387032508850098}\n",
      "...Training: end of batch 6; log content: {'loss': 11.846095085144043, 'mae': 2.6053478717803955, 'mse': 11.846095085144043}\n",
      "...Training: end of batch 7; log content: {'loss': 11.42690372467041, 'mae': 2.5593061447143555, 'mse': 11.42690372467041}\n",
      "End epoch 77 of training; log content: {'loss': 11.42690372467041, 'mae': 2.5593061447143555, 'mse': 11.42690372467041, 'val_loss': 10.419360160827637, 'val_mae': 2.4665493965148926, 'val_mse': 10.419360160827637}\n",
      "...Training: end of batch 0; log content: {'loss': 6.714498043060303, 'mae': 2.125685691833496, 'mse': 6.714498043060303}\n",
      "...Training: end of batch 1; log content: {'loss': 7.707253456115723, 'mae': 2.158841848373413, 'mse': 7.707253456115723}\n",
      "...Training: end of batch 2; log content: {'loss': 9.298490524291992, 'mae': 2.3869762420654297, 'mse': 9.298490524291992}\n",
      "...Training: end of batch 3; log content: {'loss': 11.716156959533691, 'mae': 2.6414504051208496, 'mse': 11.716156959533691}\n",
      "...Training: end of batch 4; log content: {'loss': 11.309661865234375, 'mae': 2.5665552616119385, 'mse': 11.309661865234375}\n",
      "...Training: end of batch 5; log content: {'loss': 11.248002052307129, 'mae': 2.575968027114868, 'mse': 11.248002052307129}\n",
      "...Training: end of batch 6; log content: {'loss': 10.362763404846191, 'mae': 2.4637084007263184, 'mse': 10.362763404846191}\n",
      "...Training: end of batch 7; log content: {'loss': 11.423996925354004, 'mae': 2.558220386505127, 'mse': 11.423996925354004}\n",
      "End epoch 78 of training; log content: {'loss': 11.423996925354004, 'mae': 2.558220386505127, 'mse': 11.423996925354004, 'val_loss': 10.331391334533691, 'val_mae': 2.46500301361084, 'val_mse': 10.331391334533691}\n",
      "...Training: end of batch 0; log content: {'loss': 11.581178665161133, 'mae': 2.3880722522735596, 'mse': 11.581178665161133}\n",
      "...Training: end of batch 1; log content: {'loss': 11.242300987243652, 'mae': 2.4728145599365234, 'mse': 11.242300987243652}\n",
      "...Training: end of batch 2; log content: {'loss': 12.587864875793457, 'mae': 2.652087688446045, 'mse': 12.587864875793457}\n",
      "...Training: end of batch 3; log content: {'loss': 11.756607055664062, 'mae': 2.6003687381744385, 'mse': 11.756607055664062}\n",
      "...Training: end of batch 4; log content: {'loss': 13.223248481750488, 'mae': 2.7637763023376465, 'mse': 13.223248481750488}\n",
      "...Training: end of batch 5; log content: {'loss': 12.693316459655762, 'mae': 2.715891122817993, 'mse': 12.693316459655762}\n",
      "...Training: end of batch 6; log content: {'loss': 11.743598937988281, 'mae': 2.6118881702423096, 'mse': 11.743598937988281}\n",
      "...Training: end of batch 7; log content: {'loss': 11.441522598266602, 'mae': 2.568331718444824, 'mse': 11.441522598266602}\n",
      "End epoch 79 of training; log content: {'loss': 11.441522598266602, 'mae': 2.568331718444824, 'mse': 11.441522598266602, 'val_loss': 10.329310417175293, 'val_mae': 2.4617526531219482, 'val_mse': 10.329310417175293}\n",
      "...Training: end of batch 0; log content: {'loss': 14.075072288513184, 'mae': 2.439394950866699, 'mse': 14.075072288513184}\n",
      "...Training: end of batch 1; log content: {'loss': 14.301376342773438, 'mae': 2.634085178375244, 'mse': 14.301376342773438}\n",
      "...Training: end of batch 2; log content: {'loss': 12.4669828414917, 'mae': 2.5414071083068848, 'mse': 12.4669828414917}\n",
      "...Training: end of batch 3; log content: {'loss': 12.51649284362793, 'mae': 2.5902652740478516, 'mse': 12.51649284362793}\n",
      "...Training: end of batch 4; log content: {'loss': 12.163228034973145, 'mae': 2.602792739868164, 'mse': 12.163228034973145}\n",
      "...Training: end of batch 5; log content: {'loss': 12.501622200012207, 'mae': 2.6987228393554688, 'mse': 12.501622200012207}\n",
      "...Training: end of batch 6; log content: {'loss': 11.544624328613281, 'mae': 2.5599210262298584, 'mse': 11.544624328613281}\n",
      "...Training: end of batch 7; log content: {'loss': 11.440759658813477, 'mae': 2.578493595123291, 'mse': 11.440759658813477}\n",
      "End epoch 80 of training; log content: {'loss': 11.440759658813477, 'mae': 2.578493595123291, 'mse': 11.440759658813477, 'val_loss': 10.277920722961426, 'val_mae': 2.4568235874176025, 'val_mse': 10.277920722961426}\n",
      "...Training: end of batch 0; log content: {'loss': 16.724143981933594, 'mae': 3.039564609527588, 'mse': 16.724143981933594}\n",
      "...Training: end of batch 1; log content: {'loss': 13.969029426574707, 'mae': 2.7743277549743652, 'mse': 13.969029426574707}\n",
      "...Training: end of batch 2; log content: {'loss': 12.649083137512207, 'mae': 2.6758930683135986, 'mse': 12.649083137512207}\n",
      "...Training: end of batch 3; log content: {'loss': 12.010963439941406, 'mae': 2.629628896713257, 'mse': 12.010963439941406}\n",
      "...Training: end of batch 4; log content: {'loss': 11.30833625793457, 'mae': 2.561035633087158, 'mse': 11.30833625793457}\n",
      "...Training: end of batch 5; log content: {'loss': 11.191267967224121, 'mae': 2.5472705364227295, 'mse': 11.191267967224121}\n",
      "...Training: end of batch 6; log content: {'loss': 11.54188346862793, 'mae': 2.590816020965576, 'mse': 11.54188346862793}\n",
      "...Training: end of batch 7; log content: {'loss': 11.393059730529785, 'mae': 2.5622503757476807, 'mse': 11.393059730529785}\n",
      "End epoch 81 of training; log content: {'loss': 11.393059730529785, 'mae': 2.5622503757476807, 'mse': 11.393059730529785, 'val_loss': 10.254927635192871, 'val_mae': 2.462158679962158, 'val_mse': 10.254927635192871}\n",
      "...Training: end of batch 0; log content: {'loss': 12.865262031555176, 'mae': 2.7857446670532227, 'mse': 12.865262031555176}\n",
      "...Training: end of batch 1; log content: {'loss': 11.282001495361328, 'mae': 2.5715672969818115, 'mse': 11.282001495361328}\n",
      "...Training: end of batch 2; log content: {'loss': 11.564986228942871, 'mae': 2.6460418701171875, 'mse': 11.564986228942871}\n",
      "...Training: end of batch 3; log content: {'loss': 13.736405372619629, 'mae': 2.8009910583496094, 'mse': 13.736405372619629}\n",
      "...Training: end of batch 4; log content: {'loss': 12.546707153320312, 'mae': 2.6523611545562744, 'mse': 12.546707153320312}\n",
      "...Training: end of batch 5; log content: {'loss': 11.949784278869629, 'mae': 2.6031534671783447, 'mse': 11.949784278869629}\n",
      "...Training: end of batch 6; log content: {'loss': 11.964627265930176, 'mae': 2.59727144241333, 'mse': 11.964627265930176}\n",
      "...Training: end of batch 7; log content: {'loss': 11.42541217803955, 'mae': 2.5444157123565674, 'mse': 11.42541217803955}\n",
      "End epoch 82 of training; log content: {'loss': 11.42541217803955, 'mae': 2.5444157123565674, 'mse': 11.42541217803955, 'val_loss': 10.332196235656738, 'val_mae': 2.4649362564086914, 'val_mse': 10.332196235656738}\n",
      "...Training: end of batch 0; log content: {'loss': 10.492326736450195, 'mae': 2.3745062351226807, 'mse': 10.492326736450195}\n",
      "...Training: end of batch 1; log content: {'loss': 8.791535377502441, 'mae': 2.257664680480957, 'mse': 8.791535377502441}\n",
      "...Training: end of batch 2; log content: {'loss': 9.9028959274292, 'mae': 2.4691905975341797, 'mse': 9.9028959274292}\n",
      "...Training: end of batch 3; log content: {'loss': 10.894049644470215, 'mae': 2.5154600143432617, 'mse': 10.894049644470215}\n",
      "...Training: end of batch 4; log content: {'loss': 12.957491874694824, 'mae': 2.6821236610412598, 'mse': 12.957491874694824}\n",
      "...Training: end of batch 5; log content: {'loss': 12.614986419677734, 'mae': 2.6564419269561768, 'mse': 12.614986419677734}\n",
      "...Training: end of batch 6; log content: {'loss': 12.040946006774902, 'mae': 2.598619222640991, 'mse': 12.040946006774902}\n",
      "...Training: end of batch 7; log content: {'loss': 11.428665161132812, 'mae': 2.548849105834961, 'mse': 11.428665161132812}\n",
      "End epoch 83 of training; log content: {'loss': 11.428665161132812, 'mae': 2.548849105834961, 'mse': 11.428665161132812, 'val_loss': 10.200703620910645, 'val_mae': 2.4500725269317627, 'val_mse': 10.200703620910645}\n",
      "...Training: end of batch 0; log content: {'loss': 11.39405632019043, 'mae': 2.5068511962890625, 'mse': 11.39405632019043}\n",
      "...Training: end of batch 1; log content: {'loss': 9.757286071777344, 'mae': 2.2660715579986572, 'mse': 9.757286071777344}\n",
      "...Training: end of batch 2; log content: {'loss': 10.204492568969727, 'mae': 2.4166717529296875, 'mse': 10.204492568969727}\n",
      "...Training: end of batch 3; log content: {'loss': 10.437057495117188, 'mae': 2.439579963684082, 'mse': 10.437057495117188}\n",
      "...Training: end of batch 4; log content: {'loss': 9.73232650756836, 'mae': 2.389221668243408, 'mse': 9.73232650756836}\n",
      "...Training: end of batch 5; log content: {'loss': 9.473073959350586, 'mae': 2.4006311893463135, 'mse': 9.473073959350586}\n",
      "...Training: end of batch 6; log content: {'loss': 10.091703414916992, 'mae': 2.463743209838867, 'mse': 10.091703414916992}\n",
      "...Training: end of batch 7; log content: {'loss': 11.429457664489746, 'mae': 2.575195550918579, 'mse': 11.429457664489746}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End epoch 84 of training; log content: {'loss': 11.429457664489746, 'mae': 2.575195550918579, 'mse': 11.429457664489746, 'val_loss': 10.240166664123535, 'val_mae': 2.4483139514923096, 'val_mse': 10.240166664123535}\n",
      "...Training: end of batch 0; log content: {'loss': 9.13519287109375, 'mae': 2.277327537536621, 'mse': 9.13519287109375}\n",
      "...Training: end of batch 1; log content: {'loss': 8.557243347167969, 'mae': 2.1775014400482178, 'mse': 8.557243347167969}\n",
      "...Training: end of batch 2; log content: {'loss': 9.054030418395996, 'mae': 2.2506089210510254, 'mse': 9.054030418395996}\n",
      "...Training: end of batch 3; log content: {'loss': 9.46218490600586, 'mae': 2.3568379878997803, 'mse': 9.46218490600586}\n",
      "...Training: end of batch 4; log content: {'loss': 9.656797409057617, 'mae': 2.4288620948791504, 'mse': 9.656797409057617}\n",
      "...Training: end of batch 5; log content: {'loss': 11.01569652557373, 'mae': 2.514904260635376, 'mse': 11.01569652557373}\n",
      "...Training: end of batch 6; log content: {'loss': 10.858687400817871, 'mae': 2.493651866912842, 'mse': 10.858687400817871}\n",
      "...Training: end of batch 7; log content: {'loss': 11.380289077758789, 'mae': 2.566547155380249, 'mse': 11.380289077758789}\n",
      "End epoch 85 of training; log content: {'loss': 11.380289077758789, 'mae': 2.566547155380249, 'mse': 11.380289077758789, 'val_loss': 10.251407623291016, 'val_mae': 2.450608015060425, 'val_mse': 10.251407623291016}\n",
      "...Training: end of batch 0; log content: {'loss': 8.551167488098145, 'mae': 2.2349770069122314, 'mse': 8.551167488098145}\n",
      "...Training: end of batch 1; log content: {'loss': 11.281637191772461, 'mae': 2.652012348175049, 'mse': 11.281637191772461}\n",
      "...Training: end of batch 2; log content: {'loss': 11.189704895019531, 'mae': 2.6026389598846436, 'mse': 11.189704895019531}\n",
      "...Training: end of batch 3; log content: {'loss': 11.427875518798828, 'mae': 2.6570372581481934, 'mse': 11.427875518798828}\n",
      "...Training: end of batch 4; log content: {'loss': 11.647782325744629, 'mae': 2.602255344390869, 'mse': 11.647782325744629}\n",
      "...Training: end of batch 5; log content: {'loss': 11.124865531921387, 'mae': 2.5780630111694336, 'mse': 11.124865531921387}\n",
      "...Training: end of batch 6; log content: {'loss': 11.113451957702637, 'mae': 2.5558323860168457, 'mse': 11.113451957702637}\n",
      "...Training: end of batch 7; log content: {'loss': 11.542704582214355, 'mae': 2.567612648010254, 'mse': 11.542704582214355}\n",
      "End epoch 86 of training; log content: {'loss': 11.542704582214355, 'mae': 2.567612648010254, 'mse': 11.542704582214355, 'val_loss': 10.49073314666748, 'val_mae': 2.4879798889160156, 'val_mse': 10.49073314666748}\n",
      "...Training: end of batch 0; log content: {'loss': 11.727729797363281, 'mae': 2.641684055328369, 'mse': 11.727729797363281}\n",
      "...Training: end of batch 1; log content: {'loss': 11.284004211425781, 'mae': 2.5552473068237305, 'mse': 11.284004211425781}\n",
      "...Training: end of batch 2; log content: {'loss': 12.033520698547363, 'mae': 2.5608298778533936, 'mse': 12.033520698547363}\n",
      "...Training: end of batch 3; log content: {'loss': 12.425922393798828, 'mae': 2.7307798862457275, 'mse': 12.425922393798828}\n",
      "...Training: end of batch 4; log content: {'loss': 10.955002784729004, 'mae': 2.536983013153076, 'mse': 10.955002784729004}\n",
      "...Training: end of batch 5; log content: {'loss': 12.353697776794434, 'mae': 2.6412298679351807, 'mse': 12.353697776794434}\n",
      "...Training: end of batch 6; log content: {'loss': 12.030976295471191, 'mae': 2.6336452960968018, 'mse': 12.030976295471191}\n",
      "...Training: end of batch 7; log content: {'loss': 11.421025276184082, 'mae': 2.5580389499664307, 'mse': 11.421025276184082}\n",
      "End epoch 87 of training; log content: {'loss': 11.421025276184082, 'mae': 2.5580389499664307, 'mse': 11.421025276184082, 'val_loss': 10.126175880432129, 'val_mae': 2.4408118724823, 'val_mse': 10.126175880432129}\n",
      "...Training: end of batch 0; log content: {'loss': 12.642278671264648, 'mae': 3.0262773036956787, 'mse': 12.642278671264648}\n",
      "...Training: end of batch 1; log content: {'loss': 9.184825897216797, 'mae': 2.5319743156433105, 'mse': 9.184825897216797}\n",
      "...Training: end of batch 2; log content: {'loss': 10.579377174377441, 'mae': 2.587143659591675, 'mse': 10.579377174377441}\n",
      "...Training: end of batch 3; log content: {'loss': 10.72198486328125, 'mae': 2.5990734100341797, 'mse': 10.72198486328125}\n",
      "...Training: end of batch 4; log content: {'loss': 10.654231071472168, 'mae': 2.5638864040374756, 'mse': 10.654231071472168}\n",
      "...Training: end of batch 5; log content: {'loss': 10.83265209197998, 'mae': 2.566373825073242, 'mse': 10.83265209197998}\n",
      "...Training: end of batch 6; log content: {'loss': 11.272038459777832, 'mae': 2.57450270652771, 'mse': 11.272038459777832}\n",
      "...Training: end of batch 7; log content: {'loss': 11.432645797729492, 'mae': 2.5834712982177734, 'mse': 11.432645797729492}\n",
      "End epoch 88 of training; log content: {'loss': 11.432645797729492, 'mae': 2.5834712982177734, 'mse': 11.432645797729492, 'val_loss': 10.114263534545898, 'val_mae': 2.4390416145324707, 'val_mse': 10.114263534545898}\n",
      "...Training: end of batch 0; log content: {'loss': 13.477163314819336, 'mae': 2.597562551498413, 'mse': 13.477163314819336}\n",
      "...Training: end of batch 1; log content: {'loss': 12.135016441345215, 'mae': 2.601578712463379, 'mse': 12.135016441345215}\n",
      "...Training: end of batch 2; log content: {'loss': 10.321694374084473, 'mae': 2.4324309825897217, 'mse': 10.321694374084473}\n",
      "...Training: end of batch 3; log content: {'loss': 11.089300155639648, 'mae': 2.5496673583984375, 'mse': 11.089300155639648}\n",
      "...Training: end of batch 4; log content: {'loss': 10.729390144348145, 'mae': 2.5314507484436035, 'mse': 10.729390144348145}\n",
      "...Training: end of batch 5; log content: {'loss': 10.915078163146973, 'mae': 2.525740146636963, 'mse': 10.915078163146973}\n",
      "...Training: end of batch 6; log content: {'loss': 11.231626510620117, 'mae': 2.552692174911499, 'mse': 11.231626510620117}\n",
      "...Training: end of batch 7; log content: {'loss': 11.354888916015625, 'mae': 2.5631463527679443, 'mse': 11.354888916015625}\n",
      "End epoch 89 of training; log content: {'loss': 11.354888916015625, 'mae': 2.5631463527679443, 'mse': 11.354888916015625, 'val_loss': 10.196578025817871, 'val_mae': 2.4491961002349854, 'val_mse': 10.196578025817871}\n",
      "...Training: end of batch 0; log content: {'loss': 11.379267692565918, 'mae': 2.375593900680542, 'mse': 11.379267692565918}\n",
      "...Training: end of batch 1; log content: {'loss': 8.876266479492188, 'mae': 2.142516613006592, 'mse': 8.876266479492188}\n",
      "...Training: end of batch 2; log content: {'loss': 8.207571029663086, 'mae': 2.0848896503448486, 'mse': 8.207571029663086}\n",
      "...Training: end of batch 3; log content: {'loss': 12.362672805786133, 'mae': 2.5660762786865234, 'mse': 12.362672805786133}\n",
      "...Training: end of batch 4; log content: {'loss': 12.031587600708008, 'mae': 2.5516090393066406, 'mse': 12.031587600708008}\n",
      "...Training: end of batch 5; log content: {'loss': 11.574885368347168, 'mae': 2.5327999591827393, 'mse': 11.574885368347168}\n",
      "...Training: end of batch 6; log content: {'loss': 11.247212409973145, 'mae': 2.5271430015563965, 'mse': 11.247212409973145}\n",
      "...Training: end of batch 7; log content: {'loss': 11.377120018005371, 'mae': 2.5572235584259033, 'mse': 11.377120018005371}\n",
      "End epoch 90 of training; log content: {'loss': 11.377120018005371, 'mae': 2.5572235584259033, 'mse': 11.377120018005371, 'val_loss': 10.198798179626465, 'val_mae': 2.4616408348083496, 'val_mse': 10.198798179626465}\n",
      "...Training: end of batch 0; log content: {'loss': 6.792494297027588, 'mae': 2.0704972743988037, 'mse': 6.792494297027588}\n",
      "...Training: end of batch 1; log content: {'loss': 8.618759155273438, 'mae': 2.236499071121216, 'mse': 8.618759155273438}\n",
      "...Training: end of batch 2; log content: {'loss': 10.70998764038086, 'mae': 2.349552869796753, 'mse': 10.70998764038086}\n",
      "...Training: end of batch 3; log content: {'loss': 11.612393379211426, 'mae': 2.445676565170288, 'mse': 11.612393379211426}\n",
      "...Training: end of batch 4; log content: {'loss': 11.301789283752441, 'mae': 2.485360622406006, 'mse': 11.301789283752441}\n",
      "...Training: end of batch 5; log content: {'loss': 10.642027854919434, 'mae': 2.4344065189361572, 'mse': 10.642027854919434}\n",
      "...Training: end of batch 6; log content: {'loss': 11.795454025268555, 'mae': 2.562830686569214, 'mse': 11.795454025268555}\n",
      "...Training: end of batch 7; log content: {'loss': 11.381959915161133, 'mae': 2.547175407409668, 'mse': 11.381959915161133}\n",
      "End epoch 91 of training; log content: {'loss': 11.381959915161133, 'mae': 2.547175407409668, 'mse': 11.381959915161133, 'val_loss': 10.234307289123535, 'val_mae': 2.456646203994751, 'val_mse': 10.234307289123535}\n",
      "...Training: end of batch 0; log content: {'loss': 8.231559753417969, 'mae': 2.1777477264404297, 'mse': 8.231559753417969}\n",
      "...Training: end of batch 1; log content: {'loss': 8.566041946411133, 'mae': 2.195589065551758, 'mse': 8.566041946411133}\n",
      "...Training: end of batch 2; log content: {'loss': 7.3535380363464355, 'mae': 2.0647377967834473, 'mse': 7.3535380363464355}\n",
      "...Training: end of batch 3; log content: {'loss': 8.060310363769531, 'mae': 2.2168216705322266, 'mse': 8.060310363769531}\n",
      "...Training: end of batch 4; log content: {'loss': 9.763654708862305, 'mae': 2.3951237201690674, 'mse': 9.763654708862305}\n",
      "...Training: end of batch 5; log content: {'loss': 10.626544952392578, 'mae': 2.4973275661468506, 'mse': 10.626544952392578}\n",
      "...Training: end of batch 6; log content: {'loss': 11.424880981445312, 'mae': 2.550062894821167, 'mse': 11.424880981445312}\n",
      "...Training: end of batch 7; log content: {'loss': 11.412369728088379, 'mae': 2.5517466068267822, 'mse': 11.412369728088379}\n",
      "End epoch 92 of training; log content: {'loss': 11.412369728088379, 'mae': 2.5517466068267822, 'mse': 11.412369728088379, 'val_loss': 10.235640525817871, 'val_mae': 2.459993600845337, 'val_mse': 10.235640525817871}\n",
      "...Training: end of batch 0; log content: {'loss': 15.262332916259766, 'mae': 2.7859363555908203, 'mse': 15.262332916259766}\n",
      "...Training: end of batch 1; log content: {'loss': 18.263927459716797, 'mae': 3.1223649978637695, 'mse': 18.263927459716797}\n",
      "...Training: end of batch 2; log content: {'loss': 15.590497016906738, 'mae': 2.8980484008789062, 'mse': 15.590497016906738}\n",
      "...Training: end of batch 3; log content: {'loss': 13.508309364318848, 'mae': 2.737011432647705, 'mse': 13.508309364318848}\n",
      "...Training: end of batch 4; log content: {'loss': 12.734945297241211, 'mae': 2.6604084968566895, 'mse': 12.734945297241211}\n",
      "...Training: end of batch 5; log content: {'loss': 12.789515495300293, 'mae': 2.6907596588134766, 'mse': 12.789515495300293}\n",
      "...Training: end of batch 6; log content: {'loss': 12.001450538635254, 'mae': 2.619110584259033, 'mse': 12.001450538635254}\n",
      "...Training: end of batch 7; log content: {'loss': 11.609689712524414, 'mae': 2.607337713241577, 'mse': 11.609689712524414}\n",
      "End epoch 93 of training; log content: {'loss': 11.609689712524414, 'mae': 2.607337713241577, 'mse': 11.609689712524414, 'val_loss': 10.236398696899414, 'val_mae': 2.4316325187683105, 'val_mse': 10.236398696899414}\n",
      "...Training: end of batch 0; log content: {'loss': 9.919462203979492, 'mae': 2.3689064979553223, 'mse': 9.919462203979492}\n",
      "...Training: end of batch 1; log content: {'loss': 7.876458168029785, 'mae': 2.1739838123321533, 'mse': 7.876458168029785}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Training: end of batch 2; log content: {'loss': 9.048181533813477, 'mae': 2.336766004562378, 'mse': 9.048181533813477}\n",
      "...Training: end of batch 3; log content: {'loss': 9.840761184692383, 'mae': 2.3899600505828857, 'mse': 9.840761184692383}\n",
      "...Training: end of batch 4; log content: {'loss': 12.092401504516602, 'mae': 2.6098341941833496, 'mse': 12.092401504516602}\n",
      "...Training: end of batch 5; log content: {'loss': 12.012706756591797, 'mae': 2.641591787338257, 'mse': 12.012706756591797}\n",
      "...Training: end of batch 6; log content: {'loss': 11.758798599243164, 'mae': 2.6484549045562744, 'mse': 11.758798599243164}\n",
      "...Training: end of batch 7; log content: {'loss': 11.340327262878418, 'mae': 2.5844709873199463, 'mse': 11.340327262878418}\n",
      "End epoch 94 of training; log content: {'loss': 11.340327262878418, 'mae': 2.5844709873199463, 'mse': 11.340327262878418, 'val_loss': 10.257661819458008, 'val_mae': 2.4530436992645264, 'val_mse': 10.257661819458008}\n",
      "...Training: end of batch 0; log content: {'loss': 10.795221328735352, 'mae': 2.4141628742218018, 'mse': 10.795221328735352}\n",
      "...Training: end of batch 1; log content: {'loss': 10.641868591308594, 'mae': 2.457765817642212, 'mse': 10.641868591308594}\n",
      "...Training: end of batch 2; log content: {'loss': 10.284513473510742, 'mae': 2.4131107330322266, 'mse': 10.284513473510742}\n",
      "...Training: end of batch 3; log content: {'loss': 13.151793479919434, 'mae': 2.710202217102051, 'mse': 13.151793479919434}\n",
      "...Training: end of batch 4; log content: {'loss': 12.321935653686523, 'mae': 2.6465325355529785, 'mse': 12.321935653686523}\n",
      "...Training: end of batch 5; log content: {'loss': 11.605884552001953, 'mae': 2.5375335216522217, 'mse': 11.605884552001953}\n",
      "...Training: end of batch 6; log content: {'loss': 11.719513893127441, 'mae': 2.5768721103668213, 'mse': 11.719513893127441}\n",
      "...Training: end of batch 7; log content: {'loss': 11.435301780700684, 'mae': 2.55732798576355, 'mse': 11.435301780700684}\n",
      "End epoch 95 of training; log content: {'loss': 11.435301780700684, 'mae': 2.55732798576355, 'mse': 11.435301780700684, 'val_loss': 10.404475212097168, 'val_mae': 2.4835760593414307, 'val_mse': 10.404475212097168}\n",
      "...Training: end of batch 0; log content: {'loss': 15.30951976776123, 'mae': 2.8890018463134766, 'mse': 15.30951976776123}\n",
      "...Training: end of batch 1; log content: {'loss': 13.986282348632812, 'mae': 2.886852264404297, 'mse': 13.986282348632812}\n",
      "...Training: end of batch 2; log content: {'loss': 13.367732048034668, 'mae': 2.743032693862915, 'mse': 13.367732048034668}\n",
      "...Training: end of batch 3; log content: {'loss': 12.272014617919922, 'mae': 2.6711461544036865, 'mse': 12.272014617919922}\n",
      "...Training: end of batch 4; log content: {'loss': 11.212320327758789, 'mae': 2.546865224838257, 'mse': 11.212320327758789}\n",
      "...Training: end of batch 5; log content: {'loss': 10.732939720153809, 'mae': 2.4987504482269287, 'mse': 10.732939720153809}\n",
      "...Training: end of batch 6; log content: {'loss': 11.657035827636719, 'mae': 2.5565571784973145, 'mse': 11.657035827636719}\n",
      "...Training: end of batch 7; log content: {'loss': 11.400379180908203, 'mae': 2.5461432933807373, 'mse': 11.400379180908203}\n",
      "End epoch 96 of training; log content: {'loss': 11.400379180908203, 'mae': 2.5461432933807373, 'mse': 11.400379180908203, 'val_loss': 10.168892860412598, 'val_mae': 2.462216377258301, 'val_mse': 10.168892860412598}\n",
      "...Training: end of batch 0; log content: {'loss': 10.083205223083496, 'mae': 2.2520341873168945, 'mse': 10.083205223083496}\n",
      "...Training: end of batch 1; log content: {'loss': 11.743875503540039, 'mae': 2.5539605617523193, 'mse': 11.743875503540039}\n",
      "...Training: end of batch 2; log content: {'loss': 10.424378395080566, 'mae': 2.4628398418426514, 'mse': 10.424378395080566}\n",
      "...Training: end of batch 3; log content: {'loss': 10.210579872131348, 'mae': 2.482515335083008, 'mse': 10.210579872131348}\n",
      "...Training: end of batch 4; log content: {'loss': 10.814038276672363, 'mae': 2.506135940551758, 'mse': 10.814038276672363}\n",
      "...Training: end of batch 5; log content: {'loss': 10.597649574279785, 'mae': 2.471526861190796, 'mse': 10.597649574279785}\n",
      "...Training: end of batch 6; log content: {'loss': 10.355338096618652, 'mae': 2.475328207015991, 'mse': 10.355338096618652}\n",
      "...Training: end of batch 7; log content: {'loss': 11.312508583068848, 'mae': 2.5512943267822266, 'mse': 11.312508583068848}\n",
      "End epoch 97 of training; log content: {'loss': 11.312508583068848, 'mae': 2.5512943267822266, 'mse': 11.312508583068848, 'val_loss': 10.085344314575195, 'val_mae': 2.439128875732422, 'val_mse': 10.085344314575195}\n",
      "...Training: end of batch 0; log content: {'loss': 11.666579246520996, 'mae': 2.4579615592956543, 'mse': 11.666579246520996}\n",
      "...Training: end of batch 1; log content: {'loss': 10.335881233215332, 'mae': 2.444493293762207, 'mse': 10.335881233215332}\n",
      "...Training: end of batch 2; log content: {'loss': 11.168599128723145, 'mae': 2.5997445583343506, 'mse': 11.168599128723145}\n",
      "...Training: end of batch 3; log content: {'loss': 11.271505355834961, 'mae': 2.613460063934326, 'mse': 11.271505355834961}\n",
      "...Training: end of batch 4; log content: {'loss': 11.295031547546387, 'mae': 2.599719524383545, 'mse': 11.295031547546387}\n",
      "...Training: end of batch 5; log content: {'loss': 11.567387580871582, 'mae': 2.6172711849212646, 'mse': 11.567387580871582}\n",
      "...Training: end of batch 6; log content: {'loss': 11.882689476013184, 'mae': 2.625143527984619, 'mse': 11.882689476013184}\n",
      "...Training: end of batch 7; log content: {'loss': 11.44727611541748, 'mae': 2.5883970260620117, 'mse': 11.44727611541748}\n",
      "End epoch 98 of training; log content: {'loss': 11.44727611541748, 'mae': 2.5883970260620117, 'mse': 11.44727611541748, 'val_loss': 10.062925338745117, 'val_mae': 2.421877384185791, 'val_mse': 10.062925338745117}\n",
      "...Training: end of batch 0; log content: {'loss': 18.036771774291992, 'mae': 3.10105037689209, 'mse': 18.036771774291992}\n",
      "...Training: end of batch 1; log content: {'loss': 15.418425559997559, 'mae': 3.0264289379119873, 'mse': 15.418425559997559}\n",
      "...Training: end of batch 2; log content: {'loss': 12.85838794708252, 'mae': 2.700166702270508, 'mse': 12.85838794708252}\n",
      "...Training: end of batch 3; log content: {'loss': 11.957388877868652, 'mae': 2.6517927646636963, 'mse': 11.957388877868652}\n",
      "...Training: end of batch 4; log content: {'loss': 11.852933883666992, 'mae': 2.6757164001464844, 'mse': 11.852933883666992}\n",
      "...Training: end of batch 5; log content: {'loss': 12.334248542785645, 'mae': 2.7055838108062744, 'mse': 12.334248542785645}\n",
      "...Training: end of batch 6; log content: {'loss': 11.883702278137207, 'mae': 2.647111654281616, 'mse': 11.883702278137207}\n",
      "...Training: end of batch 7; log content: {'loss': 11.389180183410645, 'mae': 2.575194835662842, 'mse': 11.389180183410645}\n",
      "End epoch 99 of training; log content: {'loss': 11.389180183410645, 'mae': 2.575194835662842, 'mse': 11.389180183410645, 'val_loss': 10.230912208557129, 'val_mae': 2.464137077331543, 'val_mse': 10.230912208557129}\n",
      "...Training: end of batch 0; log content: {'loss': 16.492420196533203, 'mae': 3.0387401580810547, 'mse': 16.492420196533203}\n",
      "...Training: end of batch 1; log content: {'loss': 12.765037536621094, 'mae': 2.764193534851074, 'mse': 12.765037536621094}\n",
      "...Training: end of batch 2; log content: {'loss': 12.578987121582031, 'mae': 2.7031142711639404, 'mse': 12.578987121582031}\n",
      "...Training: end of batch 3; log content: {'loss': 11.433553695678711, 'mae': 2.548851728439331, 'mse': 11.433553695678711}\n",
      "...Training: end of batch 4; log content: {'loss': 11.463663101196289, 'mae': 2.5574734210968018, 'mse': 11.463663101196289}\n",
      "...Training: end of batch 5; log content: {'loss': 11.264484405517578, 'mae': 2.5493061542510986, 'mse': 11.264484405517578}\n",
      "...Training: end of batch 6; log content: {'loss': 11.158003807067871, 'mae': 2.541196584701538, 'mse': 11.158003807067871}\n",
      "...Training: end of batch 7; log content: {'loss': 11.346911430358887, 'mae': 2.5480854511260986, 'mse': 11.346911430358887}\n",
      "End epoch 100 of training; log content: {'loss': 11.346911430358887, 'mae': 2.5480854511260986, 'mse': 11.346911430358887, 'val_loss': 10.183246612548828, 'val_mae': 2.4534449577331543, 'val_mse': 10.183246612548828}\n",
      "...Training: end of batch 0; log content: {'loss': 8.992288589477539, 'mae': 2.413294553756714, 'mse': 8.992288589477539}\n",
      "...Training: end of batch 1; log content: {'loss': 10.29640007019043, 'mae': 2.5013346672058105, 'mse': 10.29640007019043}\n",
      "...Training: end of batch 2; log content: {'loss': 10.012787818908691, 'mae': 2.468717098236084, 'mse': 10.012787818908691}\n",
      "...Training: end of batch 3; log content: {'loss': 10.632312774658203, 'mae': 2.4955837726593018, 'mse': 10.632312774658203}\n",
      "...Training: end of batch 4; log content: {'loss': 10.513274192810059, 'mae': 2.5290207862854004, 'mse': 10.513274192810059}\n",
      "...Training: end of batch 5; log content: {'loss': 10.47528076171875, 'mae': 2.5051934719085693, 'mse': 10.47528076171875}\n",
      "...Training: end of batch 6; log content: {'loss': 11.025960922241211, 'mae': 2.514721632003784, 'mse': 11.025960922241211}\n",
      "...Training: end of batch 7; log content: {'loss': 11.432920455932617, 'mae': 2.549837112426758, 'mse': 11.432920455932617}\n",
      "End epoch 101 of training; log content: {'loss': 11.432920455932617, 'mae': 2.549837112426758, 'mse': 11.432920455932617, 'val_loss': 10.253705024719238, 'val_mae': 2.462164878845215, 'val_mse': 10.253705024719238}\n",
      "...Training: end of batch 0; log content: {'loss': 9.656171798706055, 'mae': 2.360842704772949, 'mse': 9.656171798706055}\n",
      "...Training: end of batch 1; log content: {'loss': 10.55135726928711, 'mae': 2.5223731994628906, 'mse': 10.55135726928711}\n",
      "...Training: end of batch 2; log content: {'loss': 10.79344654083252, 'mae': 2.4331347942352295, 'mse': 10.79344654083252}\n",
      "...Training: end of batch 3; log content: {'loss': 11.462556838989258, 'mae': 2.5411696434020996, 'mse': 11.462556838989258}\n",
      "...Training: end of batch 4; log content: {'loss': 11.859610557556152, 'mae': 2.633274793624878, 'mse': 11.859610557556152}\n",
      "...Training: end of batch 5; log content: {'loss': 11.020713806152344, 'mae': 2.5351130962371826, 'mse': 11.020713806152344}\n",
      "...Training: end of batch 6; log content: {'loss': 11.14605712890625, 'mae': 2.5471320152282715, 'mse': 11.14605712890625}\n",
      "...Training: end of batch 7; log content: {'loss': 11.317509651184082, 'mae': 2.5565736293792725, 'mse': 11.317509651184082}\n",
      "End epoch 102 of training; log content: {'loss': 11.317509651184082, 'mae': 2.5565736293792725, 'mse': 11.317509651184082, 'val_loss': 10.151391983032227, 'val_mae': 2.4348697662353516, 'val_mse': 10.151391983032227}\n",
      "...Training: end of batch 0; log content: {'loss': 8.941107749938965, 'mae': 2.241976022720337, 'mse': 8.941107749938965}\n",
      "...Training: end of batch 1; log content: {'loss': 11.913793563842773, 'mae': 2.4463248252868652, 'mse': 11.913793563842773}\n",
      "...Training: end of batch 2; log content: {'loss': 12.284558296203613, 'mae': 2.5467777252197266, 'mse': 12.284558296203613}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Training: end of batch 3; log content: {'loss': 12.283809661865234, 'mae': 2.6045279502868652, 'mse': 12.283809661865234}\n",
      "...Training: end of batch 4; log content: {'loss': 11.806069374084473, 'mae': 2.5780189037323, 'mse': 11.806069374084473}\n",
      "...Training: end of batch 5; log content: {'loss': 11.243138313293457, 'mae': 2.5275561809539795, 'mse': 11.243138313293457}\n",
      "...Training: end of batch 6; log content: {'loss': 11.667283058166504, 'mae': 2.5884616374969482, 'mse': 11.667283058166504}\n",
      "...Training: end of batch 7; log content: {'loss': 11.369519233703613, 'mae': 2.580326557159424, 'mse': 11.369519233703613}\n",
      "End epoch 103 of training; log content: {'loss': 11.369519233703613, 'mae': 2.580326557159424, 'mse': 11.369519233703613, 'val_loss': 10.07176399230957, 'val_mae': 2.429921865463257, 'val_mse': 10.07176399230957}\n",
      "...Training: end of batch 0; log content: {'loss': 15.642499923706055, 'mae': 2.772655963897705, 'mse': 15.642499923706055}\n",
      "...Training: end of batch 1; log content: {'loss': 11.139171600341797, 'mae': 2.3473331928253174, 'mse': 11.139171600341797}\n",
      "...Training: end of batch 2; log content: {'loss': 11.362953186035156, 'mae': 2.427459955215454, 'mse': 11.362953186035156}\n",
      "...Training: end of batch 3; log content: {'loss': 12.24500560760498, 'mae': 2.6014435291290283, 'mse': 12.24500560760498}\n",
      "...Training: end of batch 4; log content: {'loss': 11.965707778930664, 'mae': 2.6291847229003906, 'mse': 11.965707778930664}\n",
      "...Training: end of batch 5; log content: {'loss': 12.430689811706543, 'mae': 2.680575132369995, 'mse': 12.430689811706543}\n",
      "...Training: end of batch 6; log content: {'loss': 11.804628372192383, 'mae': 2.60565447807312, 'mse': 11.804628372192383}\n",
      "...Training: end of batch 7; log content: {'loss': 11.326866149902344, 'mae': 2.5696372985839844, 'mse': 11.326866149902344}\n",
      "End epoch 104 of training; log content: {'loss': 11.326866149902344, 'mae': 2.5696372985839844, 'mse': 11.326866149902344, 'val_loss': 10.069880485534668, 'val_mae': 2.4427077770233154, 'val_mse': 10.069880485534668}\n",
      "...Training: end of batch 0; log content: {'loss': 13.006114959716797, 'mae': 3.070838451385498, 'mse': 13.006114959716797}\n",
      "...Training: end of batch 1; log content: {'loss': 10.447269439697266, 'mae': 2.66624116897583, 'mse': 10.447269439697266}\n",
      "...Training: end of batch 2; log content: {'loss': 10.595280647277832, 'mae': 2.6883962154388428, 'mse': 10.595280647277832}\n",
      "...Training: end of batch 3; log content: {'loss': 10.098541259765625, 'mae': 2.6048309803009033, 'mse': 10.098541259765625}\n",
      "...Training: end of batch 4; log content: {'loss': 10.530998229980469, 'mae': 2.6002910137176514, 'mse': 10.530998229980469}\n",
      "...Training: end of batch 5; log content: {'loss': 11.252238273620605, 'mae': 2.630624294281006, 'mse': 11.252238273620605}\n",
      "...Training: end of batch 6; log content: {'loss': 10.986322402954102, 'mae': 2.5806949138641357, 'mse': 10.986322402954102}\n",
      "...Training: end of batch 7; log content: {'loss': 11.34100341796875, 'mae': 2.553854465484619, 'mse': 11.34100341796875}\n",
      "End epoch 105 of training; log content: {'loss': 11.34100341796875, 'mae': 2.553854465484619, 'mse': 11.34100341796875, 'val_loss': 10.136597633361816, 'val_mae': 2.4530978202819824, 'val_mse': 10.136597633361816}\n",
      "...Training: end of batch 0; log content: {'loss': 11.28465461730957, 'mae': 2.5940964221954346, 'mse': 11.28465461730957}\n",
      "...Training: end of batch 1; log content: {'loss': 12.951274871826172, 'mae': 2.7000226974487305, 'mse': 12.951274871826172}\n",
      "...Training: end of batch 2; log content: {'loss': 15.614913940429688, 'mae': 2.9383151531219482, 'mse': 15.614913940429688}\n",
      "...Training: end of batch 3; log content: {'loss': 14.134145736694336, 'mae': 2.836207151412964, 'mse': 14.134145736694336}\n",
      "...Training: end of batch 4; log content: {'loss': 12.432756423950195, 'mae': 2.6335456371307373, 'mse': 12.432756423950195}\n",
      "...Training: end of batch 5; log content: {'loss': 11.530059814453125, 'mae': 2.551135778427124, 'mse': 11.530059814453125}\n",
      "...Training: end of batch 6; log content: {'loss': 11.412930488586426, 'mae': 2.564122438430786, 'mse': 11.412930488586426}\n",
      "...Training: end of batch 7; log content: {'loss': 11.363631248474121, 'mae': 2.5696914196014404, 'mse': 11.363631248474121}\n",
      "End epoch 106 of training; log content: {'loss': 11.363631248474121, 'mae': 2.5696914196014404, 'mse': 11.363631248474121, 'val_loss': 10.068229675292969, 'val_mae': 2.4381699562072754, 'val_mse': 10.068229675292969}\n",
      "...Training: end of batch 0; log content: {'loss': 7.499086380004883, 'mae': 2.1573798656463623, 'mse': 7.499086380004883}\n",
      "...Training: end of batch 1; log content: {'loss': 9.354619026184082, 'mae': 2.479269027709961, 'mse': 9.354619026184082}\n",
      "...Training: end of batch 2; log content: {'loss': 9.593900680541992, 'mae': 2.4996232986450195, 'mse': 9.593900680541992}\n",
      "...Training: end of batch 3; log content: {'loss': 11.625564575195312, 'mae': 2.7076168060302734, 'mse': 11.625564575195312}\n",
      "...Training: end of batch 4; log content: {'loss': 11.327291488647461, 'mae': 2.6129279136657715, 'mse': 11.327291488647461}\n",
      "...Training: end of batch 5; log content: {'loss': 11.71860408782959, 'mae': 2.5996510982513428, 'mse': 11.71860408782959}\n",
      "...Training: end of batch 6; log content: {'loss': 11.633135795593262, 'mae': 2.624987840652466, 'mse': 11.633135795593262}\n",
      "...Training: end of batch 7; log content: {'loss': 11.336976051330566, 'mae': 2.576362133026123, 'mse': 11.336976051330566}\n",
      "End epoch 107 of training; log content: {'loss': 11.336976051330566, 'mae': 2.576362133026123, 'mse': 11.336976051330566, 'val_loss': 10.10733413696289, 'val_mae': 2.435206890106201, 'val_mse': 10.10733413696289}\n",
      "...Training: end of batch 0; log content: {'loss': 9.195455551147461, 'mae': 2.504781723022461, 'mse': 9.195455551147461}\n",
      "...Training: end of batch 1; log content: {'loss': 11.784231185913086, 'mae': 2.7558350563049316, 'mse': 11.784231185913086}\n",
      "...Training: end of batch 2; log content: {'loss': 11.06678295135498, 'mae': 2.61034893989563, 'mse': 11.06678295135498}\n",
      "...Training: end of batch 3; log content: {'loss': 12.604788780212402, 'mae': 2.7905585765838623, 'mse': 12.604788780212402}\n",
      "...Training: end of batch 4; log content: {'loss': 12.567375183105469, 'mae': 2.775435209274292, 'mse': 12.567375183105469}\n",
      "...Training: end of batch 5; log content: {'loss': 11.737278938293457, 'mae': 2.682845115661621, 'mse': 11.737278938293457}\n",
      "...Training: end of batch 6; log content: {'loss': 11.729021072387695, 'mae': 2.6337203979492188, 'mse': 11.729021072387695}\n",
      "...Training: end of batch 7; log content: {'loss': 11.315402030944824, 'mae': 2.571472644805908, 'mse': 11.315402030944824}\n",
      "End epoch 108 of training; log content: {'loss': 11.315402030944824, 'mae': 2.571472644805908, 'mse': 11.315402030944824, 'val_loss': 10.206974029541016, 'val_mae': 2.457026243209839, 'val_mse': 10.206974029541016}\n",
      "...Training: end of batch 0; log content: {'loss': 11.620000839233398, 'mae': 2.3977973461151123, 'mse': 11.620000839233398}\n",
      "...Training: end of batch 1; log content: {'loss': 10.2857027053833, 'mae': 2.4968905448913574, 'mse': 10.2857027053833}\n",
      "...Training: end of batch 2; log content: {'loss': 13.253791809082031, 'mae': 2.7168853282928467, 'mse': 13.253791809082031}\n",
      "...Training: end of batch 3; log content: {'loss': 12.282341003417969, 'mae': 2.5920827388763428, 'mse': 12.282341003417969}\n",
      "...Training: end of batch 4; log content: {'loss': 11.24714469909668, 'mae': 2.483402729034424, 'mse': 11.24714469909668}\n",
      "...Training: end of batch 5; log content: {'loss': 10.698966979980469, 'mae': 2.435124397277832, 'mse': 10.698966979980469}\n",
      "...Training: end of batch 6; log content: {'loss': 11.329317092895508, 'mae': 2.5190043449401855, 'mse': 11.329317092895508}\n",
      "...Training: end of batch 7; log content: {'loss': 11.37930679321289, 'mae': 2.55806040763855, 'mse': 11.37930679321289}\n",
      "End epoch 109 of training; log content: {'loss': 11.37930679321289, 'mae': 2.55806040763855, 'mse': 11.37930679321289, 'val_loss': 10.25359058380127, 'val_mae': 2.4695327281951904, 'val_mse': 10.25359058380127}\n",
      "...Training: end of batch 0; log content: {'loss': 13.944368362426758, 'mae': 2.734506607055664, 'mse': 13.944368362426758}\n",
      "...Training: end of batch 1; log content: {'loss': 14.340526580810547, 'mae': 2.8561198711395264, 'mse': 14.340526580810547}\n",
      "...Training: end of batch 2; log content: {'loss': 13.35649585723877, 'mae': 2.7474682331085205, 'mse': 13.35649585723877}\n",
      "...Training: end of batch 3; log content: {'loss': 11.98161792755127, 'mae': 2.6218421459198, 'mse': 11.98161792755127}\n",
      "...Training: end of batch 4; log content: {'loss': 12.33775520324707, 'mae': 2.6186559200286865, 'mse': 12.33775520324707}\n",
      "...Training: end of batch 5; log content: {'loss': 11.877446174621582, 'mae': 2.5967679023742676, 'mse': 11.877446174621582}\n",
      "...Training: end of batch 6; log content: {'loss': 11.640337944030762, 'mae': 2.589702606201172, 'mse': 11.640337944030762}\n",
      "...Training: end of batch 7; log content: {'loss': 11.341060638427734, 'mae': 2.5619356632232666, 'mse': 11.341060638427734}\n",
      "End epoch 110 of training; log content: {'loss': 11.341060638427734, 'mae': 2.5619356632232666, 'mse': 11.341060638427734, 'val_loss': 10.125893592834473, 'val_mae': 2.4331531524658203, 'val_mse': 10.125893592834473}\n",
      "...Training: end of batch 0; log content: {'loss': 8.826480865478516, 'mae': 2.128268003463745, 'mse': 8.826480865478516}\n",
      "...Training: end of batch 1; log content: {'loss': 12.071117401123047, 'mae': 2.512791633605957, 'mse': 12.071117401123047}\n",
      "...Training: end of batch 2; log content: {'loss': 11.847248077392578, 'mae': 2.566931962966919, 'mse': 11.847248077392578}\n",
      "...Training: end of batch 3; log content: {'loss': 10.647594451904297, 'mae': 2.4649524688720703, 'mse': 10.647594451904297}\n",
      "...Training: end of batch 4; log content: {'loss': 11.196187973022461, 'mae': 2.5361008644104004, 'mse': 11.196187973022461}\n",
      "...Training: end of batch 5; log content: {'loss': 11.543993949890137, 'mae': 2.5941901206970215, 'mse': 11.543993949890137}\n",
      "...Training: end of batch 6; log content: {'loss': 11.39470386505127, 'mae': 2.556580066680908, 'mse': 11.39470386505127}\n",
      "...Training: end of batch 7; log content: {'loss': 11.360349655151367, 'mae': 2.585927724838257, 'mse': 11.360349655151367}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End epoch 111 of training; log content: {'loss': 11.360349655151367, 'mae': 2.585927724838257, 'mse': 11.360349655151367, 'val_loss': 10.078371047973633, 'val_mae': 2.431715726852417, 'val_mse': 10.078371047973633}\n",
      "...Training: end of batch 0; log content: {'loss': 5.728188514709473, 'mae': 2.0338966846466064, 'mse': 5.728188514709473}\n",
      "...Training: end of batch 1; log content: {'loss': 8.649191856384277, 'mae': 2.3612937927246094, 'mse': 8.649191856384277}\n",
      "...Training: end of batch 2; log content: {'loss': 10.046685218811035, 'mae': 2.3580052852630615, 'mse': 10.046685218811035}\n",
      "...Training: end of batch 3; log content: {'loss': 10.109508514404297, 'mae': 2.3672149181365967, 'mse': 10.109508514404297}\n",
      "...Training: end of batch 4; log content: {'loss': 10.394903182983398, 'mae': 2.4409701824188232, 'mse': 10.394903182983398}\n",
      "...Training: end of batch 5; log content: {'loss': 10.401373863220215, 'mae': 2.4355785846710205, 'mse': 10.401373863220215}\n",
      "...Training: end of batch 6; log content: {'loss': 11.068564414978027, 'mae': 2.5258004665374756, 'mse': 11.068564414978027}\n",
      "...Training: end of batch 7; log content: {'loss': 11.280680656433105, 'mae': 2.554020404815674, 'mse': 11.280680656433105}\n",
      "End epoch 112 of training; log content: {'loss': 11.280680656433105, 'mae': 2.554020404815674, 'mse': 11.280680656433105, 'val_loss': 10.154528617858887, 'val_mae': 2.4523396492004395, 'val_mse': 10.154528617858887}\n",
      "...Training: end of batch 0; log content: {'loss': 9.905584335327148, 'mae': 2.5390303134918213, 'mse': 9.905584335327148}\n",
      "...Training: end of batch 1; log content: {'loss': 12.98054313659668, 'mae': 2.783501625061035, 'mse': 12.98054313659668}\n",
      "...Training: end of batch 2; log content: {'loss': 11.97705078125, 'mae': 2.697050094604492, 'mse': 11.97705078125}\n",
      "...Training: end of batch 3; log content: {'loss': 11.119037628173828, 'mae': 2.553241491317749, 'mse': 11.119037628173828}\n",
      "...Training: end of batch 4; log content: {'loss': 11.724687576293945, 'mae': 2.5744194984436035, 'mse': 11.724687576293945}\n",
      "...Training: end of batch 5; log content: {'loss': 11.31566333770752, 'mae': 2.518697500228882, 'mse': 11.31566333770752}\n",
      "...Training: end of batch 6; log content: {'loss': 11.40047550201416, 'mae': 2.5331637859344482, 'mse': 11.40047550201416}\n",
      "...Training: end of batch 7; log content: {'loss': 11.491965293884277, 'mae': 2.55880069732666, 'mse': 11.491965293884277}\n",
      "End epoch 113 of training; log content: {'loss': 11.491965293884277, 'mae': 2.55880069732666, 'mse': 11.491965293884277, 'val_loss': 10.157852172851562, 'val_mae': 2.4685871601104736, 'val_mse': 10.157852172851562}\n",
      "...Training: end of batch 0; log content: {'loss': 12.937914848327637, 'mae': 2.72776460647583, 'mse': 12.937914848327637}\n",
      "...Training: end of batch 1; log content: {'loss': 15.149717330932617, 'mae': 2.986943006515503, 'mse': 15.149717330932617}\n",
      "...Training: end of batch 2; log content: {'loss': 13.854748725891113, 'mae': 2.851423978805542, 'mse': 13.854748725891113}\n",
      "...Training: end of batch 3; log content: {'loss': 12.020376205444336, 'mae': 2.6677422523498535, 'mse': 12.020376205444336}\n",
      "...Training: end of batch 4; log content: {'loss': 11.248942375183105, 'mae': 2.5780436992645264, 'mse': 11.248942375183105}\n",
      "...Training: end of batch 5; log content: {'loss': 10.819212913513184, 'mae': 2.5360937118530273, 'mse': 10.819212913513184}\n",
      "...Training: end of batch 6; log content: {'loss': 10.824773788452148, 'mae': 2.5433058738708496, 'mse': 10.824773788452148}\n",
      "...Training: end of batch 7; log content: {'loss': 11.398477554321289, 'mae': 2.5766396522521973, 'mse': 11.398477554321289}\n",
      "End epoch 114 of training; log content: {'loss': 11.398477554321289, 'mae': 2.5766396522521973, 'mse': 11.398477554321289, 'val_loss': 9.975919723510742, 'val_mae': 2.421161413192749, 'val_mse': 9.975919723510742}\n",
      "...Training: end of batch 0; log content: {'loss': 9.679170608520508, 'mae': 2.487168073654175, 'mse': 9.679170608520508}\n",
      "...Training: end of batch 1; log content: {'loss': 11.769916534423828, 'mae': 2.6689085960388184, 'mse': 11.769916534423828}\n",
      "...Training: end of batch 2; log content: {'loss': 11.027636528015137, 'mae': 2.6080856323242188, 'mse': 11.027636528015137}\n",
      "...Training: end of batch 3; log content: {'loss': 11.618754386901855, 'mae': 2.6423068046569824, 'mse': 11.618754386901855}\n",
      "...Training: end of batch 4; log content: {'loss': 11.019880294799805, 'mae': 2.596083402633667, 'mse': 11.019880294799805}\n",
      "...Training: end of batch 5; log content: {'loss': 11.537193298339844, 'mae': 2.6264657974243164, 'mse': 11.537193298339844}\n",
      "...Training: end of batch 6; log content: {'loss': 11.07481861114502, 'mae': 2.5580084323883057, 'mse': 11.07481861114502}\n",
      "...Training: end of batch 7; log content: {'loss': 11.378104209899902, 'mae': 2.580031394958496, 'mse': 11.378104209899902}\n",
      "End epoch 115 of training; log content: {'loss': 11.378104209899902, 'mae': 2.580031394958496, 'mse': 11.378104209899902, 'val_loss': 10.16374397277832, 'val_mae': 2.4467756748199463, 'val_mse': 10.16374397277832}\n",
      "...Training: end of batch 0; log content: {'loss': 13.999692916870117, 'mae': 2.8497793674468994, 'mse': 13.999692916870117}\n",
      "...Training: end of batch 1; log content: {'loss': 13.775643348693848, 'mae': 2.765481948852539, 'mse': 13.775643348693848}\n",
      "...Training: end of batch 2; log content: {'loss': 11.89168643951416, 'mae': 2.5814290046691895, 'mse': 11.89168643951416}\n",
      "...Training: end of batch 3; log content: {'loss': 11.119298934936523, 'mae': 2.5010037422180176, 'mse': 11.119298934936523}\n",
      "...Training: end of batch 4; log content: {'loss': 11.326493263244629, 'mae': 2.5557940006256104, 'mse': 11.326493263244629}\n",
      "...Training: end of batch 5; log content: {'loss': 11.77576732635498, 'mae': 2.5944457054138184, 'mse': 11.77576732635498}\n",
      "...Training: end of batch 6; log content: {'loss': 11.416597366333008, 'mae': 2.5683445930480957, 'mse': 11.416597366333008}\n",
      "...Training: end of batch 7; log content: {'loss': 11.31993579864502, 'mae': 2.5616838932037354, 'mse': 11.31993579864502}\n",
      "End epoch 116 of training; log content: {'loss': 11.31993579864502, 'mae': 2.5616838932037354, 'mse': 11.31993579864502, 'val_loss': 10.199968338012695, 'val_mae': 2.448061943054199, 'val_mse': 10.199968338012695}\n",
      "...Training: end of batch 0; log content: {'loss': 11.907610893249512, 'mae': 2.7291743755340576, 'mse': 11.907610893249512}\n",
      "...Training: end of batch 1; log content: {'loss': 8.17449951171875, 'mae': 2.2661848068237305, 'mse': 8.17449951171875}\n",
      "...Training: end of batch 2; log content: {'loss': 10.359854698181152, 'mae': 2.5035672187805176, 'mse': 10.359854698181152}\n",
      "...Training: end of batch 3; log content: {'loss': 11.692243576049805, 'mae': 2.626858711242676, 'mse': 11.692243576049805}\n",
      "...Training: end of batch 4; log content: {'loss': 11.841840744018555, 'mae': 2.6393630504608154, 'mse': 11.841840744018555}\n",
      "...Training: end of batch 5; log content: {'loss': 11.420140266418457, 'mae': 2.590820550918579, 'mse': 11.420140266418457}\n",
      "...Training: end of batch 6; log content: {'loss': 10.891313552856445, 'mae': 2.5330960750579834, 'mse': 10.891313552856445}\n",
      "...Training: end of batch 7; log content: {'loss': 11.32045841217041, 'mae': 2.574676275253296, 'mse': 11.32045841217041}\n",
      "End epoch 117 of training; log content: {'loss': 11.32045841217041, 'mae': 2.574676275253296, 'mse': 11.32045841217041, 'val_loss': 10.079106330871582, 'val_mae': 2.4347593784332275, 'val_mse': 10.079106330871582}\n",
      "...Training: end of batch 0; log content: {'loss': 9.949283599853516, 'mae': 2.7114529609680176, 'mse': 9.949283599853516}\n",
      "...Training: end of batch 1; log content: {'loss': 9.767937660217285, 'mae': 2.442003011703491, 'mse': 9.767937660217285}\n",
      "...Training: end of batch 2; log content: {'loss': 9.894499778747559, 'mae': 2.483452558517456, 'mse': 9.894499778747559}\n",
      "...Training: end of batch 3; log content: {'loss': 8.319588661193848, 'mae': 2.2747201919555664, 'mse': 8.319588661193848}\n",
      "...Training: end of batch 4; log content: {'loss': 8.903249740600586, 'mae': 2.3596315383911133, 'mse': 8.903249740600586}\n",
      "...Training: end of batch 5; log content: {'loss': 9.235568046569824, 'mae': 2.4052064418792725, 'mse': 9.235568046569824}\n",
      "...Training: end of batch 6; log content: {'loss': 9.326762199401855, 'mae': 2.4107158184051514, 'mse': 9.326762199401855}\n",
      "...Training: end of batch 7; log content: {'loss': 11.412294387817383, 'mae': 2.5666770935058594, 'mse': 11.412294387817383}\n",
      "End epoch 118 of training; log content: {'loss': 11.412294387817383, 'mae': 2.5666770935058594, 'mse': 11.412294387817383, 'val_loss': 10.209802627563477, 'val_mae': 2.466498851776123, 'val_mse': 10.209802627563477}\n",
      "...Training: end of batch 0; log content: {'loss': 9.844927787780762, 'mae': 2.343292713165283, 'mse': 9.844927787780762}\n",
      "...Training: end of batch 1; log content: {'loss': 11.009056091308594, 'mae': 2.443399429321289, 'mse': 11.009056091308594}\n",
      "...Training: end of batch 2; log content: {'loss': 12.031440734863281, 'mae': 2.6147077083587646, 'mse': 12.031440734863281}\n",
      "...Training: end of batch 3; log content: {'loss': 13.72329044342041, 'mae': 2.7705464363098145, 'mse': 13.72329044342041}\n",
      "...Training: end of batch 4; log content: {'loss': 12.93840503692627, 'mae': 2.7112228870391846, 'mse': 12.93840503692627}\n",
      "...Training: end of batch 5; log content: {'loss': 12.821696281433105, 'mae': 2.7026994228363037, 'mse': 12.821696281433105}\n",
      "...Training: end of batch 6; log content: {'loss': 11.902342796325684, 'mae': 2.6209170818328857, 'mse': 11.902342796325684}\n",
      "...Training: end of batch 7; log content: {'loss': 11.381734848022461, 'mae': 2.5745339393615723, 'mse': 11.381734848022461}\n",
      "End epoch 119 of training; log content: {'loss': 11.381734848022461, 'mae': 2.5745339393615723, 'mse': 11.381734848022461, 'val_loss': 10.074431419372559, 'val_mae': 2.431335926055908, 'val_mse': 10.074431419372559}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Training: end of batch 0; log content: {'loss': 14.621796607971191, 'mae': 2.9855029582977295, 'mse': 14.621796607971191}\n",
      "...Training: end of batch 1; log content: {'loss': 11.047760009765625, 'mae': 2.6430020332336426, 'mse': 11.047760009765625}\n",
      "...Training: end of batch 2; log content: {'loss': 11.862040519714355, 'mae': 2.6891214847564697, 'mse': 11.862040519714355}\n",
      "...Training: end of batch 3; log content: {'loss': 11.529729843139648, 'mae': 2.5956473350524902, 'mse': 11.529729843139648}\n",
      "...Training: end of batch 4; log content: {'loss': 11.087690353393555, 'mae': 2.565415859222412, 'mse': 11.087690353393555}\n",
      "...Training: end of batch 5; log content: {'loss': 10.77916431427002, 'mae': 2.532031774520874, 'mse': 10.77916431427002}\n",
      "...Training: end of batch 6; log content: {'loss': 10.736971855163574, 'mae': 2.5389304161071777, 'mse': 10.736971855163574}\n",
      "...Training: end of batch 7; log content: {'loss': 11.383538246154785, 'mae': 2.575547218322754, 'mse': 11.383538246154785}\n",
      "End epoch 120 of training; log content: {'loss': 11.383538246154785, 'mae': 2.575547218322754, 'mse': 11.383538246154785, 'val_loss': 10.040515899658203, 'val_mae': 2.4541661739349365, 'val_mse': 10.040515899658203}\n",
      "...Training: end of batch 0; log content: {'loss': 13.67302131652832, 'mae': 3.0530591011047363, 'mse': 13.67302131652832}\n",
      "...Training: end of batch 1; log content: {'loss': 10.897642135620117, 'mae': 2.6211111545562744, 'mse': 10.897642135620117}\n",
      "...Training: end of batch 2; log content: {'loss': 10.930848121643066, 'mae': 2.6355106830596924, 'mse': 10.930848121643066}\n",
      "...Training: end of batch 3; log content: {'loss': 10.814801216125488, 'mae': 2.5989062786102295, 'mse': 10.814801216125488}\n",
      "...Training: end of batch 4; log content: {'loss': 12.051274299621582, 'mae': 2.667940139770508, 'mse': 12.051274299621582}\n",
      "...Training: end of batch 5; log content: {'loss': 12.312286376953125, 'mae': 2.654287099838257, 'mse': 12.312286376953125}\n",
      "...Training: end of batch 6; log content: {'loss': 11.905427932739258, 'mae': 2.63203501701355, 'mse': 11.905427932739258}\n",
      "...Training: end of batch 7; log content: {'loss': 11.300928115844727, 'mae': 2.5715651512145996, 'mse': 11.300928115844727}\n",
      "End epoch 121 of training; log content: {'loss': 11.300928115844727, 'mae': 2.5715651512145996, 'mse': 11.300928115844727, 'val_loss': 10.02895736694336, 'val_mae': 2.43056583404541, 'val_mse': 10.02895736694336}\n",
      "...Training: end of batch 0; log content: {'loss': 8.218576431274414, 'mae': 2.2064104080200195, 'mse': 8.218576431274414}\n",
      "...Training: end of batch 1; log content: {'loss': 9.600997924804688, 'mae': 2.3530898094177246, 'mse': 9.600997924804688}\n",
      "...Training: end of batch 2; log content: {'loss': 11.283485412597656, 'mae': 2.571398973464966, 'mse': 11.283485412597656}\n",
      "...Training: end of batch 3; log content: {'loss': 10.711870193481445, 'mae': 2.5393166542053223, 'mse': 10.711870193481445}\n",
      "...Training: end of batch 4; log content: {'loss': 12.256860733032227, 'mae': 2.674097776412964, 'mse': 12.256860733032227}\n",
      "...Training: end of batch 5; log content: {'loss': 11.76020336151123, 'mae': 2.5954625606536865, 'mse': 11.76020336151123}\n",
      "...Training: end of batch 6; log content: {'loss': 12.021498680114746, 'mae': 2.6628530025482178, 'mse': 12.021498680114746}\n",
      "...Training: end of batch 7; log content: {'loss': 11.340593338012695, 'mae': 2.5810647010803223, 'mse': 11.340593338012695}\n",
      "End epoch 122 of training; log content: {'loss': 11.340593338012695, 'mae': 2.5810647010803223, 'mse': 11.340593338012695, 'val_loss': 10.11376667022705, 'val_mae': 2.4522316455841064, 'val_mse': 10.11376667022705}\n",
      "...Training: end of batch 0; log content: {'loss': 10.28738021850586, 'mae': 2.2156310081481934, 'mse': 10.28738021850586}\n",
      "...Training: end of batch 1; log content: {'loss': 9.820880889892578, 'mae': 2.371919631958008, 'mse': 9.820880889892578}\n",
      "...Training: end of batch 2; log content: {'loss': 10.07610034942627, 'mae': 2.4680652618408203, 'mse': 10.07610034942627}\n",
      "...Training: end of batch 3; log content: {'loss': 10.47831916809082, 'mae': 2.4784059524536133, 'mse': 10.47831916809082}\n",
      "...Training: end of batch 4; log content: {'loss': 11.453489303588867, 'mae': 2.577056407928467, 'mse': 11.453489303588867}\n",
      "...Training: end of batch 5; log content: {'loss': 11.778260231018066, 'mae': 2.5888235569000244, 'mse': 11.778260231018066}\n",
      "...Training: end of batch 6; log content: {'loss': 11.302115440368652, 'mae': 2.54191517829895, 'mse': 11.302115440368652}\n",
      "...Training: end of batch 7; log content: {'loss': 11.325927734375, 'mae': 2.566922664642334, 'mse': 11.325927734375}\n",
      "End epoch 123 of training; log content: {'loss': 11.325927734375, 'mae': 2.566922664642334, 'mse': 11.325927734375, 'val_loss': 10.049813270568848, 'val_mae': 2.4409964084625244, 'val_mse': 10.049813270568848}\n",
      "...Training: end of batch 0; log content: {'loss': 6.37657356262207, 'mae': 2.041565418243408, 'mse': 6.37657356262207}\n",
      "...Training: end of batch 1; log content: {'loss': 6.3749566078186035, 'mae': 2.037318706512451, 'mse': 6.3749566078186035}\n",
      "...Training: end of batch 2; log content: {'loss': 6.8606181144714355, 'mae': 2.128030300140381, 'mse': 6.8606181144714355}\n",
      "...Training: end of batch 3; log content: {'loss': 7.61059045791626, 'mae': 2.1722145080566406, 'mse': 7.61059045791626}\n",
      "...Training: end of batch 4; log content: {'loss': 9.426005363464355, 'mae': 2.312122344970703, 'mse': 9.426005363464355}\n",
      "...Training: end of batch 5; log content: {'loss': 9.752270698547363, 'mae': 2.369016408920288, 'mse': 9.752270698547363}\n",
      "...Training: end of batch 6; log content: {'loss': 10.783976554870605, 'mae': 2.4846065044403076, 'mse': 10.783976554870605}\n",
      "...Training: end of batch 7; log content: {'loss': 11.39041805267334, 'mae': 2.56410551071167, 'mse': 11.39041805267334}\n",
      "End epoch 124 of training; log content: {'loss': 11.39041805267334, 'mae': 2.56410551071167, 'mse': 11.39041805267334, 'val_loss': 10.099440574645996, 'val_mae': 2.43888783454895, 'val_mse': 10.099440574645996}\n",
      "...Training: end of batch 0; log content: {'loss': 14.111795425415039, 'mae': 2.8791701793670654, 'mse': 14.111795425415039}\n",
      "...Training: end of batch 1; log content: {'loss': 13.194075584411621, 'mae': 2.750459671020508, 'mse': 13.194075584411621}\n",
      "...Training: end of batch 2; log content: {'loss': 13.092723846435547, 'mae': 2.6500627994537354, 'mse': 13.092723846435547}\n",
      "...Training: end of batch 3; log content: {'loss': 12.47089958190918, 'mae': 2.6748626232147217, 'mse': 12.47089958190918}\n",
      "...Training: end of batch 4; log content: {'loss': 11.977218627929688, 'mae': 2.6403679847717285, 'mse': 11.977218627929688}\n",
      "...Training: end of batch 5; log content: {'loss': 11.708527565002441, 'mae': 2.6310746669769287, 'mse': 11.708527565002441}\n",
      "...Training: end of batch 6; log content: {'loss': 11.691070556640625, 'mae': 2.630812168121338, 'mse': 11.691070556640625}\n",
      "...Training: end of batch 7; log content: {'loss': 11.317381858825684, 'mae': 2.570704698562622, 'mse': 11.317381858825684}\n",
      "End epoch 125 of training; log content: {'loss': 11.317381858825684, 'mae': 2.570704698562622, 'mse': 11.317381858825684, 'val_loss': 10.095096588134766, 'val_mae': 2.425884485244751, 'val_mse': 10.095096588134766}\n",
      "...Training: end of batch 0; log content: {'loss': 10.166297912597656, 'mae': 2.576083183288574, 'mse': 10.166297912597656}\n",
      "...Training: end of batch 1; log content: {'loss': 8.007081031799316, 'mae': 2.2914342880249023, 'mse': 8.007081031799316}\n",
      "...Training: end of batch 2; log content: {'loss': 11.211566925048828, 'mae': 2.5399482250213623, 'mse': 11.211566925048828}\n",
      "...Training: end of batch 3; log content: {'loss': 10.856282234191895, 'mae': 2.5201008319854736, 'mse': 10.856282234191895}\n",
      "...Training: end of batch 4; log content: {'loss': 11.806396484375, 'mae': 2.613118886947632, 'mse': 11.806396484375}\n",
      "...Training: end of batch 5; log content: {'loss': 11.203177452087402, 'mae': 2.5566418170928955, 'mse': 11.203177452087402}\n",
      "...Training: end of batch 6; log content: {'loss': 11.995657920837402, 'mae': 2.6513869762420654, 'mse': 11.995657920837402}\n",
      "...Training: end of batch 7; log content: {'loss': 11.314939498901367, 'mae': 2.5711588859558105, 'mse': 11.314939498901367}\n",
      "End epoch 126 of training; log content: {'loss': 11.314939498901367, 'mae': 2.5711588859558105, 'mse': 11.314939498901367, 'val_loss': 10.03083324432373, 'val_mae': 2.4355623722076416, 'val_mse': 10.03083324432373}\n",
      "...Training: end of batch 0; log content: {'loss': 9.255128860473633, 'mae': 2.6523919105529785, 'mse': 9.255128860473633}\n",
      "...Training: end of batch 1; log content: {'loss': 11.764608383178711, 'mae': 2.7855942249298096, 'mse': 11.764608383178711}\n",
      "...Training: end of batch 2; log content: {'loss': 11.252558708190918, 'mae': 2.691629409790039, 'mse': 11.252558708190918}\n",
      "...Training: end of batch 3; log content: {'loss': 10.717369079589844, 'mae': 2.5769505500793457, 'mse': 10.717369079589844}\n",
      "...Training: end of batch 4; log content: {'loss': 9.97424030303955, 'mae': 2.439901828765869, 'mse': 9.97424030303955}\n",
      "...Training: end of batch 5; log content: {'loss': 9.989121437072754, 'mae': 2.443185567855835, 'mse': 9.989121437072754}\n",
      "...Training: end of batch 6; log content: {'loss': 11.268178939819336, 'mae': 2.5279324054718018, 'mse': 11.268178939819336}\n",
      "...Training: end of batch 7; log content: {'loss': 11.339376449584961, 'mae': 2.562016725540161, 'mse': 11.339376449584961}\n",
      "End epoch 127 of training; log content: {'loss': 11.339376449584961, 'mae': 2.562016725540161, 'mse': 11.339376449584961, 'val_loss': 10.075409889221191, 'val_mae': 2.449744939804077, 'val_mse': 10.075409889221191}\n",
      "...Training: end of batch 0; log content: {'loss': 14.743083953857422, 'mae': 2.9550163745880127, 'mse': 14.743083953857422}\n",
      "...Training: end of batch 1; log content: {'loss': 12.992798805236816, 'mae': 2.8441824913024902, 'mse': 12.992798805236816}\n",
      "...Training: end of batch 2; log content: {'loss': 12.369415283203125, 'mae': 2.7226078510284424, 'mse': 12.369415283203125}\n",
      "...Training: end of batch 3; log content: {'loss': 12.940592765808105, 'mae': 2.726569175720215, 'mse': 12.940592765808105}\n",
      "...Training: end of batch 4; log content: {'loss': 12.171107292175293, 'mae': 2.628331422805786, 'mse': 12.171107292175293}\n",
      "...Training: end of batch 5; log content: {'loss': 11.528423309326172, 'mae': 2.579058885574341, 'mse': 11.528423309326172}\n",
      "...Training: end of batch 6; log content: {'loss': 11.476903915405273, 'mae': 2.560020685195923, 'mse': 11.476903915405273}\n",
      "...Training: end of batch 7; log content: {'loss': 11.396946907043457, 'mae': 2.590672254562378, 'mse': 11.396946907043457}\n",
      "End epoch 128 of training; log content: {'loss': 11.396946907043457, 'mae': 2.590672254562378, 'mse': 11.396946907043457, 'val_loss': 9.984906196594238, 'val_mae': 2.4197680950164795, 'val_mse': 9.984906196594238}\n",
      "...Training: end of batch 0; log content: {'loss': 18.413009643554688, 'mae': 3.5475854873657227, 'mse': 18.413009643554688}\n",
      "...Training: end of batch 1; log content: {'loss': 14.990166664123535, 'mae': 2.9849486351013184, 'mse': 14.990166664123535}\n",
      "...Training: end of batch 2; log content: {'loss': 14.17444133758545, 'mae': 2.8926990032196045, 'mse': 14.17444133758545}\n",
      "...Training: end of batch 3; log content: {'loss': 14.020191192626953, 'mae': 2.8468923568725586, 'mse': 14.020191192626953}\n",
      "...Training: end of batch 4; log content: {'loss': 13.517854690551758, 'mae': 2.8147494792938232, 'mse': 13.517854690551758}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Training: end of batch 5; log content: {'loss': 12.460037231445312, 'mae': 2.699021100997925, 'mse': 12.460037231445312}\n",
      "...Training: end of batch 6; log content: {'loss': 11.960405349731445, 'mae': 2.651123285293579, 'mse': 11.960405349731445}\n",
      "...Training: end of batch 7; log content: {'loss': 11.31297779083252, 'mae': 2.579160451889038, 'mse': 11.31297779083252}\n",
      "End epoch 129 of training; log content: {'loss': 11.31297779083252, 'mae': 2.579160451889038, 'mse': 11.31297779083252, 'val_loss': 10.041238784790039, 'val_mae': 2.4488422870635986, 'val_mse': 10.041238784790039}\n",
      "...Training: end of batch 0; log content: {'loss': 13.854005813598633, 'mae': 2.9107556343078613, 'mse': 13.854005813598633}\n",
      "...Training: end of batch 1; log content: {'loss': 11.085378646850586, 'mae': 2.673971652984619, 'mse': 11.085378646850586}\n",
      "...Training: end of batch 2; log content: {'loss': 12.655142784118652, 'mae': 2.838080644607544, 'mse': 12.655142784118652}\n",
      "...Training: end of batch 3; log content: {'loss': 13.960592269897461, 'mae': 2.8835489749908447, 'mse': 13.960592269897461}\n",
      "...Training: end of batch 4; log content: {'loss': 12.367049217224121, 'mae': 2.698909044265747, 'mse': 12.367049217224121}\n",
      "...Training: end of batch 5; log content: {'loss': 11.803009986877441, 'mae': 2.6288578510284424, 'mse': 11.803009986877441}\n",
      "...Training: end of batch 6; log content: {'loss': 11.202423095703125, 'mae': 2.54461932182312, 'mse': 11.202423095703125}\n",
      "...Training: end of batch 7; log content: {'loss': 11.338205337524414, 'mae': 2.5641844272613525, 'mse': 11.338205337524414}\n",
      "End epoch 130 of training; log content: {'loss': 11.338205337524414, 'mae': 2.5641844272613525, 'mse': 11.338205337524414, 'val_loss': 10.098133087158203, 'val_mae': 2.456716775894165, 'val_mse': 10.098133087158203}\n",
      "...Training: end of batch 0; log content: {'loss': 10.959314346313477, 'mae': 2.800294876098633, 'mse': 10.959314346313477}\n",
      "...Training: end of batch 1; log content: {'loss': 14.18673324584961, 'mae': 2.8952884674072266, 'mse': 14.18673324584961}\n",
      "...Training: end of batch 2; log content: {'loss': 12.68896484375, 'mae': 2.73119854927063, 'mse': 12.68896484375}\n",
      "...Training: end of batch 3; log content: {'loss': 12.357911109924316, 'mae': 2.7002739906311035, 'mse': 12.357911109924316}\n",
      "...Training: end of batch 4; log content: {'loss': 12.637866020202637, 'mae': 2.651082992553711, 'mse': 12.637866020202637}\n",
      "...Training: end of batch 5; log content: {'loss': 11.810161590576172, 'mae': 2.552060842514038, 'mse': 11.810161590576172}\n",
      "...Training: end of batch 6; log content: {'loss': 11.447179794311523, 'mae': 2.560401201248169, 'mse': 11.447179794311523}\n",
      "...Training: end of batch 7; log content: {'loss': 11.332335472106934, 'mae': 2.5764808654785156, 'mse': 11.332335472106934}\n",
      "End epoch 131 of training; log content: {'loss': 11.332335472106934, 'mae': 2.5764808654785156, 'mse': 11.332335472106934, 'val_loss': 10.006218910217285, 'val_mae': 2.4256625175476074, 'val_mse': 10.006218910217285}\n",
      "...Training: end of batch 0; log content: {'loss': 16.397281646728516, 'mae': 3.1462836265563965, 'mse': 16.397281646728516}\n",
      "...Training: end of batch 1; log content: {'loss': 14.45147705078125, 'mae': 2.998556137084961, 'mse': 14.45147705078125}\n",
      "...Training: end of batch 2; log content: {'loss': 15.498181343078613, 'mae': 3.0215256214141846, 'mse': 15.498181343078613}\n",
      "...Training: end of batch 3; log content: {'loss': 14.815793991088867, 'mae': 2.92724347114563, 'mse': 14.815793991088867}\n",
      "...Training: end of batch 4; log content: {'loss': 13.719799995422363, 'mae': 2.794861316680908, 'mse': 13.719799995422363}\n",
      "...Training: end of batch 5; log content: {'loss': 12.63700008392334, 'mae': 2.7066192626953125, 'mse': 12.63700008392334}\n",
      "...Training: end of batch 6; log content: {'loss': 11.692341804504395, 'mae': 2.6018357276916504, 'mse': 11.692341804504395}\n",
      "...Training: end of batch 7; log content: {'loss': 11.35203742980957, 'mae': 2.5752434730529785, 'mse': 11.35203742980957}\n",
      "End epoch 132 of training; log content: {'loss': 11.35203742980957, 'mae': 2.5752434730529785, 'mse': 11.35203742980957, 'val_loss': 10.062973022460938, 'val_mae': 2.445542097091675, 'val_mse': 10.062973022460938}\n",
      "...Training: end of batch 0; log content: {'loss': 9.055063247680664, 'mae': 2.2849645614624023, 'mse': 9.055063247680664}\n",
      "...Training: end of batch 1; log content: {'loss': 8.132802963256836, 'mae': 2.2035984992980957, 'mse': 8.132802963256836}\n",
      "...Training: end of batch 2; log content: {'loss': 8.430747032165527, 'mae': 2.2743895053863525, 'mse': 8.430747032165527}\n",
      "...Training: end of batch 3; log content: {'loss': 9.82043743133545, 'mae': 2.31447696685791, 'mse': 9.82043743133545}\n",
      "...Training: end of batch 4; log content: {'loss': 10.300688743591309, 'mae': 2.4044241905212402, 'mse': 10.300688743591309}\n",
      "...Training: end of batch 5; log content: {'loss': 10.898615837097168, 'mae': 2.4927406311035156, 'mse': 10.898615837097168}\n",
      "...Training: end of batch 6; log content: {'loss': 11.127340316772461, 'mae': 2.517444133758545, 'mse': 11.127340316772461}\n",
      "...Training: end of batch 7; log content: {'loss': 11.397522926330566, 'mae': 2.576524257659912, 'mse': 11.397522926330566}\n",
      "End epoch 133 of training; log content: {'loss': 11.397522926330566, 'mae': 2.576524257659912, 'mse': 11.397522926330566, 'val_loss': 10.048365592956543, 'val_mae': 2.4214162826538086, 'val_mse': 10.048365592956543}\n",
      "...Training: end of batch 0; log content: {'loss': 7.006298065185547, 'mae': 2.0326528549194336, 'mse': 7.006298065185547}\n",
      "...Training: end of batch 1; log content: {'loss': 7.278067111968994, 'mae': 2.1004085540771484, 'mse': 7.278067111968994}\n",
      "...Training: end of batch 2; log content: {'loss': 13.138781547546387, 'mae': 2.6116085052490234, 'mse': 13.138781547546387}\n",
      "...Training: end of batch 3; log content: {'loss': 12.57575798034668, 'mae': 2.60648250579834, 'mse': 12.57575798034668}\n",
      "...Training: end of batch 4; log content: {'loss': 11.879295349121094, 'mae': 2.5499026775360107, 'mse': 11.879295349121094}\n",
      "...Training: end of batch 5; log content: {'loss': 11.364060401916504, 'mae': 2.56174635887146, 'mse': 11.364060401916504}\n",
      "...Training: end of batch 6; log content: {'loss': 11.295110702514648, 'mae': 2.5763134956359863, 'mse': 11.295110702514648}\n",
      "...Training: end of batch 7; log content: {'loss': 11.41105842590332, 'mae': 2.5854990482330322, 'mse': 11.41105842590332}\n",
      "End epoch 134 of training; log content: {'loss': 11.41105842590332, 'mae': 2.5854990482330322, 'mse': 11.41105842590332, 'val_loss': 10.100725173950195, 'val_mae': 2.432328462600708, 'val_mse': 10.100725173950195}\n",
      "...Training: end of batch 0; log content: {'loss': 12.473505020141602, 'mae': 2.5195865631103516, 'mse': 12.473505020141602}\n",
      "...Training: end of batch 1; log content: {'loss': 12.931558609008789, 'mae': 2.7375736236572266, 'mse': 12.931558609008789}\n",
      "...Training: end of batch 2; log content: {'loss': 10.766669273376465, 'mae': 2.5340795516967773, 'mse': 10.766669273376465}\n",
      "...Training: end of batch 3; log content: {'loss': 11.366561889648438, 'mae': 2.570063591003418, 'mse': 11.366561889648438}\n",
      "...Training: end of batch 4; log content: {'loss': 11.278947830200195, 'mae': 2.583606243133545, 'mse': 11.278947830200195}\n",
      "...Training: end of batch 5; log content: {'loss': 11.50413990020752, 'mae': 2.593055009841919, 'mse': 11.50413990020752}\n",
      "...Training: end of batch 6; log content: {'loss': 11.340120315551758, 'mae': 2.589810848236084, 'mse': 11.340120315551758}\n",
      "...Training: end of batch 7; log content: {'loss': 11.319729804992676, 'mae': 2.572178840637207, 'mse': 11.319729804992676}\n",
      "End epoch 135 of training; log content: {'loss': 11.319729804992676, 'mae': 2.572178840637207, 'mse': 11.319729804992676, 'val_loss': 10.128433227539062, 'val_mae': 2.465327739715576, 'val_mse': 10.128433227539062}\n",
      "...Training: end of batch 0; log content: {'loss': 12.359587669372559, 'mae': 2.8822431564331055, 'mse': 12.359587669372559}\n",
      "...Training: end of batch 1; log content: {'loss': 12.347036361694336, 'mae': 2.813900947570801, 'mse': 12.347036361694336}\n",
      "...Training: end of batch 2; log content: {'loss': 11.43860912322998, 'mae': 2.5979788303375244, 'mse': 11.43860912322998}\n",
      "...Training: end of batch 3; log content: {'loss': 10.614338874816895, 'mae': 2.5246994495391846, 'mse': 10.614338874816895}\n",
      "...Training: end of batch 4; log content: {'loss': 11.265592575073242, 'mae': 2.6153318881988525, 'mse': 11.265592575073242}\n",
      "...Training: end of batch 5; log content: {'loss': 11.958924293518066, 'mae': 2.6343705654144287, 'mse': 11.958924293518066}\n",
      "...Training: end of batch 6; log content: {'loss': 11.768495559692383, 'mae': 2.617605447769165, 'mse': 11.768495559692383}\n",
      "...Training: end of batch 7; log content: {'loss': 11.337757110595703, 'mae': 2.5582211017608643, 'mse': 11.337757110595703}\n",
      "End epoch 136 of training; log content: {'loss': 11.337757110595703, 'mae': 2.5582211017608643, 'mse': 11.337757110595703, 'val_loss': 10.030325889587402, 'val_mae': 2.454965353012085, 'val_mse': 10.030325889587402}\n",
      "...Training: end of batch 0; log content: {'loss': 12.226434707641602, 'mae': 2.7038910388946533, 'mse': 12.226434707641602}\n",
      "...Training: end of batch 1; log content: {'loss': 9.417342185974121, 'mae': 2.3976993560791016, 'mse': 9.417342185974121}\n",
      "...Training: end of batch 2; log content: {'loss': 10.212220191955566, 'mae': 2.4806745052337646, 'mse': 10.212220191955566}\n",
      "...Training: end of batch 3; log content: {'loss': 10.309992790222168, 'mae': 2.522482395172119, 'mse': 10.309992790222168}\n",
      "...Training: end of batch 4; log content: {'loss': 10.046112060546875, 'mae': 2.4851021766662598, 'mse': 10.046112060546875}\n",
      "...Training: end of batch 5; log content: {'loss': 11.157218933105469, 'mae': 2.5470333099365234, 'mse': 11.157218933105469}\n",
      "...Training: end of batch 6; log content: {'loss': 11.839739799499512, 'mae': 2.61706280708313, 'mse': 11.839739799499512}\n",
      "...Training: end of batch 7; log content: {'loss': 11.297266960144043, 'mae': 2.5643064975738525, 'mse': 11.297266960144043}\n",
      "End epoch 137 of training; log content: {'loss': 11.297266960144043, 'mae': 2.5643064975738525, 'mse': 11.297266960144043, 'val_loss': 9.994684219360352, 'val_mae': 2.438999652862549, 'val_mse': 9.994684219360352}\n",
      "...Training: end of batch 0; log content: {'loss': 8.62092399597168, 'mae': 2.3575072288513184, 'mse': 8.62092399597168}\n",
      "...Training: end of batch 1; log content: {'loss': 10.327829360961914, 'mae': 2.413473129272461, 'mse': 10.327829360961914}\n",
      "...Training: end of batch 2; log content: {'loss': 13.274517059326172, 'mae': 2.6802146434783936, 'mse': 13.274517059326172}\n",
      "...Training: end of batch 3; log content: {'loss': 12.110854148864746, 'mae': 2.6207034587860107, 'mse': 12.110854148864746}\n",
      "...Training: end of batch 4; log content: {'loss': 12.367314338684082, 'mae': 2.6843650341033936, 'mse': 12.367314338684082}\n",
      "...Training: end of batch 5; log content: {'loss': 11.505938529968262, 'mae': 2.601304054260254, 'mse': 11.505938529968262}\n",
      "...Training: end of batch 6; log content: {'loss': 11.766478538513184, 'mae': 2.6284213066101074, 'mse': 11.766478538513184}\n",
      "...Training: end of batch 7; log content: {'loss': 11.33808422088623, 'mae': 2.580759286880493, 'mse': 11.33808422088623}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End epoch 138 of training; log content: {'loss': 11.33808422088623, 'mae': 2.580759286880493, 'mse': 11.33808422088623, 'val_loss': 10.014967918395996, 'val_mae': 2.423929452896118, 'val_mse': 10.014967918395996}\n",
      "...Training: end of batch 0; log content: {'loss': 10.517919540405273, 'mae': 2.504194736480713, 'mse': 10.517919540405273}\n",
      "...Training: end of batch 1; log content: {'loss': 11.551580429077148, 'mae': 2.5665111541748047, 'mse': 11.551580429077148}\n",
      "...Training: end of batch 2; log content: {'loss': 9.770895957946777, 'mae': 2.37129282951355, 'mse': 9.770895957946777}\n",
      "...Training: end of batch 3; log content: {'loss': 9.949148178100586, 'mae': 2.3982324600219727, 'mse': 9.949148178100586}\n",
      "...Training: end of batch 4; log content: {'loss': 10.388580322265625, 'mae': 2.501370906829834, 'mse': 10.388580322265625}\n",
      "...Training: end of batch 5; log content: {'loss': 11.81733226776123, 'mae': 2.6298418045043945, 'mse': 11.81733226776123}\n",
      "...Training: end of batch 6; log content: {'loss': 11.223795890808105, 'mae': 2.586334228515625, 'mse': 11.223795890808105}\n",
      "...Training: end of batch 7; log content: {'loss': 11.314592361450195, 'mae': 2.583345890045166, 'mse': 11.314592361450195}\n",
      "End epoch 139 of training; log content: {'loss': 11.314592361450195, 'mae': 2.583345890045166, 'mse': 11.314592361450195, 'val_loss': 10.056204795837402, 'val_mae': 2.435096263885498, 'val_mse': 10.056204795837402}\n",
      "...Training: end of batch 0; log content: {'loss': 14.314774513244629, 'mae': 2.9769461154937744, 'mse': 14.314774513244629}\n",
      "...Training: end of batch 1; log content: {'loss': 11.884321212768555, 'mae': 2.6898741722106934, 'mse': 11.884321212768555}\n",
      "...Training: end of batch 2; log content: {'loss': 13.595309257507324, 'mae': 2.779146909713745, 'mse': 13.595309257507324}\n",
      "...Training: end of batch 3; log content: {'loss': 12.1364164352417, 'mae': 2.6302621364593506, 'mse': 12.1364164352417}\n",
      "...Training: end of batch 4; log content: {'loss': 12.016148567199707, 'mae': 2.652942419052124, 'mse': 12.016148567199707}\n",
      "...Training: end of batch 5; log content: {'loss': 11.95775318145752, 'mae': 2.648683786392212, 'mse': 11.95775318145752}\n",
      "...Training: end of batch 6; log content: {'loss': 11.542147636413574, 'mae': 2.590831756591797, 'mse': 11.542147636413574}\n",
      "...Training: end of batch 7; log content: {'loss': 11.285126686096191, 'mae': 2.5681278705596924, 'mse': 11.285126686096191}\n",
      "End epoch 140 of training; log content: {'loss': 11.285126686096191, 'mae': 2.5681278705596924, 'mse': 11.285126686096191, 'val_loss': 10.233541488647461, 'val_mae': 2.462766170501709, 'val_mse': 10.233541488647461}\n",
      "...Training: end of batch 0; log content: {'loss': 15.23296070098877, 'mae': 2.7651150226593018, 'mse': 15.23296070098877}\n",
      "...Training: end of batch 1; log content: {'loss': 13.858793258666992, 'mae': 2.7422537803649902, 'mse': 13.858793258666992}\n",
      "...Training: end of batch 2; log content: {'loss': 12.171862602233887, 'mae': 2.619621753692627, 'mse': 12.171862602233887}\n",
      "...Training: end of batch 3; log content: {'loss': 13.321285247802734, 'mae': 2.7751545906066895, 'mse': 13.321285247802734}\n",
      "...Training: end of batch 4; log content: {'loss': 12.153999328613281, 'mae': 2.647104263305664, 'mse': 12.153999328613281}\n",
      "...Training: end of batch 5; log content: {'loss': 11.264790534973145, 'mae': 2.550128221511841, 'mse': 11.264790534973145}\n",
      "...Training: end of batch 6; log content: {'loss': 10.453874588012695, 'mae': 2.464120388031006, 'mse': 10.453874588012695}\n",
      "...Training: end of batch 7; log content: {'loss': 11.316195487976074, 'mae': 2.5565297603607178, 'mse': 11.316195487976074}\n",
      "End epoch 141 of training; log content: {'loss': 11.316195487976074, 'mae': 2.5565297603607178, 'mse': 11.316195487976074, 'val_loss': 10.127923965454102, 'val_mae': 2.456696033477783, 'val_mse': 10.127923965454102}\n",
      "...Training: end of batch 0; log content: {'loss': 15.689751625061035, 'mae': 3.0301339626312256, 'mse': 15.689751625061035}\n",
      "...Training: end of batch 1; log content: {'loss': 13.464090347290039, 'mae': 2.775221824645996, 'mse': 13.464090347290039}\n",
      "...Training: end of batch 2; log content: {'loss': 12.233165740966797, 'mae': 2.686227560043335, 'mse': 12.233165740966797}\n",
      "...Training: end of batch 3; log content: {'loss': 11.236379623413086, 'mae': 2.541461229324341, 'mse': 11.236379623413086}\n",
      "...Training: end of batch 4; log content: {'loss': 11.403546333312988, 'mae': 2.558572769165039, 'mse': 11.403546333312988}\n",
      "...Training: end of batch 5; log content: {'loss': 11.690373420715332, 'mae': 2.614314079284668, 'mse': 11.690373420715332}\n",
      "...Training: end of batch 6; log content: {'loss': 11.824342727661133, 'mae': 2.6383559703826904, 'mse': 11.824342727661133}\n",
      "...Training: end of batch 7; log content: {'loss': 11.299055099487305, 'mae': 2.569892406463623, 'mse': 11.299055099487305}\n",
      "End epoch 142 of training; log content: {'loss': 11.299055099487305, 'mae': 2.569892406463623, 'mse': 11.299055099487305, 'val_loss': 9.948821067810059, 'val_mae': 2.4234185218811035, 'val_mse': 9.948821067810059}\n",
      "...Training: end of batch 0; log content: {'loss': 14.296552658081055, 'mae': 2.9635190963745117, 'mse': 14.296552658081055}\n",
      "...Training: end of batch 1; log content: {'loss': 12.77995491027832, 'mae': 2.8215746879577637, 'mse': 12.77995491027832}\n",
      "...Training: end of batch 2; log content: {'loss': 12.944897651672363, 'mae': 2.7885334491729736, 'mse': 12.944897651672363}\n",
      "...Training: end of batch 3; log content: {'loss': 11.749027252197266, 'mae': 2.66801118850708, 'mse': 11.749027252197266}\n",
      "...Training: end of batch 4; log content: {'loss': 11.517797470092773, 'mae': 2.570143699645996, 'mse': 11.517797470092773}\n",
      "...Training: end of batch 5; log content: {'loss': 10.815186500549316, 'mae': 2.514136552810669, 'mse': 10.815186500549316}\n",
      "...Training: end of batch 6; log content: {'loss': 11.269171714782715, 'mae': 2.5727698802948, 'mse': 11.269171714782715}\n",
      "...Training: end of batch 7; log content: {'loss': 11.325489044189453, 'mae': 2.5883195400238037, 'mse': 11.325489044189453}\n",
      "End epoch 143 of training; log content: {'loss': 11.325489044189453, 'mae': 2.5883195400238037, 'mse': 11.325489044189453, 'val_loss': 10.027267456054688, 'val_mae': 2.432694673538208, 'val_mse': 10.027267456054688}\n",
      "...Training: end of batch 0; log content: {'loss': 16.555686950683594, 'mae': 3.147675037384033, 'mse': 16.555686950683594}\n",
      "...Training: end of batch 1; log content: {'loss': 13.179853439331055, 'mae': 2.8238210678100586, 'mse': 13.179853439331055}\n",
      "...Training: end of batch 2; log content: {'loss': 14.559021949768066, 'mae': 2.9073636531829834, 'mse': 14.559021949768066}\n",
      "...Training: end of batch 3; log content: {'loss': 12.242685317993164, 'mae': 2.649869918823242, 'mse': 12.242685317993164}\n",
      "...Training: end of batch 4; log content: {'loss': 11.929533958435059, 'mae': 2.629896879196167, 'mse': 11.929533958435059}\n",
      "...Training: end of batch 5; log content: {'loss': 11.192639350891113, 'mae': 2.5382421016693115, 'mse': 11.192639350891113}\n",
      "...Training: end of batch 6; log content: {'loss': 11.427699089050293, 'mae': 2.58087420463562, 'mse': 11.427699089050293}\n",
      "...Training: end of batch 7; log content: {'loss': 11.294502258300781, 'mae': 2.570662260055542, 'mse': 11.294502258300781}\n",
      "End epoch 144 of training; log content: {'loss': 11.294502258300781, 'mae': 2.570662260055542, 'mse': 11.294502258300781, 'val_loss': 10.111185073852539, 'val_mae': 2.454164505004883, 'val_mse': 10.111185073852539}\n",
      "...Training: end of batch 0; log content: {'loss': 11.039607048034668, 'mae': 2.506678342819214, 'mse': 11.039607048034668}\n",
      "...Training: end of batch 1; log content: {'loss': 10.005794525146484, 'mae': 2.356205940246582, 'mse': 10.005794525146484}\n",
      "...Training: end of batch 2; log content: {'loss': 12.248298645019531, 'mae': 2.683185577392578, 'mse': 12.248298645019531}\n",
      "...Training: end of batch 3; log content: {'loss': 12.877154350280762, 'mae': 2.652801990509033, 'mse': 12.877154350280762}\n",
      "...Training: end of batch 4; log content: {'loss': 12.053239822387695, 'mae': 2.6186859607696533, 'mse': 12.053239822387695}\n",
      "...Training: end of batch 5; log content: {'loss': 11.910420417785645, 'mae': 2.631249189376831, 'mse': 11.910420417785645}\n",
      "...Training: end of batch 6; log content: {'loss': 11.205574035644531, 'mae': 2.5484745502471924, 'mse': 11.205574035644531}\n",
      "...Training: end of batch 7; log content: {'loss': 11.320920944213867, 'mae': 2.570394515991211, 'mse': 11.320920944213867}\n",
      "End epoch 145 of training; log content: {'loss': 11.320920944213867, 'mae': 2.570394515991211, 'mse': 11.320920944213867, 'val_loss': 10.057404518127441, 'val_mae': 2.4382126331329346, 'val_mse': 10.057404518127441}\n",
      "...Training: end of batch 0; log content: {'loss': 10.311616897583008, 'mae': 2.4991683959960938, 'mse': 10.311616897583008}\n",
      "...Training: end of batch 1; log content: {'loss': 10.830859184265137, 'mae': 2.52471661567688, 'mse': 10.830859184265137}\n",
      "...Training: end of batch 2; log content: {'loss': 11.22329330444336, 'mae': 2.5658023357391357, 'mse': 11.22329330444336}\n",
      "...Training: end of batch 3; log content: {'loss': 10.281133651733398, 'mae': 2.4913277626037598, 'mse': 10.281133651733398}\n",
      "...Training: end of batch 4; log content: {'loss': 11.85942554473877, 'mae': 2.64080548286438, 'mse': 11.85942554473877}\n",
      "...Training: end of batch 5; log content: {'loss': 12.247757911682129, 'mae': 2.663764715194702, 'mse': 12.247757911682129}\n",
      "...Training: end of batch 6; log content: {'loss': 11.612322807312012, 'mae': 2.628654956817627, 'mse': 11.612322807312012}\n",
      "...Training: end of batch 7; log content: {'loss': 11.330886840820312, 'mae': 2.5822272300720215, 'mse': 11.330886840820312}\n",
      "End epoch 146 of training; log content: {'loss': 11.330886840820312, 'mae': 2.5822272300720215, 'mse': 11.330886840820312, 'val_loss': 9.962879180908203, 'val_mae': 2.4214835166931152, 'val_mse': 9.962879180908203}\n",
      "...Training: end of batch 0; log content: {'loss': 5.980196475982666, 'mae': 2.04960298538208, 'mse': 5.980196475982666}\n",
      "...Training: end of batch 1; log content: {'loss': 10.991127967834473, 'mae': 2.573364496231079, 'mse': 10.991127967834473}\n",
      "...Training: end of batch 2; log content: {'loss': 13.440559387207031, 'mae': 2.7238121032714844, 'mse': 13.440559387207031}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Training: end of batch 3; log content: {'loss': 12.91079044342041, 'mae': 2.7449913024902344, 'mse': 12.91079044342041}\n",
      "...Training: end of batch 4; log content: {'loss': 11.446213722229004, 'mae': 2.5737760066986084, 'mse': 11.446213722229004}\n",
      "...Training: end of batch 5; log content: {'loss': 11.35612964630127, 'mae': 2.541987180709839, 'mse': 11.35612964630127}\n",
      "...Training: end of batch 6; log content: {'loss': 11.355813026428223, 'mae': 2.5619640350341797, 'mse': 11.355813026428223}\n",
      "...Training: end of batch 7; log content: {'loss': 11.338687896728516, 'mae': 2.5768258571624756, 'mse': 11.338687896728516}\n",
      "End epoch 147 of training; log content: {'loss': 11.338687896728516, 'mae': 2.5768258571624756, 'mse': 11.338687896728516, 'val_loss': 9.946730613708496, 'val_mae': 2.442250967025757, 'val_mse': 9.946730613708496}\n",
      "...Training: end of batch 0; log content: {'loss': 9.944671630859375, 'mae': 2.5002951622009277, 'mse': 9.944671630859375}\n",
      "...Training: end of batch 1; log content: {'loss': 9.040642738342285, 'mae': 2.409782886505127, 'mse': 9.040642738342285}\n",
      "...Training: end of batch 2; log content: {'loss': 11.879837989807129, 'mae': 2.656076192855835, 'mse': 11.879837989807129}\n",
      "...Training: end of batch 3; log content: {'loss': 10.753593444824219, 'mae': 2.5150301456451416, 'mse': 10.753593444824219}\n",
      "...Training: end of batch 4; log content: {'loss': 12.161256790161133, 'mae': 2.6298904418945312, 'mse': 12.161256790161133}\n",
      "...Training: end of batch 5; log content: {'loss': 11.790393829345703, 'mae': 2.57918643951416, 'mse': 11.790393829345703}\n",
      "...Training: end of batch 6; log content: {'loss': 11.785468101501465, 'mae': 2.602267026901245, 'mse': 11.785468101501465}\n",
      "...Training: end of batch 7; log content: {'loss': 11.421168327331543, 'mae': 2.5649118423461914, 'mse': 11.421168327331543}\n",
      "End epoch 148 of training; log content: {'loss': 11.421168327331543, 'mae': 2.5649118423461914, 'mse': 11.421168327331543, 'val_loss': 10.112870216369629, 'val_mae': 2.449082851409912, 'val_mse': 10.112870216369629}\n",
      "...Training: end of batch 0; log content: {'loss': 8.489937782287598, 'mae': 2.2091774940490723, 'mse': 8.489937782287598}\n",
      "...Training: end of batch 1; log content: {'loss': 12.377666473388672, 'mae': 2.584279775619507, 'mse': 12.377666473388672}\n",
      "...Training: end of batch 2; log content: {'loss': 13.084063529968262, 'mae': 2.704746961593628, 'mse': 13.084063529968262}\n",
      "...Training: end of batch 3; log content: {'loss': 11.435637474060059, 'mae': 2.5393178462982178, 'mse': 11.435637474060059}\n",
      "...Training: end of batch 4; log content: {'loss': 11.269513130187988, 'mae': 2.546182155609131, 'mse': 11.269513130187988}\n",
      "...Training: end of batch 5; log content: {'loss': 11.061078071594238, 'mae': 2.5510642528533936, 'mse': 11.061078071594238}\n",
      "...Training: end of batch 6; log content: {'loss': 10.967734336853027, 'mae': 2.5574190616607666, 'mse': 10.967734336853027}\n",
      "...Training: end of batch 7; log content: {'loss': 11.368592262268066, 'mae': 2.5874381065368652, 'mse': 11.368592262268066}\n",
      "End epoch 149 of training; log content: {'loss': 11.368592262268066, 'mae': 2.5874381065368652, 'mse': 11.368592262268066, 'val_loss': 9.995194435119629, 'val_mae': 2.4217257499694824, 'val_mse': 9.995194435119629}\n",
      "...Training: end of batch 0; log content: {'loss': 9.041451454162598, 'mae': 2.33445405960083, 'mse': 9.041451454162598}\n",
      "...Training: end of batch 1; log content: {'loss': 8.765830993652344, 'mae': 2.3945395946502686, 'mse': 8.765830993652344}\n",
      "...Training: end of batch 2; log content: {'loss': 8.636017799377441, 'mae': 2.329667806625366, 'mse': 8.636017799377441}\n",
      "...Training: end of batch 3; log content: {'loss': 9.185562133789062, 'mae': 2.3995025157928467, 'mse': 9.185562133789062}\n",
      "...Training: end of batch 4; log content: {'loss': 10.39268970489502, 'mae': 2.5246098041534424, 'mse': 10.39268970489502}\n",
      "...Training: end of batch 5; log content: {'loss': 10.066895484924316, 'mae': 2.487853527069092, 'mse': 10.066895484924316}\n",
      "...Training: end of batch 6; log content: {'loss': 11.8292236328125, 'mae': 2.6154396533966064, 'mse': 11.8292236328125}\n",
      "...Training: end of batch 7; log content: {'loss': 11.362323760986328, 'mae': 2.5678439140319824, 'mse': 11.362323760986328}\n",
      "End epoch 150 of training; log content: {'loss': 11.362323760986328, 'mae': 2.5678439140319824, 'mse': 11.362323760986328, 'val_loss': 10.066571235656738, 'val_mae': 2.4510746002197266, 'val_mse': 10.066571235656738}\n",
      "...Training: end of batch 0; log content: {'loss': 6.075647354125977, 'mae': 2.0037310123443604, 'mse': 6.075647354125977}\n",
      "...Training: end of batch 1; log content: {'loss': 9.100708961486816, 'mae': 2.2811379432678223, 'mse': 9.100708961486816}\n",
      "...Training: end of batch 2; log content: {'loss': 11.048548698425293, 'mae': 2.515239953994751, 'mse': 11.048548698425293}\n",
      "...Training: end of batch 3; log content: {'loss': 11.40188980102539, 'mae': 2.617086410522461, 'mse': 11.40188980102539}\n",
      "...Training: end of batch 4; log content: {'loss': 12.330220222473145, 'mae': 2.666980743408203, 'mse': 12.330220222473145}\n",
      "...Training: end of batch 5; log content: {'loss': 11.901406288146973, 'mae': 2.6278207302093506, 'mse': 11.901406288146973}\n",
      "...Training: end of batch 6; log content: {'loss': 11.805283546447754, 'mae': 2.6249725818634033, 'mse': 11.805283546447754}\n",
      "...Training: end of batch 7; log content: {'loss': 11.357744216918945, 'mae': 2.5627524852752686, 'mse': 11.357744216918945}\n",
      "End epoch 151 of training; log content: {'loss': 11.357744216918945, 'mae': 2.5627524852752686, 'mse': 11.357744216918945, 'val_loss': 10.177565574645996, 'val_mae': 2.456939458847046, 'val_mse': 10.177565574645996}\n",
      "...Training: end of batch 0; log content: {'loss': 11.254157066345215, 'mae': 2.515781879425049, 'mse': 11.254157066345215}\n",
      "...Training: end of batch 1; log content: {'loss': 10.011669158935547, 'mae': 2.4191577434539795, 'mse': 10.011669158935547}\n",
      "...Training: end of batch 2; log content: {'loss': 10.533045768737793, 'mae': 2.485915184020996, 'mse': 10.533045768737793}\n",
      "...Training: end of batch 3; log content: {'loss': 11.94880485534668, 'mae': 2.530977964401245, 'mse': 11.94880485534668}\n",
      "...Training: end of batch 4; log content: {'loss': 11.529598236083984, 'mae': 2.4852044582366943, 'mse': 11.529598236083984}\n",
      "...Training: end of batch 5; log content: {'loss': 11.126238822937012, 'mae': 2.513305425643921, 'mse': 11.126238822937012}\n",
      "...Training: end of batch 6; log content: {'loss': 11.326233863830566, 'mae': 2.5799319744110107, 'mse': 11.326233863830566}\n",
      "...Training: end of batch 7; log content: {'loss': 11.513248443603516, 'mae': 2.618424654006958, 'mse': 11.513248443603516}\n",
      "End epoch 152 of training; log content: {'loss': 11.513248443603516, 'mae': 2.618424654006958, 'mse': 11.513248443603516, 'val_loss': 10.020719528198242, 'val_mae': 2.404996633529663, 'val_mse': 10.020719528198242}\n",
      "...Training: end of batch 0; log content: {'loss': 13.160871505737305, 'mae': 2.9238810539245605, 'mse': 13.160871505737305}\n",
      "...Training: end of batch 1; log content: {'loss': 11.302276611328125, 'mae': 2.635256767272949, 'mse': 11.302276611328125}\n",
      "...Training: end of batch 2; log content: {'loss': 11.282646179199219, 'mae': 2.6549360752105713, 'mse': 11.282646179199219}\n",
      "...Training: end of batch 3; log content: {'loss': 10.116579055786133, 'mae': 2.520019769668579, 'mse': 10.116579055786133}\n",
      "...Training: end of batch 4; log content: {'loss': 9.17228889465332, 'mae': 2.386776924133301, 'mse': 9.17228889465332}\n",
      "...Training: end of batch 5; log content: {'loss': 10.848670959472656, 'mae': 2.5762851238250732, 'mse': 10.848670959472656}\n",
      "...Training: end of batch 6; log content: {'loss': 10.862136840820312, 'mae': 2.571279764175415, 'mse': 10.862136840820312}\n",
      "...Training: end of batch 7; log content: {'loss': 11.299701690673828, 'mae': 2.5854079723358154, 'mse': 11.299701690673828}\n",
      "End epoch 153 of training; log content: {'loss': 11.299701690673828, 'mae': 2.5854079723358154, 'mse': 11.299701690673828, 'val_loss': 10.110610008239746, 'val_mae': 2.456352472305298, 'val_mse': 10.110610008239746}\n",
      "...Training: end of batch 0; log content: {'loss': 10.05284595489502, 'mae': 2.2560322284698486, 'mse': 10.05284595489502}\n",
      "...Training: end of batch 1; log content: {'loss': 9.199094772338867, 'mae': 2.310886859893799, 'mse': 9.199094772338867}\n",
      "...Training: end of batch 2; log content: {'loss': 11.687255859375, 'mae': 2.622265577316284, 'mse': 11.687255859375}\n",
      "...Training: end of batch 3; log content: {'loss': 11.245462417602539, 'mae': 2.604567527770996, 'mse': 11.245462417602539}\n",
      "...Training: end of batch 4; log content: {'loss': 11.421521186828613, 'mae': 2.6333978176116943, 'mse': 11.421521186828613}\n",
      "...Training: end of batch 5; log content: {'loss': 10.87336254119873, 'mae': 2.5468523502349854, 'mse': 10.87336254119873}\n",
      "...Training: end of batch 6; log content: {'loss': 11.12722110748291, 'mae': 2.5489299297332764, 'mse': 11.12722110748291}\n",
      "...Training: end of batch 7; log content: {'loss': 11.343035697937012, 'mae': 2.5682573318481445, 'mse': 11.343035697937012}\n",
      "End epoch 154 of training; log content: {'loss': 11.343035697937012, 'mae': 2.5682573318481445, 'mse': 11.343035697937012, 'val_loss': 10.188089370727539, 'val_mae': 2.4637818336486816, 'val_mse': 10.188089370727539}\n",
      "...Training: end of batch 0; log content: {'loss': 20.692768096923828, 'mae': 3.3326644897460938, 'mse': 20.692768096923828}\n",
      "...Training: end of batch 1; log content: {'loss': 14.368131637573242, 'mae': 2.7862558364868164, 'mse': 14.368131637573242}\n",
      "...Training: end of batch 2; log content: {'loss': 12.810715675354004, 'mae': 2.66280198097229, 'mse': 12.810715675354004}\n",
      "...Training: end of batch 3; log content: {'loss': 13.266939163208008, 'mae': 2.7757232189178467, 'mse': 13.266939163208008}\n",
      "...Training: end of batch 4; log content: {'loss': 11.56137466430664, 'mae': 2.558321475982666, 'mse': 11.56137466430664}\n",
      "...Training: end of batch 5; log content: {'loss': 11.616020202636719, 'mae': 2.609236478805542, 'mse': 11.616020202636719}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Training: end of batch 6; log content: {'loss': 11.480706214904785, 'mae': 2.591892719268799, 'mse': 11.480706214904785}\n",
      "...Training: end of batch 7; log content: {'loss': 11.413748741149902, 'mae': 2.599073886871338, 'mse': 11.413748741149902}\n",
      "End epoch 155 of training; log content: {'loss': 11.413748741149902, 'mae': 2.599073886871338, 'mse': 11.413748741149902, 'val_loss': 10.048480033874512, 'val_mae': 2.421628713607788, 'val_mse': 10.048480033874512}\n",
      "...Training: end of batch 0; log content: {'loss': 8.000626564025879, 'mae': 2.3386712074279785, 'mse': 8.000626564025879}\n",
      "...Training: end of batch 1; log content: {'loss': 11.644508361816406, 'mae': 2.4196040630340576, 'mse': 11.644508361816406}\n",
      "...Training: end of batch 2; log content: {'loss': 11.344955444335938, 'mae': 2.4757702350616455, 'mse': 11.344955444335938}\n",
      "...Training: end of batch 3; log content: {'loss': 11.897747039794922, 'mae': 2.5746986865997314, 'mse': 11.897747039794922}\n",
      "...Training: end of batch 4; log content: {'loss': 10.944894790649414, 'mae': 2.5005369186401367, 'mse': 10.944894790649414}\n",
      "...Training: end of batch 5; log content: {'loss': 12.14840316772461, 'mae': 2.628582239151001, 'mse': 12.14840316772461}\n",
      "...Training: end of batch 6; log content: {'loss': 11.744662284851074, 'mae': 2.6299076080322266, 'mse': 11.744662284851074}\n",
      "...Training: end of batch 7; log content: {'loss': 11.338197708129883, 'mae': 2.587613105773926, 'mse': 11.338197708129883}\n",
      "End epoch 156 of training; log content: {'loss': 11.338197708129883, 'mae': 2.587613105773926, 'mse': 11.338197708129883, 'val_loss': 9.907877922058105, 'val_mae': 2.4295127391815186, 'val_mse': 9.907877922058105}\n",
      "...Training: end of batch 0; log content: {'loss': 8.929391860961914, 'mae': 2.392212390899658, 'mse': 8.929391860961914}\n",
      "...Training: end of batch 1; log content: {'loss': 7.666165351867676, 'mae': 2.0507140159606934, 'mse': 7.666165351867676}\n",
      "...Training: end of batch 2; log content: {'loss': 11.971199989318848, 'mae': 2.4769399166107178, 'mse': 11.971199989318848}\n",
      "...Training: end of batch 3; log content: {'loss': 11.974136352539062, 'mae': 2.5663886070251465, 'mse': 11.974136352539062}\n",
      "...Training: end of batch 4; log content: {'loss': 11.288278579711914, 'mae': 2.5189056396484375, 'mse': 11.288278579711914}\n",
      "...Training: end of batch 5; log content: {'loss': 10.972640037536621, 'mae': 2.495204210281372, 'mse': 10.972640037536621}\n",
      "...Training: end of batch 6; log content: {'loss': 11.393168449401855, 'mae': 2.5443413257598877, 'mse': 11.393168449401855}\n",
      "...Training: end of batch 7; log content: {'loss': 11.319755554199219, 'mae': 2.553497076034546, 'mse': 11.319755554199219}\n",
      "End epoch 157 of training; log content: {'loss': 11.319755554199219, 'mae': 2.553497076034546, 'mse': 11.319755554199219, 'val_loss': 10.022315979003906, 'val_mae': 2.450032949447632, 'val_mse': 10.022315979003906}\n",
      "...Training: end of batch 0; log content: {'loss': 11.260042190551758, 'mae': 2.6316792964935303, 'mse': 11.260042190551758}\n",
      "...Training: end of batch 1; log content: {'loss': 16.417774200439453, 'mae': 3.129117488861084, 'mse': 16.417774200439453}\n",
      "...Training: end of batch 2; log content: {'loss': 12.64758586883545, 'mae': 2.6545257568359375, 'mse': 12.64758586883545}\n",
      "...Training: end of batch 3; log content: {'loss': 11.751739501953125, 'mae': 2.567105770111084, 'mse': 11.751739501953125}\n",
      "...Training: end of batch 4; log content: {'loss': 11.675518989562988, 'mae': 2.5776896476745605, 'mse': 11.675518989562988}\n",
      "...Training: end of batch 5; log content: {'loss': 11.653228759765625, 'mae': 2.593817949295044, 'mse': 11.653228759765625}\n",
      "...Training: end of batch 6; log content: {'loss': 11.697400093078613, 'mae': 2.603572130203247, 'mse': 11.697400093078613}\n",
      "...Training: end of batch 7; log content: {'loss': 11.301858901977539, 'mae': 2.564854621887207, 'mse': 11.301858901977539}\n",
      "End epoch 158 of training; log content: {'loss': 11.301858901977539, 'mae': 2.564854621887207, 'mse': 11.301858901977539, 'val_loss': 10.022526741027832, 'val_mae': 2.4316272735595703, 'val_mse': 10.022526741027832}\n",
      "...Training: end of batch 0; log content: {'loss': 8.523707389831543, 'mae': 2.3453938961029053, 'mse': 8.523707389831543}\n",
      "...Training: end of batch 1; log content: {'loss': 9.008703231811523, 'mae': 2.3519043922424316, 'mse': 9.008703231811523}\n",
      "...Training: end of batch 2; log content: {'loss': 9.910185813903809, 'mae': 2.417989492416382, 'mse': 9.910185813903809}\n",
      "...Training: end of batch 3; log content: {'loss': 11.347763061523438, 'mae': 2.5631847381591797, 'mse': 11.347763061523438}\n",
      "...Training: end of batch 4; log content: {'loss': 10.219736099243164, 'mae': 2.431849956512451, 'mse': 10.219736099243164}\n",
      "...Training: end of batch 5; log content: {'loss': 11.55828857421875, 'mae': 2.564739942550659, 'mse': 11.55828857421875}\n",
      "...Training: end of batch 6; log content: {'loss': 11.47976016998291, 'mae': 2.605086088180542, 'mse': 11.47976016998291}\n",
      "...Training: end of batch 7; log content: {'loss': 11.296563148498535, 'mae': 2.5739898681640625, 'mse': 11.296563148498535}\n",
      "End epoch 159 of training; log content: {'loss': 11.296563148498535, 'mae': 2.5739898681640625, 'mse': 11.296563148498535, 'val_loss': 10.117949485778809, 'val_mae': 2.4332127571105957, 'val_mse': 10.117949485778809}\n",
      "...Training: end of batch 0; log content: {'loss': 12.604486465454102, 'mae': 2.501415729522705, 'mse': 12.604486465454102}\n",
      "...Training: end of batch 1; log content: {'loss': 11.606642723083496, 'mae': 2.7007923126220703, 'mse': 11.606642723083496}\n",
      "...Training: end of batch 2; log content: {'loss': 9.927118301391602, 'mae': 2.4802539348602295, 'mse': 9.927118301391602}\n",
      "...Training: end of batch 3; log content: {'loss': 9.745677947998047, 'mae': 2.4388234615325928, 'mse': 9.745677947998047}\n",
      "...Training: end of batch 4; log content: {'loss': 9.638338088989258, 'mae': 2.4331250190734863, 'mse': 9.638338088989258}\n",
      "...Training: end of batch 5; log content: {'loss': 9.6399507522583, 'mae': 2.4237544536590576, 'mse': 9.6399507522583}\n",
      "...Training: end of batch 6; log content: {'loss': 9.894986152648926, 'mae': 2.448181629180908, 'mse': 9.894986152648926}\n",
      "...Training: end of batch 7; log content: {'loss': 11.353221893310547, 'mae': 2.5812230110168457, 'mse': 11.353221893310547}\n",
      "End epoch 160 of training; log content: {'loss': 11.353221893310547, 'mae': 2.5812230110168457, 'mse': 11.353221893310547, 'val_loss': 10.047283172607422, 'val_mae': 2.4386212825775146, 'val_mse': 10.047283172607422}\n",
      "...Training: end of batch 0; log content: {'loss': 12.91844367980957, 'mae': 2.6852834224700928, 'mse': 12.91844367980957}\n",
      "...Training: end of batch 1; log content: {'loss': 14.214741706848145, 'mae': 2.8906211853027344, 'mse': 14.214741706848145}\n",
      "...Training: end of batch 2; log content: {'loss': 14.404247283935547, 'mae': 2.842768430709839, 'mse': 14.404247283935547}\n",
      "...Training: end of batch 3; log content: {'loss': 13.471601486206055, 'mae': 2.745142936706543, 'mse': 13.471601486206055}\n",
      "...Training: end of batch 4; log content: {'loss': 12.665122985839844, 'mae': 2.699397563934326, 'mse': 12.665122985839844}\n",
      "...Training: end of batch 5; log content: {'loss': 12.413615226745605, 'mae': 2.685319662094116, 'mse': 12.413615226745605}\n",
      "...Training: end of batch 6; log content: {'loss': 11.824216842651367, 'mae': 2.6155102252960205, 'mse': 11.824216842651367}\n",
      "...Training: end of batch 7; log content: {'loss': 11.340532302856445, 'mae': 2.5821266174316406, 'mse': 11.340532302856445}\n",
      "End epoch 161 of training; log content: {'loss': 11.340532302856445, 'mae': 2.5821266174316406, 'mse': 11.340532302856445, 'val_loss': 9.949616432189941, 'val_mae': 2.4272284507751465, 'val_mse': 9.949616432189941}\n",
      "...Training: end of batch 0; log content: {'loss': 10.520580291748047, 'mae': 2.5281593799591064, 'mse': 10.520580291748047}\n",
      "...Training: end of batch 1; log content: {'loss': 13.155133247375488, 'mae': 2.8373212814331055, 'mse': 13.155133247375488}\n",
      "...Training: end of batch 2; log content: {'loss': 11.510249137878418, 'mae': 2.638108491897583, 'mse': 11.510249137878418}\n",
      "...Training: end of batch 3; log content: {'loss': 12.84586238861084, 'mae': 2.718949317932129, 'mse': 12.84586238861084}\n",
      "...Training: end of batch 4; log content: {'loss': 12.38123893737793, 'mae': 2.6818225383758545, 'mse': 12.38123893737793}\n",
      "...Training: end of batch 5; log content: {'loss': 12.056700706481934, 'mae': 2.648435592651367, 'mse': 12.056700706481934}\n",
      "...Training: end of batch 6; log content: {'loss': 11.839547157287598, 'mae': 2.6208865642547607, 'mse': 11.839547157287598}\n",
      "...Training: end of batch 7; log content: {'loss': 11.332406044006348, 'mae': 2.570098400115967, 'mse': 11.332406044006348}\n",
      "End epoch 162 of training; log content: {'loss': 11.332406044006348, 'mae': 2.570098400115967, 'mse': 11.332406044006348, 'val_loss': 10.134325981140137, 'val_mae': 2.4615986347198486, 'val_mse': 10.134325981140137}\n",
      "...Training: end of batch 0; log content: {'loss': 10.698076248168945, 'mae': 2.533878803253174, 'mse': 10.698076248168945}\n",
      "...Training: end of batch 1; log content: {'loss': 8.997894287109375, 'mae': 2.2516894340515137, 'mse': 8.997894287109375}\n",
      "...Training: end of batch 2; log content: {'loss': 9.326266288757324, 'mae': 2.2512762546539307, 'mse': 9.326266288757324}\n",
      "...Training: end of batch 3; log content: {'loss': 11.181085586547852, 'mae': 2.4433212280273438, 'mse': 11.181085586547852}\n",
      "...Training: end of batch 4; log content: {'loss': 11.821942329406738, 'mae': 2.5846831798553467, 'mse': 11.821942329406738}\n",
      "...Training: end of batch 5; log content: {'loss': 11.616019248962402, 'mae': 2.5615475177764893, 'mse': 11.616019248962402}\n",
      "...Training: end of batch 6; log content: {'loss': 11.55125904083252, 'mae': 2.565732717514038, 'mse': 11.55125904083252}\n",
      "...Training: end of batch 7; log content: {'loss': 11.299420356750488, 'mae': 2.5615458488464355, 'mse': 11.299420356750488}\n",
      "End epoch 163 of training; log content: {'loss': 11.299420356750488, 'mae': 2.5615458488464355, 'mse': 11.299420356750488, 'val_loss': 10.064839363098145, 'val_mae': 2.43697190284729, 'val_mse': 10.064839363098145}\n",
      "...Training: end of batch 0; log content: {'loss': 12.427839279174805, 'mae': 2.845615863800049, 'mse': 12.427839279174805}\n",
      "...Training: end of batch 1; log content: {'loss': 14.264443397521973, 'mae': 2.906674861907959, 'mse': 14.264443397521973}\n",
      "...Training: end of batch 2; log content: {'loss': 12.875740051269531, 'mae': 2.6712234020233154, 'mse': 12.875740051269531}\n",
      "...Training: end of batch 3; log content: {'loss': 11.718635559082031, 'mae': 2.559661865234375, 'mse': 11.718635559082031}\n",
      "...Training: end of batch 4; log content: {'loss': 11.639671325683594, 'mae': 2.599334716796875, 'mse': 11.639671325683594}\n",
      "...Training: end of batch 5; log content: {'loss': 11.530208587646484, 'mae': 2.6176536083221436, 'mse': 11.530208587646484}\n",
      "...Training: end of batch 6; log content: {'loss': 11.314550399780273, 'mae': 2.5804519653320312, 'mse': 11.314550399780273}\n",
      "...Training: end of batch 7; log content: {'loss': 11.432588577270508, 'mae': 2.606843948364258, 'mse': 11.432588577270508}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End epoch 164 of training; log content: {'loss': 11.432588577270508, 'mae': 2.606843948364258, 'mse': 11.432588577270508, 'val_loss': 9.924283027648926, 'val_mae': 2.4150688648223877, 'val_mse': 9.924283027648926}\n",
      "...Training: end of batch 0; log content: {'loss': 10.877325057983398, 'mae': 2.730469226837158, 'mse': 10.877325057983398}\n",
      "...Training: end of batch 1; log content: {'loss': 10.55762767791748, 'mae': 2.592839479446411, 'mse': 10.55762767791748}\n",
      "...Training: end of batch 2; log content: {'loss': 9.60256576538086, 'mae': 2.506728410720825, 'mse': 9.60256576538086}\n",
      "...Training: end of batch 3; log content: {'loss': 8.848785400390625, 'mae': 2.3971240520477295, 'mse': 8.848785400390625}\n",
      "...Training: end of batch 4; log content: {'loss': 9.819811820983887, 'mae': 2.495211601257324, 'mse': 9.819811820983887}\n",
      "...Training: end of batch 5; log content: {'loss': 11.377223014831543, 'mae': 2.5955944061279297, 'mse': 11.377223014831543}\n",
      "...Training: end of batch 6; log content: {'loss': 11.629754066467285, 'mae': 2.6092453002929688, 'mse': 11.629754066467285}\n",
      "...Training: end of batch 7; log content: {'loss': 11.393308639526367, 'mae': 2.57766056060791, 'mse': 11.39330768585205}\n",
      "End epoch 165 of training; log content: {'loss': 11.393308639526367, 'mae': 2.57766056060791, 'mse': 11.39330768585205, 'val_loss': 10.216806411743164, 'val_mae': 2.470319986343384, 'val_mse': 10.216806411743164}\n",
      "...Training: end of batch 0; log content: {'loss': 10.29107666015625, 'mae': 2.5209250450134277, 'mse': 10.29107666015625}\n",
      "...Training: end of batch 1; log content: {'loss': 10.722082138061523, 'mae': 2.627016067504883, 'mse': 10.722082138061523}\n",
      "...Training: end of batch 2; log content: {'loss': 10.94575023651123, 'mae': 2.5765292644500732, 'mse': 10.94575023651123}\n",
      "...Training: end of batch 3; log content: {'loss': 11.037467002868652, 'mae': 2.5919387340545654, 'mse': 11.037467002868652}\n",
      "...Training: end of batch 4; log content: {'loss': 12.13124942779541, 'mae': 2.6684327125549316, 'mse': 12.13124942779541}\n",
      "...Training: end of batch 5; log content: {'loss': 11.248581886291504, 'mae': 2.5732991695404053, 'mse': 11.248581886291504}\n",
      "...Training: end of batch 6; log content: {'loss': 11.192039489746094, 'mae': 2.5780670642852783, 'mse': 11.192039489746094}\n",
      "...Training: end of batch 7; log content: {'loss': 11.308414459228516, 'mae': 2.558657646179199, 'mse': 11.308414459228516}\n",
      "End epoch 166 of training; log content: {'loss': 11.308414459228516, 'mae': 2.558657646179199, 'mse': 11.308414459228516, 'val_loss': 10.09593391418457, 'val_mae': 2.442385196685791, 'val_mse': 10.09593391418457}\n",
      "...Training: end of batch 0; log content: {'loss': 5.185019016265869, 'mae': 1.727391242980957, 'mse': 5.185019016265869}\n",
      "...Training: end of batch 1; log content: {'loss': 7.360058784484863, 'mae': 1.9761884212493896, 'mse': 7.360058784484863}\n",
      "...Training: end of batch 2; log content: {'loss': 9.500128746032715, 'mae': 2.3256146907806396, 'mse': 9.500128746032715}\n",
      "...Training: end of batch 3; log content: {'loss': 10.019432067871094, 'mae': 2.4193780422210693, 'mse': 10.019432067871094}\n",
      "...Training: end of batch 4; log content: {'loss': 10.585652351379395, 'mae': 2.501769542694092, 'mse': 10.585652351379395}\n",
      "...Training: end of batch 5; log content: {'loss': 11.358471870422363, 'mae': 2.538140058517456, 'mse': 11.358471870422363}\n",
      "...Training: end of batch 6; log content: {'loss': 11.33881950378418, 'mae': 2.593174457550049, 'mse': 11.33881950378418}\n",
      "...Training: end of batch 7; log content: {'loss': 11.405474662780762, 'mae': 2.606412887573242, 'mse': 11.405474662780762}\n",
      "End epoch 167 of training; log content: {'loss': 11.405474662780762, 'mae': 2.606412887573242, 'mse': 11.405474662780762, 'val_loss': 10.041869163513184, 'val_mae': 2.411083459854126, 'val_mse': 10.041869163513184}\n",
      "...Training: end of batch 0; log content: {'loss': 11.992406845092773, 'mae': 2.901668071746826, 'mse': 11.992406845092773}\n",
      "...Training: end of batch 1; log content: {'loss': 14.436595916748047, 'mae': 2.9333581924438477, 'mse': 14.436595916748047}\n",
      "...Training: end of batch 2; log content: {'loss': 11.68129825592041, 'mae': 2.625244379043579, 'mse': 11.68129825592041}\n",
      "...Training: end of batch 3; log content: {'loss': 11.504010200500488, 'mae': 2.586883306503296, 'mse': 11.504010200500488}\n",
      "...Training: end of batch 4; log content: {'loss': 11.357803344726562, 'mae': 2.5850558280944824, 'mse': 11.357803344726562}\n",
      "...Training: end of batch 5; log content: {'loss': 11.442340850830078, 'mae': 2.551203489303589, 'mse': 11.442340850830078}\n",
      "...Training: end of batch 6; log content: {'loss': 11.509340286254883, 'mae': 2.589803457260132, 'mse': 11.509340286254883}\n",
      "...Training: end of batch 7; log content: {'loss': 11.597403526306152, 'mae': 2.6039462089538574, 'mse': 11.597403526306152}\n",
      "End epoch 168 of training; log content: {'loss': 11.597403526306152, 'mae': 2.6039462089538574, 'mse': 11.597403526306152, 'val_loss': 10.245851516723633, 'val_mae': 2.477559804916382, 'val_mse': 10.245851516723633}\n",
      "...Training: end of batch 0; log content: {'loss': 12.170797348022461, 'mae': 2.753818988800049, 'mse': 12.170797348022461}\n",
      "...Training: end of batch 1; log content: {'loss': 10.095288276672363, 'mae': 2.42110538482666, 'mse': 10.095288276672363}\n",
      "...Training: end of batch 2; log content: {'loss': 9.645709037780762, 'mae': 2.352573871612549, 'mse': 9.645709037780762}\n",
      "...Training: end of batch 3; log content: {'loss': 9.656584739685059, 'mae': 2.3682913780212402, 'mse': 9.656584739685059}\n",
      "...Training: end of batch 4; log content: {'loss': 9.807319641113281, 'mae': 2.3858227729797363, 'mse': 9.807319641113281}\n",
      "...Training: end of batch 5; log content: {'loss': 9.813323020935059, 'mae': 2.372466802597046, 'mse': 9.813323020935059}\n",
      "...Training: end of batch 6; log content: {'loss': 9.621328353881836, 'mae': 2.367635488510132, 'mse': 9.621328353881836}\n",
      "...Training: end of batch 7; log content: {'loss': 11.278250694274902, 'mae': 2.5534989833831787, 'mse': 11.278250694274902}\n",
      "End epoch 169 of training; log content: {'loss': 11.278250694274902, 'mae': 2.5534989833831787, 'mse': 11.278250694274902, 'val_loss': 9.920836448669434, 'val_mae': 2.4248669147491455, 'val_mse': 9.920836448669434}\n",
      "...Training: end of batch 0; log content: {'loss': 10.002134323120117, 'mae': 2.5017032623291016, 'mse': 10.002134323120117}\n",
      "...Training: end of batch 1; log content: {'loss': 10.582794189453125, 'mae': 2.4897210597991943, 'mse': 10.582794189453125}\n",
      "...Training: end of batch 2; log content: {'loss': 11.08655071258545, 'mae': 2.590172529220581, 'mse': 11.08655071258545}\n",
      "...Training: end of batch 3; log content: {'loss': 10.372262001037598, 'mae': 2.5093414783477783, 'mse': 10.372262001037598}\n",
      "...Training: end of batch 4; log content: {'loss': 10.252548217773438, 'mae': 2.5054867267608643, 'mse': 10.252548217773438}\n",
      "...Training: end of batch 5; log content: {'loss': 10.710726737976074, 'mae': 2.554267644882202, 'mse': 10.710726737976074}\n",
      "...Training: end of batch 6; log content: {'loss': 11.615456581115723, 'mae': 2.6088366508483887, 'mse': 11.615456581115723}\n",
      "...Training: end of batch 7; log content: {'loss': 11.402495384216309, 'mae': 2.6046600341796875, 'mse': 11.402495384216309}\n",
      "End epoch 170 of training; log content: {'loss': 11.402495384216309, 'mae': 2.6046600341796875, 'mse': 11.402495384216309, 'val_loss': 9.8546724319458, 'val_mae': 2.4106638431549072, 'val_mse': 9.8546724319458}\n",
      "...Training: end of batch 0; log content: {'loss': 10.456705093383789, 'mae': 2.7673637866973877, 'mse': 10.456705093383789}\n",
      "...Training: end of batch 1; log content: {'loss': 11.489020347595215, 'mae': 2.833617687225342, 'mse': 11.489020347595215}\n",
      "...Training: end of batch 2; log content: {'loss': 9.390378952026367, 'mae': 2.4870526790618896, 'mse': 9.390378952026367}\n",
      "...Training: end of batch 3; log content: {'loss': 9.516508102416992, 'mae': 2.4659290313720703, 'mse': 9.516508102416992}\n",
      "...Training: end of batch 4; log content: {'loss': 11.25225830078125, 'mae': 2.6208598613739014, 'mse': 11.25225830078125}\n",
      "...Training: end of batch 5; log content: {'loss': 11.039572715759277, 'mae': 2.557319402694702, 'mse': 11.039572715759277}\n",
      "...Training: end of batch 6; log content: {'loss': 11.890189170837402, 'mae': 2.6433346271514893, 'mse': 11.890189170837402}\n",
      "...Training: end of batch 7; log content: {'loss': 11.35096549987793, 'mae': 2.573655605316162, 'mse': 11.35096549987793}\n",
      "End epoch 171 of training; log content: {'loss': 11.35096549987793, 'mae': 2.573655605316162, 'mse': 11.35096549987793, 'val_loss': 10.055047988891602, 'val_mae': 2.455251693725586, 'val_mse': 10.055047988891602}\n",
      "...Training: end of batch 0; log content: {'loss': 11.263426780700684, 'mae': 2.5439040660858154, 'mse': 11.263426780700684}\n",
      "...Training: end of batch 1; log content: {'loss': 9.525187492370605, 'mae': 2.363041877746582, 'mse': 9.525187492370605}\n",
      "...Training: end of batch 2; log content: {'loss': 12.462922096252441, 'mae': 2.625105142593384, 'mse': 12.462922096252441}\n",
      "...Training: end of batch 3; log content: {'loss': 11.493207931518555, 'mae': 2.548358201980591, 'mse': 11.493207931518555}\n",
      "...Training: end of batch 4; log content: {'loss': 11.31386661529541, 'mae': 2.515881299972534, 'mse': 11.31386661529541}\n",
      "...Training: end of batch 5; log content: {'loss': 11.344001770019531, 'mae': 2.5437257289886475, 'mse': 11.344001770019531}\n",
      "...Training: end of batch 6; log content: {'loss': 11.01283073425293, 'mae': 2.5084359645843506, 'mse': 11.01283073425293}\n",
      "...Training: end of batch 7; log content: {'loss': 11.348480224609375, 'mae': 2.581740617752075, 'mse': 11.348480224609375}\n",
      "End epoch 172 of training; log content: {'loss': 11.348480224609375, 'mae': 2.581740617752075, 'mse': 11.348480224609375, 'val_loss': 10.097799301147461, 'val_mae': 2.4388723373413086, 'val_mse': 10.097799301147461}\n",
      "...Training: end of batch 0; log content: {'loss': 13.372841835021973, 'mae': 2.717648983001709, 'mse': 13.372841835021973}\n",
      "...Training: end of batch 1; log content: {'loss': 9.112407684326172, 'mae': 2.3013908863067627, 'mse': 9.112407684326172}\n",
      "...Training: end of batch 2; log content: {'loss': 9.202579498291016, 'mae': 2.3201091289520264, 'mse': 9.202579498291016}\n",
      "...Training: end of batch 3; log content: {'loss': 9.527600288391113, 'mae': 2.3873047828674316, 'mse': 9.527600288391113}\n",
      "...Training: end of batch 4; log content: {'loss': 11.169239044189453, 'mae': 2.570610523223877, 'mse': 11.169239044189453}\n",
      "...Training: end of batch 5; log content: {'loss': 10.823959350585938, 'mae': 2.5553205013275146, 'mse': 10.823959350585938}\n",
      "...Training: end of batch 6; log content: {'loss': 10.885246276855469, 'mae': 2.5752532482147217, 'mse': 10.885246276855469}\n",
      "...Training: end of batch 7; log content: {'loss': 11.323868751525879, 'mae': 2.5857317447662354, 'mse': 11.323868751525879}\n",
      "End epoch 173 of training; log content: {'loss': 11.323868751525879, 'mae': 2.5857317447662354, 'mse': 11.323868751525879, 'val_loss': 9.99538516998291, 'val_mae': 2.436854362487793, 'val_mse': 9.99538516998291}\n",
      "...Training: end of batch 0; log content: {'loss': 11.13595199584961, 'mae': 2.6429378986358643, 'mse': 11.13595199584961}\n",
      "...Training: end of batch 1; log content: {'loss': 10.83693790435791, 'mae': 2.5201640129089355, 'mse': 10.83693790435791}\n",
      "...Training: end of batch 2; log content: {'loss': 11.515392303466797, 'mae': 2.6417202949523926, 'mse': 11.515392303466797}\n",
      "...Training: end of batch 3; log content: {'loss': 11.32144546508789, 'mae': 2.617353677749634, 'mse': 11.32144546508789}\n",
      "...Training: end of batch 4; log content: {'loss': 10.521364212036133, 'mae': 2.495910882949829, 'mse': 10.521364212036133}\n",
      "...Training: end of batch 5; log content: {'loss': 10.90229320526123, 'mae': 2.5497195720672607, 'mse': 10.90229320526123}\n",
      "...Training: end of batch 6; log content: {'loss': 10.85438346862793, 'mae': 2.558093547821045, 'mse': 10.85438346862793}\n",
      "...Training: end of batch 7; log content: {'loss': 11.291980743408203, 'mae': 2.573643684387207, 'mse': 11.291980743408203}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End epoch 174 of training; log content: {'loss': 11.291980743408203, 'mae': 2.573643684387207, 'mse': 11.291980743408203, 'val_loss': 10.011096954345703, 'val_mae': 2.445298433303833, 'val_mse': 10.011096954345703}\n",
      "...Training: end of batch 0; log content: {'loss': 10.098746299743652, 'mae': 2.3413033485412598, 'mse': 10.098746299743652}\n",
      "...Training: end of batch 1; log content: {'loss': 14.49561882019043, 'mae': 2.7893435955047607, 'mse': 14.49561882019043}\n",
      "...Training: end of batch 2; log content: {'loss': 12.478515625, 'mae': 2.554910898208618, 'mse': 12.478515625}\n",
      "...Training: end of batch 3; log content: {'loss': 13.329014778137207, 'mae': 2.743533134460449, 'mse': 13.329014778137207}\n",
      "...Training: end of batch 4; log content: {'loss': 12.412055969238281, 'mae': 2.625807523727417, 'mse': 12.412055969238281}\n",
      "...Training: end of batch 5; log content: {'loss': 11.858630180358887, 'mae': 2.5915772914886475, 'mse': 11.858630180358887}\n",
      "...Training: end of batch 6; log content: {'loss': 11.59423828125, 'mae': 2.5678915977478027, 'mse': 11.59423828125}\n",
      "...Training: end of batch 7; log content: {'loss': 11.301054000854492, 'mae': 2.568420886993408, 'mse': 11.301053047180176}\n",
      "End epoch 175 of training; log content: {'loss': 11.301054000854492, 'mae': 2.568420886993408, 'mse': 11.301053047180176, 'val_loss': 10.002337455749512, 'val_mae': 2.4279935359954834, 'val_mse': 10.002337455749512}\n",
      "...Training: end of batch 0; log content: {'loss': 13.53271770477295, 'mae': 3.15903902053833, 'mse': 13.53271770477295}\n",
      "...Training: end of batch 1; log content: {'loss': 12.647024154663086, 'mae': 2.7857894897460938, 'mse': 12.647024154663086}\n",
      "...Training: end of batch 2; log content: {'loss': 13.86212158203125, 'mae': 2.79602313041687, 'mse': 13.86212158203125}\n",
      "...Training: end of batch 3; log content: {'loss': 12.630203247070312, 'mae': 2.6916754245758057, 'mse': 12.630203247070312}\n",
      "...Training: end of batch 4; log content: {'loss': 11.983548164367676, 'mae': 2.6531882286071777, 'mse': 11.983548164367676}\n",
      "...Training: end of batch 5; log content: {'loss': 12.038204193115234, 'mae': 2.623244285583496, 'mse': 12.038204193115234}\n",
      "...Training: end of batch 6; log content: {'loss': 11.264117240905762, 'mae': 2.55078387260437, 'mse': 11.264117240905762}\n",
      "...Training: end of batch 7; log content: {'loss': 11.297396659851074, 'mae': 2.564811944961548, 'mse': 11.297396659851074}\n",
      "End epoch 176 of training; log content: {'loss': 11.297396659851074, 'mae': 2.564811944961548, 'mse': 11.297396659851074, 'val_loss': 10.108037948608398, 'val_mae': 2.446787118911743, 'val_mse': 10.108037948608398}\n",
      "...Training: end of batch 0; log content: {'loss': 8.93912124633789, 'mae': 2.2381389141082764, 'mse': 8.93912124633789}\n",
      "...Training: end of batch 1; log content: {'loss': 10.203278541564941, 'mae': 2.453557014465332, 'mse': 10.203278541564941}\n",
      "...Training: end of batch 2; log content: {'loss': 9.373284339904785, 'mae': 2.374523878097534, 'mse': 9.373284339904785}\n",
      "...Training: end of batch 3; log content: {'loss': 9.206182479858398, 'mae': 2.336331605911255, 'mse': 9.206182479858398}\n",
      "...Training: end of batch 4; log content: {'loss': 9.619949340820312, 'mae': 2.396402597427368, 'mse': 9.619949340820312}\n",
      "...Training: end of batch 5; log content: {'loss': 10.201485633850098, 'mae': 2.4529407024383545, 'mse': 10.201485633850098}\n",
      "...Training: end of batch 6; log content: {'loss': 10.687257766723633, 'mae': 2.5082435607910156, 'mse': 10.687257766723633}\n",
      "...Training: end of batch 7; log content: {'loss': 11.35689640045166, 'mae': 2.5595381259918213, 'mse': 11.35689640045166}\n",
      "End epoch 177 of training; log content: {'loss': 11.35689640045166, 'mae': 2.5595381259918213, 'mse': 11.35689640045166, 'val_loss': 10.045819282531738, 'val_mae': 2.437091827392578, 'val_mse': 10.045819282531738}\n",
      "...Training: end of batch 0; log content: {'loss': 9.864349365234375, 'mae': 2.5663461685180664, 'mse': 9.864349365234375}\n",
      "...Training: end of batch 1; log content: {'loss': 11.152517318725586, 'mae': 2.7912240028381348, 'mse': 11.152517318725586}\n",
      "...Training: end of batch 2; log content: {'loss': 13.361956596374512, 'mae': 2.8351573944091797, 'mse': 13.361956596374512}\n",
      "...Training: end of batch 3; log content: {'loss': 11.687591552734375, 'mae': 2.654179096221924, 'mse': 11.687591552734375}\n",
      "...Training: end of batch 4; log content: {'loss': 11.791555404663086, 'mae': 2.6438424587249756, 'mse': 11.791555404663086}\n",
      "...Training: end of batch 5; log content: {'loss': 11.876181602478027, 'mae': 2.650801658630371, 'mse': 11.876181602478027}\n",
      "...Training: end of batch 6; log content: {'loss': 11.955811500549316, 'mae': 2.664637804031372, 'mse': 11.955811500549316}\n",
      "...Training: end of batch 7; log content: {'loss': 11.551536560058594, 'mae': 2.6243693828582764, 'mse': 11.551536560058594}\n",
      "End epoch 178 of training; log content: {'loss': 11.551536560058594, 'mae': 2.6243693828582764, 'mse': 11.551536560058594, 'val_loss': 10.031913757324219, 'val_mae': 2.3973870277404785, 'val_mse': 10.031913757324219}\n",
      "...Training: end of batch 0; log content: {'loss': 9.945211410522461, 'mae': 2.5151138305664062, 'mse': 9.945211410522461}\n",
      "...Training: end of batch 1; log content: {'loss': 15.487136840820312, 'mae': 3.03061580657959, 'mse': 15.487136840820312}\n",
      "...Training: end of batch 2; log content: {'loss': 12.488116264343262, 'mae': 2.700618028640747, 'mse': 12.488116264343262}\n",
      "...Training: end of batch 3; log content: {'loss': 11.185155868530273, 'mae': 2.553597927093506, 'mse': 11.185155868530273}\n",
      "...Training: end of batch 4; log content: {'loss': 11.977947235107422, 'mae': 2.669482946395874, 'mse': 11.977947235107422}\n",
      "...Training: end of batch 5; log content: {'loss': 11.939549446105957, 'mae': 2.631593942642212, 'mse': 11.939549446105957}\n",
      "...Training: end of batch 6; log content: {'loss': 11.260018348693848, 'mae': 2.5783753395080566, 'mse': 11.260018348693848}\n",
      "...Training: end of batch 7; log content: {'loss': 11.41011905670166, 'mae': 2.589726448059082, 'mse': 11.41011905670166}\n",
      "End epoch 179 of training; log content: {'loss': 11.41011905670166, 'mae': 2.589726448059082, 'mse': 11.41011905670166, 'val_loss': 10.425352096557617, 'val_mae': 2.48384952545166, 'val_mse': 10.425352096557617}\n",
      "...Training: end of batch 0; log content: {'loss': 16.44475746154785, 'mae': 3.318601131439209, 'mse': 16.44475746154785}\n",
      "...Training: end of batch 1; log content: {'loss': 15.485578536987305, 'mae': 3.11423921585083, 'mse': 15.485578536987305}\n",
      "...Training: end of batch 2; log content: {'loss': 12.75171184539795, 'mae': 2.824817657470703, 'mse': 12.75171184539795}\n",
      "...Training: end of batch 3; log content: {'loss': 12.459915161132812, 'mae': 2.7210755348205566, 'mse': 12.459915161132812}\n",
      "...Training: end of batch 4; log content: {'loss': 12.223993301391602, 'mae': 2.6771321296691895, 'mse': 12.223993301391602}\n",
      "...Training: end of batch 5; log content: {'loss': 11.411957740783691, 'mae': 2.5994350910186768, 'mse': 11.411957740783691}\n",
      "...Training: end of batch 6; log content: {'loss': 11.102871894836426, 'mae': 2.54038405418396, 'mse': 11.102871894836426}\n",
      "...Training: end of batch 7; log content: {'loss': 11.403464317321777, 'mae': 2.5737743377685547, 'mse': 11.403464317321777}\n",
      "End epoch 180 of training; log content: {'loss': 11.403464317321777, 'mae': 2.5737743377685547, 'mse': 11.403464317321777, 'val_loss': 9.891210556030273, 'val_mae': 2.4239561557769775, 'val_mse': 9.891210556030273}\n",
      "...Training: end of batch 0; log content: {'loss': 18.15677261352539, 'mae': 3.2210726737976074, 'mse': 18.15677261352539}\n",
      "...Training: end of batch 1; log content: {'loss': 14.790898323059082, 'mae': 2.923377275466919, 'mse': 14.790898323059082}\n",
      "...Training: end of batch 2; log content: {'loss': 12.568718910217285, 'mae': 2.7161598205566406, 'mse': 12.568718910217285}\n",
      "...Training: end of batch 3; log content: {'loss': 11.20433235168457, 'mae': 2.5459518432617188, 'mse': 11.20433235168457}\n",
      "...Training: end of batch 4; log content: {'loss': 10.85693645477295, 'mae': 2.5008950233459473, 'mse': 10.85693645477295}\n",
      "...Training: end of batch 5; log content: {'loss': 10.655834197998047, 'mae': 2.4617340564727783, 'mse': 10.655834197998047}\n",
      "...Training: end of batch 6; log content: {'loss': 11.260003089904785, 'mae': 2.5385658740997314, 'mse': 11.260003089904785}\n",
      "...Training: end of batch 7; log content: {'loss': 11.341814994812012, 'mae': 2.5890653133392334, 'mse': 11.341814994812012}\n",
      "End epoch 181 of training; log content: {'loss': 11.341814994812012, 'mae': 2.5890653133392334, 'mse': 11.341814994812012, 'val_loss': 9.81904125213623, 'val_mae': 2.4293668270111084, 'val_mse': 9.81904125213623}\n",
      "...Training: end of batch 0; log content: {'loss': 7.938017845153809, 'mae': 2.3285083770751953, 'mse': 7.938017845153809}\n",
      "...Training: end of batch 1; log content: {'loss': 10.823562622070312, 'mae': 2.4868428707122803, 'mse': 10.823562622070312}\n",
      "...Training: end of batch 2; log content: {'loss': 11.894991874694824, 'mae': 2.6641476154327393, 'mse': 11.894991874694824}\n",
      "...Training: end of batch 3; log content: {'loss': 11.60594367980957, 'mae': 2.6289072036743164, 'mse': 11.60594367980957}\n",
      "...Training: end of batch 4; log content: {'loss': 11.283940315246582, 'mae': 2.569572925567627, 'mse': 11.283940315246582}\n",
      "...Training: end of batch 5; log content: {'loss': 11.1842041015625, 'mae': 2.569199323654175, 'mse': 11.1842041015625}\n",
      "...Training: end of batch 6; log content: {'loss': 11.68890380859375, 'mae': 2.601389169692993, 'mse': 11.68890380859375}\n",
      "...Training: end of batch 7; log content: {'loss': 11.301274299621582, 'mae': 2.574479103088379, 'mse': 11.301274299621582}\n",
      "End epoch 182 of training; log content: {'loss': 11.301274299621582, 'mae': 2.574479103088379, 'mse': 11.301274299621582, 'val_loss': 9.903047561645508, 'val_mae': 2.4344124794006348, 'val_mse': 9.903047561645508}\n",
      "...Training: end of batch 0; log content: {'loss': 8.162711143493652, 'mae': 2.1612801551818848, 'mse': 8.162711143493652}\n",
      "...Training: end of batch 1; log content: {'loss': 6.908867359161377, 'mae': 2.0154659748077393, 'mse': 6.908867359161377}\n",
      "...Training: end of batch 2; log content: {'loss': 8.044479370117188, 'mae': 2.1050617694854736, 'mse': 8.044479370117188}\n",
      "...Training: end of batch 3; log content: {'loss': 10.721355438232422, 'mae': 2.3624701499938965, 'mse': 10.721355438232422}\n",
      "...Training: end of batch 4; log content: {'loss': 11.922987937927246, 'mae': 2.517829656600952, 'mse': 11.922987937927246}\n",
      "...Training: end of batch 5; log content: {'loss': 11.489109992980957, 'mae': 2.5141594409942627, 'mse': 11.489109992980957}\n",
      "...Training: end of batch 6; log content: {'loss': 11.057129859924316, 'mae': 2.5212745666503906, 'mse': 11.057129859924316}\n",
      "...Training: end of batch 7; log content: {'loss': 11.342024803161621, 'mae': 2.5781197547912598, 'mse': 11.342024803161621}\n",
      "End epoch 183 of training; log content: {'loss': 11.342024803161621, 'mae': 2.5781197547912598, 'mse': 11.342024803161621, 'val_loss': 10.0523681640625, 'val_mae': 2.4404218196868896, 'val_mse': 10.0523681640625}\n",
      "...Training: end of batch 0; log content: {'loss': 8.420048713684082, 'mae': 2.2406063079833984, 'mse': 8.420048713684082}\n",
      "...Training: end of batch 1; log content: {'loss': 9.766366958618164, 'mae': 2.4186902046203613, 'mse': 9.766366958618164}\n",
      "...Training: end of batch 2; log content: {'loss': 10.345968246459961, 'mae': 2.5052099227905273, 'mse': 10.345968246459961}\n",
      "...Training: end of batch 3; log content: {'loss': 10.706603050231934, 'mae': 2.5033295154571533, 'mse': 10.706603050231934}\n",
      "...Training: end of batch 4; log content: {'loss': 10.716672897338867, 'mae': 2.520167350769043, 'mse': 10.716672897338867}\n",
      "...Training: end of batch 5; log content: {'loss': 10.526651382446289, 'mae': 2.53116774559021, 'mse': 10.526651382446289}\n",
      "...Training: end of batch 6; log content: {'loss': 11.291505813598633, 'mae': 2.545274019241333, 'mse': 11.291505813598633}\n",
      "...Training: end of batch 7; log content: {'loss': 11.285603523254395, 'mae': 2.5758602619171143, 'mse': 11.285603523254395}\n",
      "End epoch 184 of training; log content: {'loss': 11.285603523254395, 'mae': 2.5758602619171143, 'mse': 11.285603523254395, 'val_loss': 10.061060905456543, 'val_mae': 2.445176362991333, 'val_mse': 10.061060905456543}\n",
      "...Training: end of batch 0; log content: {'loss': 10.794230461120605, 'mae': 2.6522669792175293, 'mse': 10.794230461120605}\n",
      "...Training: end of batch 1; log content: {'loss': 10.568166732788086, 'mae': 2.546998977661133, 'mse': 10.568166732788086}\n",
      "...Training: end of batch 2; log content: {'loss': 11.165542602539062, 'mae': 2.688230276107788, 'mse': 11.165542602539062}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Training: end of batch 3; log content: {'loss': 10.248334884643555, 'mae': 2.587010145187378, 'mse': 10.248334884643555}\n",
      "...Training: end of batch 4; log content: {'loss': 10.638434410095215, 'mae': 2.606750965118408, 'mse': 10.638434410095215}\n",
      "...Training: end of batch 5; log content: {'loss': 10.939873695373535, 'mae': 2.610248327255249, 'mse': 10.939873695373535}\n",
      "...Training: end of batch 6; log content: {'loss': 11.34882926940918, 'mae': 2.6258223056793213, 'mse': 11.34882926940918}\n",
      "...Training: end of batch 7; log content: {'loss': 11.323558807373047, 'mae': 2.5732691287994385, 'mse': 11.323558807373047}\n",
      "End epoch 185 of training; log content: {'loss': 11.323558807373047, 'mae': 2.5732691287994385, 'mse': 11.323558807373047, 'val_loss': 10.019936561584473, 'val_mae': 2.440904378890991, 'val_mse': 10.019936561584473}\n",
      "...Training: end of batch 0; log content: {'loss': 5.464108467102051, 'mae': 1.8658342361450195, 'mse': 5.464108467102051}\n",
      "...Training: end of batch 1; log content: {'loss': 8.473737716674805, 'mae': 2.2441914081573486, 'mse': 8.473737716674805}\n",
      "...Training: end of batch 2; log content: {'loss': 9.683605194091797, 'mae': 2.417659044265747, 'mse': 9.683605194091797}\n",
      "...Training: end of batch 3; log content: {'loss': 9.985544204711914, 'mae': 2.4687438011169434, 'mse': 9.985544204711914}\n",
      "...Training: end of batch 4; log content: {'loss': 11.732495307922363, 'mae': 2.669325828552246, 'mse': 11.732495307922363}\n",
      "...Training: end of batch 5; log content: {'loss': 10.564131736755371, 'mae': 2.5021815299987793, 'mse': 10.564131736755371}\n",
      "...Training: end of batch 6; log content: {'loss': 11.05994987487793, 'mae': 2.555178165435791, 'mse': 11.05994987487793}\n",
      "...Training: end of batch 7; log content: {'loss': 11.33466911315918, 'mae': 2.5906283855438232, 'mse': 11.33466911315918}\n",
      "End epoch 186 of training; log content: {'loss': 11.33466911315918, 'mae': 2.5906283855438232, 'mse': 11.33466911315918, 'val_loss': 9.93620777130127, 'val_mae': 2.408439874649048, 'val_mse': 9.93620777130127}\n",
      "...Training: end of batch 0; log content: {'loss': 9.784162521362305, 'mae': 2.5521678924560547, 'mse': 9.784162521362305}\n",
      "...Training: end of batch 1; log content: {'loss': 8.774955749511719, 'mae': 2.4355392456054688, 'mse': 8.774955749511719}\n",
      "...Training: end of batch 2; log content: {'loss': 8.667119026184082, 'mae': 2.346349000930786, 'mse': 8.667119026184082}\n",
      "...Training: end of batch 3; log content: {'loss': 8.774669647216797, 'mae': 2.3159894943237305, 'mse': 8.774669647216797}\n",
      "...Training: end of batch 4; log content: {'loss': 10.037216186523438, 'mae': 2.4302124977111816, 'mse': 10.037216186523438}\n",
      "...Training: end of batch 5; log content: {'loss': 9.70576000213623, 'mae': 2.397780179977417, 'mse': 9.70576000213623}\n",
      "...Training: end of batch 6; log content: {'loss': 11.233400344848633, 'mae': 2.5266001224517822, 'mse': 11.233400344848633}\n",
      "...Training: end of batch 7; log content: {'loss': 11.349594116210938, 'mae': 2.5802481174468994, 'mse': 11.349594116210938}\n",
      "End epoch 187 of training; log content: {'loss': 11.349594116210938, 'mae': 2.5802481174468994, 'mse': 11.349594116210938, 'val_loss': 10.127056121826172, 'val_mae': 2.447997808456421, 'val_mse': 10.127056121826172}\n",
      "...Training: end of batch 0; log content: {'loss': 15.523733139038086, 'mae': 2.8784518241882324, 'mse': 15.523733139038086}\n",
      "...Training: end of batch 1; log content: {'loss': 14.577536582946777, 'mae': 2.8891379833221436, 'mse': 14.577536582946777}\n",
      "...Training: end of batch 2; log content: {'loss': 13.626716613769531, 'mae': 2.720245361328125, 'mse': 13.626716613769531}\n",
      "...Training: end of batch 3; log content: {'loss': 11.908175468444824, 'mae': 2.569643020629883, 'mse': 11.908175468444824}\n",
      "...Training: end of batch 4; log content: {'loss': 10.90681266784668, 'mae': 2.5007951259613037, 'mse': 10.90681266784668}\n",
      "...Training: end of batch 5; log content: {'loss': 10.81978702545166, 'mae': 2.4998672008514404, 'mse': 10.81978702545166}\n",
      "...Training: end of batch 6; log content: {'loss': 11.38597583770752, 'mae': 2.5815985202789307, 'mse': 11.38597583770752}\n",
      "...Training: end of batch 7; log content: {'loss': 11.338821411132812, 'mae': 2.581446647644043, 'mse': 11.338821411132812}\n",
      "End epoch 188 of training; log content: {'loss': 11.338821411132812, 'mae': 2.581446647644043, 'mse': 11.338821411132812, 'val_loss': 9.970283508300781, 'val_mae': 2.4287269115448, 'val_mse': 9.970283508300781}\n",
      "...Training: end of batch 0; log content: {'loss': 13.206735610961914, 'mae': 2.947688579559326, 'mse': 13.206735610961914}\n",
      "...Training: end of batch 1; log content: {'loss': 10.761119842529297, 'mae': 2.599949598312378, 'mse': 10.761119842529297}\n",
      "...Training: end of batch 2; log content: {'loss': 11.733304023742676, 'mae': 2.7205753326416016, 'mse': 11.733304023742676}\n",
      "...Training: end of batch 3; log content: {'loss': 10.857858657836914, 'mae': 2.6214771270751953, 'mse': 10.857858657836914}\n",
      "...Training: end of batch 4; log content: {'loss': 10.951759338378906, 'mae': 2.574183702468872, 'mse': 10.951759338378906}\n",
      "...Training: end of batch 5; log content: {'loss': 11.644874572753906, 'mae': 2.6062185764312744, 'mse': 11.644874572753906}\n",
      "...Training: end of batch 6; log content: {'loss': 11.302911758422852, 'mae': 2.566913366317749, 'mse': 11.302911758422852}\n",
      "...Training: end of batch 7; log content: {'loss': 11.3270902633667, 'mae': 2.565699577331543, 'mse': 11.3270902633667}\n",
      "End epoch 189 of training; log content: {'loss': 11.3270902633667, 'mae': 2.565699577331543, 'mse': 11.3270902633667, 'val_loss': 10.043940544128418, 'val_mae': 2.4572012424468994, 'val_mse': 10.043940544128418}\n",
      "...Training: end of batch 0; log content: {'loss': 14.613272666931152, 'mae': 2.9083967208862305, 'mse': 14.613272666931152}\n",
      "...Training: end of batch 1; log content: {'loss': 14.074203491210938, 'mae': 2.784217596054077, 'mse': 14.074203491210938}\n",
      "...Training: end of batch 2; log content: {'loss': 12.413107872009277, 'mae': 2.7167112827301025, 'mse': 12.413107872009277}\n",
      "...Training: end of batch 3; log content: {'loss': 12.009772300720215, 'mae': 2.6887407302856445, 'mse': 12.009772300720215}\n",
      "...Training: end of batch 4; log content: {'loss': 12.504196166992188, 'mae': 2.7136216163635254, 'mse': 12.504196166992188}\n",
      "...Training: end of batch 5; log content: {'loss': 11.760539054870605, 'mae': 2.625223159790039, 'mse': 11.760539054870605}\n",
      "...Training: end of batch 6; log content: {'loss': 11.305665016174316, 'mae': 2.5839850902557373, 'mse': 11.305665016174316}\n",
      "...Training: end of batch 7; log content: {'loss': 11.381524085998535, 'mae': 2.582030773162842, 'mse': 11.381524085998535}\n",
      "End epoch 190 of training; log content: {'loss': 11.381524085998535, 'mae': 2.582030773162842, 'mse': 11.381524085998535, 'val_loss': 9.878433227539062, 'val_mae': 2.4177772998809814, 'val_mse': 9.878433227539062}\n",
      "...Training: end of batch 0; log content: {'loss': 8.982708930969238, 'mae': 2.454226493835449, 'mse': 8.982708930969238}\n",
      "...Training: end of batch 1; log content: {'loss': 10.472845077514648, 'mae': 2.4973039627075195, 'mse': 10.472845077514648}\n",
      "...Training: end of batch 2; log content: {'loss': 9.219009399414062, 'mae': 2.343839645385742, 'mse': 9.219009399414062}\n",
      "...Training: end of batch 3; log content: {'loss': 9.58218002319336, 'mae': 2.3969199657440186, 'mse': 9.58218002319336}\n",
      "...Training: end of batch 4; log content: {'loss': 10.52253532409668, 'mae': 2.462404251098633, 'mse': 10.52253532409668}\n",
      "...Training: end of batch 5; log content: {'loss': 10.725110054016113, 'mae': 2.511190176010132, 'mse': 10.725110054016113}\n",
      "...Training: end of batch 6; log content: {'loss': 10.341158866882324, 'mae': 2.4641494750976562, 'mse': 10.341158866882324}\n",
      "...Training: end of batch 7; log content: {'loss': 11.300814628601074, 'mae': 2.573807954788208, 'mse': 11.300814628601074}\n",
      "End epoch 191 of training; log content: {'loss': 11.300814628601074, 'mae': 2.573807954788208, 'mse': 11.300814628601074, 'val_loss': 10.054932594299316, 'val_mae': 2.439483404159546, 'val_mse': 10.054932594299316}\n",
      "...Training: end of batch 0; log content: {'loss': 15.16004753112793, 'mae': 2.6904397010803223, 'mse': 15.16004753112793}\n",
      "...Training: end of batch 1; log content: {'loss': 14.512569427490234, 'mae': 2.8022069931030273, 'mse': 14.512569427490234}\n",
      "...Training: end of batch 2; log content: {'loss': 14.540303230285645, 'mae': 2.839833974838257, 'mse': 14.540303230285645}\n",
      "...Training: end of batch 3; log content: {'loss': 13.441778182983398, 'mae': 2.8031229972839355, 'mse': 13.441778182983398}\n",
      "...Training: end of batch 4; log content: {'loss': 13.347799301147461, 'mae': 2.7845306396484375, 'mse': 13.347799301147461}\n",
      "...Training: end of batch 5; log content: {'loss': 12.140052795410156, 'mae': 2.6536552906036377, 'mse': 12.140052795410156}\n",
      "...Training: end of batch 6; log content: {'loss': 11.826106071472168, 'mae': 2.626490831375122, 'mse': 11.826106071472168}\n",
      "...Training: end of batch 7; log content: {'loss': 11.307941436767578, 'mae': 2.5812230110168457, 'mse': 11.307941436767578}\n",
      "End epoch 192 of training; log content: {'loss': 11.307941436767578, 'mae': 2.5812230110168457, 'mse': 11.307941436767578, 'val_loss': 10.215209007263184, 'val_mae': 2.4541423320770264, 'val_mse': 10.215209007263184}\n",
      "...Training: end of batch 0; log content: {'loss': 12.646289825439453, 'mae': 2.752373218536377, 'mse': 12.646289825439453}\n",
      "...Training: end of batch 1; log content: {'loss': 9.51736068725586, 'mae': 2.425960063934326, 'mse': 9.51736068725586}\n",
      "...Training: end of batch 2; log content: {'loss': 12.199424743652344, 'mae': 2.622360944747925, 'mse': 12.199424743652344}\n",
      "...Training: end of batch 3; log content: {'loss': 11.653017044067383, 'mae': 2.524235963821411, 'mse': 11.653017044067383}\n",
      "...Training: end of batch 4; log content: {'loss': 11.920196533203125, 'mae': 2.593870162963867, 'mse': 11.920196533203125}\n",
      "...Training: end of batch 5; log content: {'loss': 12.053123474121094, 'mae': 2.6531856060028076, 'mse': 12.053123474121094}\n",
      "...Training: end of batch 6; log content: {'loss': 11.419600486755371, 'mae': 2.5735714435577393, 'mse': 11.419600486755371}\n",
      "...Training: end of batch 7; log content: {'loss': 11.304228782653809, 'mae': 2.579429864883423, 'mse': 11.304228782653809}\n",
      "End epoch 193 of training; log content: {'loss': 11.304228782653809, 'mae': 2.579429864883423, 'mse': 11.304228782653809, 'val_loss': 10.078703880310059, 'val_mae': 2.4376885890960693, 'val_mse': 10.078703880310059}\n",
      "...Training: end of batch 0; log content: {'loss': 11.173731803894043, 'mae': 2.575131893157959, 'mse': 11.173731803894043}\n",
      "...Training: end of batch 1; log content: {'loss': 13.594743728637695, 'mae': 2.824310541152954, 'mse': 13.594743728637695}\n",
      "...Training: end of batch 2; log content: {'loss': 12.315546989440918, 'mae': 2.704249620437622, 'mse': 12.315546989440918}\n",
      "...Training: end of batch 3; log content: {'loss': 12.296215057373047, 'mae': 2.6907474994659424, 'mse': 12.296215057373047}\n",
      "...Training: end of batch 4; log content: {'loss': 11.946905136108398, 'mae': 2.633509397506714, 'mse': 11.946905136108398}\n",
      "...Training: end of batch 5; log content: {'loss': 12.231982231140137, 'mae': 2.6474902629852295, 'mse': 12.231982231140137}\n",
      "...Training: end of batch 6; log content: {'loss': 11.919690132141113, 'mae': 2.6557252407073975, 'mse': 11.919690132141113}\n",
      "...Training: end of batch 7; log content: {'loss': 11.375995635986328, 'mae': 2.6037495136260986, 'mse': 11.375995635986328}\n",
      "End epoch 194 of training; log content: {'loss': 11.375995635986328, 'mae': 2.6037495136260986, 'mse': 11.375995635986328, 'val_loss': 9.868770599365234, 'val_mae': 2.414921760559082, 'val_mse': 9.868770599365234}\n",
      "...Training: end of batch 0; log content: {'loss': 8.518775939941406, 'mae': 2.1754767894744873, 'mse': 8.518775939941406}\n",
      "...Training: end of batch 1; log content: {'loss': 8.877974510192871, 'mae': 2.266632556915283, 'mse': 8.877974510192871}\n",
      "...Training: end of batch 2; log content: {'loss': 10.013932228088379, 'mae': 2.430830478668213, 'mse': 10.013932228088379}\n",
      "...Training: end of batch 3; log content: {'loss': 10.533708572387695, 'mae': 2.4299583435058594, 'mse': 10.533708572387695}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Training: end of batch 4; log content: {'loss': 11.849218368530273, 'mae': 2.6018729209899902, 'mse': 11.849218368530273}\n",
      "...Training: end of batch 5; log content: {'loss': 11.50069808959961, 'mae': 2.5721538066864014, 'mse': 11.50069808959961}\n",
      "...Training: end of batch 6; log content: {'loss': 11.498367309570312, 'mae': 2.5616977214813232, 'mse': 11.498367309570312}\n",
      "...Training: end of batch 7; log content: {'loss': 11.356362342834473, 'mae': 2.5678436756134033, 'mse': 11.356362342834473}\n",
      "End epoch 195 of training; log content: {'loss': 11.356362342834473, 'mae': 2.5678436756134033, 'mse': 11.356362342834473, 'val_loss': 10.046046257019043, 'val_mae': 2.4511568546295166, 'val_mse': 10.046046257019043}\n",
      "...Training: end of batch 0; log content: {'loss': 8.762008666992188, 'mae': 2.4012558460235596, 'mse': 8.762008666992188}\n",
      "...Training: end of batch 1; log content: {'loss': 11.206226348876953, 'mae': 2.597317695617676, 'mse': 11.206226348876953}\n",
      "...Training: end of batch 2; log content: {'loss': 12.68940258026123, 'mae': 2.653820037841797, 'mse': 12.68940258026123}\n",
      "...Training: end of batch 3; log content: {'loss': 12.649980545043945, 'mae': 2.640871047973633, 'mse': 12.649980545043945}\n",
      "...Training: end of batch 4; log content: {'loss': 12.036213874816895, 'mae': 2.580753803253174, 'mse': 12.036213874816895}\n",
      "...Training: end of batch 5; log content: {'loss': 12.00668716430664, 'mae': 2.613659143447876, 'mse': 12.00668716430664}\n",
      "...Training: end of batch 6; log content: {'loss': 11.420355796813965, 'mae': 2.587001323699951, 'mse': 11.420355796813965}\n",
      "...Training: end of batch 7; log content: {'loss': 11.335223197937012, 'mae': 2.5719985961914062, 'mse': 11.335223197937012}\n",
      "End epoch 196 of training; log content: {'loss': 11.335223197937012, 'mae': 2.5719985961914062, 'mse': 11.335223197937012, 'val_loss': 9.903148651123047, 'val_mae': 2.4271342754364014, 'val_mse': 9.903148651123047}\n",
      "...Training: end of batch 0; log content: {'loss': 7.011923313140869, 'mae': 2.2189688682556152, 'mse': 7.011923313140869}\n",
      "...Training: end of batch 1; log content: {'loss': 9.075851440429688, 'mae': 2.4223337173461914, 'mse': 9.075851440429688}\n",
      "...Training: end of batch 2; log content: {'loss': 9.860005378723145, 'mae': 2.3710720539093018, 'mse': 9.860005378723145}\n",
      "...Training: end of batch 3; log content: {'loss': 9.74776840209961, 'mae': 2.386608600616455, 'mse': 9.74776840209961}\n",
      "...Training: end of batch 4; log content: {'loss': 9.696748733520508, 'mae': 2.405917167663574, 'mse': 9.696748733520508}\n",
      "...Training: end of batch 5; log content: {'loss': 10.099644660949707, 'mae': 2.461895704269409, 'mse': 10.099644660949707}\n",
      "...Training: end of batch 6; log content: {'loss': 10.380706787109375, 'mae': 2.4904561042785645, 'mse': 10.380706787109375}\n",
      "...Training: end of batch 7; log content: {'loss': 11.291720390319824, 'mae': 2.5684585571289062, 'mse': 11.291720390319824}\n",
      "End epoch 197 of training; log content: {'loss': 11.291720390319824, 'mae': 2.5684585571289062, 'mse': 11.291720390319824, 'val_loss': 9.960358619689941, 'val_mae': 2.436235189437866, 'val_mse': 9.960358619689941}\n",
      "...Training: end of batch 0; log content: {'loss': 10.457006454467773, 'mae': 2.6049680709838867, 'mse': 10.457006454467773}\n",
      "...Training: end of batch 1; log content: {'loss': 12.632461547851562, 'mae': 2.7494826316833496, 'mse': 12.632461547851562}\n",
      "...Training: end of batch 2; log content: {'loss': 11.993694305419922, 'mae': 2.6861655712127686, 'mse': 11.993694305419922}\n",
      "...Training: end of batch 3; log content: {'loss': 11.795989990234375, 'mae': 2.676677703857422, 'mse': 11.795989990234375}\n",
      "...Training: end of batch 4; log content: {'loss': 11.451241493225098, 'mae': 2.6172091960906982, 'mse': 11.451241493225098}\n",
      "...Training: end of batch 5; log content: {'loss': 11.41287899017334, 'mae': 2.5945844650268555, 'mse': 11.41287899017334}\n",
      "...Training: end of batch 6; log content: {'loss': 11.372002601623535, 'mae': 2.56801438331604, 'mse': 11.372002601623535}\n",
      "...Training: end of batch 7; log content: {'loss': 11.332562446594238, 'mae': 2.5666372776031494, 'mse': 11.332562446594238}\n",
      "End epoch 198 of training; log content: {'loss': 11.332562446594238, 'mae': 2.5666372776031494, 'mse': 11.332562446594238, 'val_loss': 10.15987491607666, 'val_mae': 2.441192865371704, 'val_mse': 10.15987491607666}\n",
      "...Training: end of batch 0; log content: {'loss': 11.947837829589844, 'mae': 2.72963547706604, 'mse': 11.947837829589844}\n",
      "...Training: end of batch 1; log content: {'loss': 10.149660110473633, 'mae': 2.439065933227539, 'mse': 10.149660110473633}\n",
      "...Training: end of batch 2; log content: {'loss': 8.454413414001465, 'mae': 2.246542453765869, 'mse': 8.454413414001465}\n",
      "...Training: end of batch 3; log content: {'loss': 10.722827911376953, 'mae': 2.4666876792907715, 'mse': 10.722827911376953}\n",
      "...Training: end of batch 4; log content: {'loss': 10.175670623779297, 'mae': 2.4627315998077393, 'mse': 10.175670623779297}\n",
      "...Training: end of batch 5; log content: {'loss': 9.852303504943848, 'mae': 2.4365170001983643, 'mse': 9.852303504943848}\n",
      "...Training: end of batch 6; log content: {'loss': 10.581533432006836, 'mae': 2.517969846725464, 'mse': 10.581533432006836}\n",
      "...Training: end of batch 7; log content: {'loss': 11.354451179504395, 'mae': 2.5908710956573486, 'mse': 11.354451179504395}\n",
      "End epoch 199 of training; log content: {'loss': 11.354451179504395, 'mae': 2.5908710956573486, 'mse': 11.354451179504395, 'val_loss': 10.103252410888672, 'val_mae': 2.4258363246917725, 'val_mse': 10.103252410888672}\n",
      "Stop training; got log content: {'loss': 11.354451179504395, 'mae': 2.5908710956573486, 'mse': 11.354451179504395, 'val_loss': 10.103252410888672, 'val_mae': 2.4258363246917725, 'val_mse': 10.103252410888672}\n"
     ]
    }
   ],
   "source": [
    "log_callback = CustomLogger()\n",
    "\n",
    "model = get_model()\n",
    "history = model.fit(train_features, train_labels, epochs=200,\n",
    "                                verbose=0, #verbose=0 to avoid mixing our prints and those of standard keras\n",
    "                                validation_split = 0.2,\n",
    "                                callbacks=[log_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Callback N. 2\n",
    "\n",
    "Let us write another example of custom callback. This time, let's implement a custom early stopping mechanism on a pre-defined Validation MAE value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEarlyStopping(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        if(logs['val_mae']< 10.0):\n",
    "            print(\"\\nReached MAE < 10.0, so cancelling training!\")\n",
    "            self.model.stop_training = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 587.5167 - mae: 23.0101 - mse: 587.5167 - val_loss: 556.3007 - val_mae: 22.8800 - val_mse: 556.3007\n",
      "Epoch 2/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 515.3938 - mae: 22.1182 - mse: 515.3938 - val_loss: 502.1757 - val_mae: 22.0296 - val_mse: 502.1757\n",
      "Epoch 3/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 473.7619 - mae: 21.3449 - mse: 473.7619 - val_loss: 466.4300 - val_mae: 21.2633 - val_mse: 466.4300\n",
      "Epoch 4/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 438.5339 - mae: 20.5317 - mse: 438.5339 - val_loss: 435.8839 - val_mae: 20.5349 - val_mse: 435.8839\n",
      "Epoch 5/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 406.1021 - mae: 19.7481 - mse: 406.1021 - val_loss: 407.7557 - val_mae: 19.8639 - val_mse: 407.7557\n",
      "Epoch 6/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 376.3737 - mae: 18.9950 - mse: 376.3737 - val_loss: 381.6593 - val_mae: 19.1978 - val_mse: 381.6593\n",
      "Epoch 7/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 346.5205 - mae: 18.2061 - mse: 346.5205 - val_loss: 354.3599 - val_mae: 18.4804 - val_mse: 354.3599\n",
      "Epoch 8/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 320.5755 - mae: 17.4918 - mse: 320.5755 - val_loss: 328.1643 - val_mae: 17.7687 - val_mse: 328.1643\n",
      "Epoch 9/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 296.0773 - mae: 16.7787 - mse: 296.0773 - val_loss: 305.0453 - val_mae: 17.0967 - val_mse: 305.0453\n",
      "Epoch 10/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 272.4704 - mae: 16.0740 - mse: 272.4704 - val_loss: 281.6156 - val_mae: 16.4112 - val_mse: 281.6156\n",
      "Epoch 11/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 250.6308 - mae: 15.3872 - mse: 250.6308 - val_loss: 259.9113 - val_mae: 15.7412 - val_mse: 259.9113\n",
      "Epoch 12/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 231.1865 - mae: 14.7453 - mse: 231.1865 - val_loss: 240.9937 - val_mae: 15.1156 - val_mse: 240.9937\n",
      "Epoch 13/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 212.4107 - mae: 14.0906 - mse: 212.4107 - val_loss: 220.9851 - val_mae: 14.4552 - val_mse: 220.9851\n",
      "Epoch 14/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 194.5405 - mae: 13.4493 - mse: 194.5405 - val_loss: 204.7625 - val_mae: 13.8689 - val_mse: 204.7625\n",
      "Epoch 15/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 178.0640 - mae: 12.8255 - mse: 178.0640 - val_loss: 188.1510 - val_mae: 13.2610 - val_mse: 188.1510\n",
      "Epoch 16/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 163.7031 - mae: 12.2492 - mse: 163.7031 - val_loss: 173.8330 - val_mae: 12.6934 - val_mse: 173.8330\n",
      "Epoch 17/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 149.2130 - mae: 11.6549 - mse: 149.2130 - val_loss: 158.1731 - val_mae: 12.0920 - val_mse: 158.1731\n",
      "Epoch 18/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 136.4448 - mae: 11.1106 - mse: 136.4448 - val_loss: 145.0962 - val_mae: 11.5403 - val_mse: 145.0962\n",
      "Epoch 19/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 124.6628 - mae: 10.5703 - mse: 124.6628 - val_loss: 132.8482 - val_mae: 11.0016 - val_mse: 132.8482\n",
      "Epoch 20/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 113.5448 - mae: 10.0424 - mse: 113.5448 - val_loss: 121.6739 - val_mae: 10.4833 - val_mse: 121.6739\n",
      "Epoch 21/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 103.6329 - mae: 9.5343 - mse: 103.6329 - val_loss: 112.0829 - val_mae: 10.0000 - val_mse: 112.0829\n",
      "Epoch 22/200\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 97.8023 - mae: 9.4119 - mse: 97.8023\n",
      "Reached MAE < 10.0, so cancelling training!\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 94.5927 - mae: 9.0518 - mse: 94.5927 - val_loss: 102.6473 - val_mae: 9.5185 - val_mse: 102.6473\n"
     ]
    }
   ],
   "source": [
    "my_es_callback = MyEarlyStopping()\n",
    "\n",
    "model = get_model()\n",
    "history = model.fit(train_features, train_labels, epochs=200, validation_split = 0.2, callbacks=[my_es_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
