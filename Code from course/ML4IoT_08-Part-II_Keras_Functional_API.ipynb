{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Functional API\n",
    "\n",
    "## Classify images with a toy ResNet\n",
    "\n",
    "In this notebook, we will train a reduced version of the popular ResNet neural network, which uses residual connections, to classify images from the CIFAR10 dataset. The notebook is based on the public tutorial found here: https://www.tensorflow.org/guide/keras/functional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-22 12:54:47.746950: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-22 12:54:47.746975: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download the CIFAR10 Dataset\n",
    "\n",
    "As in the previous notebook, will use the `keras.datasets` package to download the dataset. We will also do some (manual) normalization of the data. Notice that it's actually better to use a `Normalization()` Keras layer, so that the input pipeline remains part of your model (more portable). But this will do for a simple example.\n",
    "\n",
    "The `to_categorical` method applies one-hot encoding to the integer class labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train_int), (x_test, y_test_int) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "y_train = keras.utils.to_categorical(y_train_int, 10)\n",
    "y_test = keras.utils.to_categorical(y_test_int, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "CIFAR10 contains 32x32 color images, so the shape of `x_train` and `x_test` is $(n_{images}, 32, 32, 3)$, whereas the shape of `y_train` and `y_test` is $(n_{images}, 10)$ because of the one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define the class names (they are not defined in the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\n",
    "    'airplane',\n",
    "    'automobile',\n",
    "    'bird',\n",
    "    'cat',\n",
    "    'deer',\n",
    "    'dog',\n",
    "    'frog',\n",
    "    'horse',\n",
    "    'ship',\n",
    "    'truck',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let us plot some images as an example. Wow, the resolution is quite low...!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 1\n",
    "plt.figure()\n",
    "plt.imshow(x_train[idx])\n",
    "plt.xlabel(class_names[y_train_int[idx][0]])\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let us build a small ResNet-like CNN model with the Functional API\n",
    "\n",
    "In doing this, we are also seing some new important layers, such as `Input`, `Conv2D`, `MaxPooling2D`, `Dropout`, `add` (element-wise addition), etc. \n",
    "\n",
    "- The `Conv2D` layer takes as mandatory parameters the number of output channels and the kernel size. Optional parameters include the stride, the activation function, the padding type, etc.\n",
    "- The `MaxPooling2D` layer takes as mandatory parameter the pool size. Optional parameters include the stride (defaults to `pool_size`, the padding type, etc.\n",
    "- The `Dropout` layer takes as mandatory parameter the dropout rate.\n",
    "- The `add` layer does not have parameters.\n",
    "\n",
    "\n",
    "Notice that since the `add` layers take two inputs from different parts of the network, this model could not be defined using the Sequential API.\n",
    "\n",
    "Notice the final `keras.Model` call which specifies the inputs and outputs of our model's graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-22 12:54:51.541150: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-12-22 12:54:51.541182: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-12-22 12:54:51.541203: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (matteo-Inspiron-7591-2n1): /proc/driver/nvidia/version does not exist\n",
      "2021-12-22 12:54:51.541396: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(32, 32, 3), name=\"img\")\n",
    "x = keras.layers.Conv2D(32, 3, activation=\"relu\")(inputs)\n",
    "x = keras.layers.Conv2D(64, 3, activation=\"relu\")(x)\n",
    "block_1_output = keras.layers.MaxPooling2D(3)(x)\n",
    "\n",
    "x = keras.layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(block_1_output)\n",
    "x = keras.layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "block_2_output = keras.layers.add([x, block_1_output])\n",
    "\n",
    "x = keras.layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(block_2_output)\n",
    "x = keras.layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "block_3_output = keras.layers.add([x, block_2_output])\n",
    "\n",
    "x = keras.layers.Conv2D(64, 3, activation=\"relu\")(block_3_output)\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = keras.layers.Dense(256, activation=\"relu\")(x)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "outputs = keras.layers.Dense(10)(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs, name=\"toy_resnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us show a summary of the model and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"toy_resnet\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " img (InputLayer)               [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 30, 30, 32)   896         ['img[0][0]']                    \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 28, 28, 64)   18496       ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 9, 9, 64)     0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 9, 9, 64)     36928       ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 9, 9, 64)     36928       ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 9, 9, 64)     0           ['conv2d_3[0][0]',               \n",
      "                                                                  'max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 9, 9, 64)     36928       ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 9, 9, 64)     36928       ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 9, 9, 64)     0           ['conv2d_5[0][0]',               \n",
      "                                                                  'add[0][0]']                    \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 7, 7, 64)     36928       ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 64)          0           ['conv2d_6[0][0]']               \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 256)          16640       ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 256)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 10)           2570        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 223,242\n",
      "Trainable params: 223,242\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "keras.utils.plot_model(model, 'toy_resnet.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compile the model\n",
    "\n",
    "Let us now compile the model, specifying optimizer, loss function and metrics.\n",
    "\n",
    "Notice that, differently from the previous notebook, we use `RMSprop` instead of `Adam` this time. We also use the `CategoricalCrossentropy` loss (not the \"sparse\" variant) since we have generated 1-hot labels, and we use the `from_logits` option because the last layer of the model does not include a softmax activation. This approach is normally more stable from a numerical point of view, so it's suggested.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train the model\n",
    "\n",
    "Notice the new option `validation_split=0.2`. More details on this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-22 12:54:52.323671: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 491520000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  3/625 [..............................] - ETA: 45s - loss: 2.3347 - accuracy: 0.1094"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-22 12:54:53.756036: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 24385536 exceeds 10% of free system memory.\n",
      "2021-12-22 12:54:53.756132: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 24385536 exceeds 10% of free system memory.\n",
      "2021-12-22 12:54:53.830640: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 24385536 exceeds 10% of free system memory.\n",
      "2021-12-22 12:54:53.830870: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 24385536 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 63s 100ms/step - loss: 1.8937 - accuracy: 0.2747 - val_loss: 1.6652 - val_accuracy: 0.3840\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 72s 115ms/step - loss: 1.4862 - accuracy: 0.4557 - val_loss: 1.3847 - val_accuracy: 0.4975\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 67s 108ms/step - loss: 1.2711 - accuracy: 0.5434 - val_loss: 1.0475 - val_accuracy: 0.6205\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 72s 115ms/step - loss: 1.0941 - accuracy: 0.6122 - val_loss: 0.9841 - val_accuracy: 0.6404\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 70s 112ms/step - loss: 0.9675 - accuracy: 0.6604 - val_loss: 0.9402 - val_accuracy: 0.6658\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 79s 126ms/step - loss: 0.8790 - accuracy: 0.6943 - val_loss: 0.8067 - val_accuracy: 0.7175\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 66s 106ms/step - loss: 0.8018 - accuracy: 0.7243 - val_loss: 0.7851 - val_accuracy: 0.7224\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 75s 120ms/step - loss: 0.7371 - accuracy: 0.7466 - val_loss: 0.7719 - val_accuracy: 0.7335\n",
      "Epoch 9/10\n",
      "269/625 [===========>..................] - ETA: 43s - loss: 0.6810 - accuracy: 0.7660"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=64, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate on unseen data\n",
    "\n",
    "Let us test the model on unseen data using the `evaluate()` method. The model is slightly over-fitting the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test,  y_test, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perform a prediction on a single image\n",
    "\n",
    "Let us run a prediction on one image and see the result. Notice that the bar chart is negative this time because we have logits, not probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "\n",
    "pred = model.predict(x_test[idx].reshape(1, 32, 32, 3))\n",
    "pred_lidx = tf.argmax(pred[0])\n",
    "pred_label = class_names[pred_lidx]\n",
    "\n",
    "print(\"Predicted label: {} ({})\".format(pred_lidx, pred_label))\n",
    "print(\"Real label: {} ({})\".format(y_test_int[idx][0], class_names[y_test_int[idx][0]]))\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(x_test[idx])\n",
    "plt.grid(False)\n",
    "plt.figure()\n",
    "plt.bar(range(len(class_names)), pred[0], tick_label=class_names)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
