{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Losses and Metrics in Keras\n",
    "\n",
    "Source of this notebook: https://www.tensorflow.org/tutorials/keras/regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download the Auto-MPG Dataset\n",
    "\n",
    "The goal of this task is to predict the fuel efficiency (Mileage per Gallon, MPG) of late-1970s and early 1980s cars. The input data for each car includes attributes like: cylinders, displacement, horsepower, weight, etc.\n",
    "\n",
    "This dataset is not included in the `keras.datasets` so we will import it from a CSV file using `pandas` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin']\n",
    "\n",
    "raw_dataset = pd.read_csv(url, names=column_names,\n",
    "                          na_values='?', comment='\\t',\n",
    "                          sep=' ', skipinitialspace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a copy (to keep the original untouched) and see the first rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = raw_dataset.copy()\n",
    "dataset.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some basic preprocessing: eliminate rows with missing values, and convert the numeric `Origin` column to the corresponding geographic area. Finally, convert this categorical column to a one-hot representation using the `get_dummies` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = dataset.dropna()\n",
    "dataset['Origin'] = dataset['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})\n",
    "dataset = pd.get_dummies(dataset, prefix='', prefix_sep='')\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train and test subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset.sample(frac=0.8, random_state=0)\n",
    "test_dataset = dataset.drop(train_dataset.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split features from labels (the `pop()` method removes a column and creates a copy at the same time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_dataset.copy()\n",
    "test_features = test_dataset.copy()\n",
    "\n",
    "train_labels = train_features.pop('MPG')\n",
    "test_labels = test_features.pop('MPG')\n",
    "\n",
    "train_features.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build a simple model\n",
    "\n",
    "We will approach the task with a simple linear model, built with the Sequential API. Before the single `Dense` layer, however, we will add a `Normalization()` layer, which normalizes each feature column to have 0 mean and unitary standard deviation.\n",
    "\n",
    "We declare the normalization layer before the `Sequential()` call because we need to `adapt()` it to the input features statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = preprocessing.Normalization()\n",
    "\n",
    "# convert the pandas df to a tensor-like numpy array\n",
    "normalizer.adapt(np.array(train_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we build a sequential model with a single linear layer. We can also use a multi-layer NN if we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    normalizer,\n",
    "    layers.Dense(units=1)\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "keras.utils.plot_model(model, 'mpg_regression.png', show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define a custom loss function\n",
    "\n",
    "Let us define a custom loss function that we want to use to train this model. Here, we just re-define the Mean Squared Error (MSE), which of course is already present in `keras.losses`. However, in general, this is useful when you need to use task-specific or complex losses that are not already available.\n",
    "\n",
    "Clearly, since the loss function has to be differentiated, we need to make sure that our custom definition only uses TensorFlow ops (or overloaded operators)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_mse(y, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_pred - y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can simply pass our custom `my_mse` loss to the compile function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.optimizers.Adam(learning_rate=0.1),\n",
    "    loss=my_mse, metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can train the model, normally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_features, train_labels,\n",
    "    epochs=40,\n",
    "    validation_split = 0.2,\n",
    "    validation_freq=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use the history data structure to plot learning curves\n",
    "\n",
    "Here we also show how to use the `history` data structure returned by `fit()` to plot the learning curves of our model ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "  plt.plot(history.history['loss'], label='loss')\n",
    "  plt.plot(history.history['val_loss'], label='val_loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Error [MPG]')\n",
    "  plt.legend()\n",
    "  plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate on test data\n",
    "\n",
    "As usual, after training we evaluate the model on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = model.evaluate(test_features, test_labels, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### See the effect of normalization\n",
    "\n",
    "Remember that we can call individual keras layers as functions. Let us try to see the effect of the `Normalization()` layer on one row of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't use scientific notation\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# note that we use 0:1 instead of 0 to make this a (1,9) tensor, and not a (9,) tensor\n",
    "# the latter would not be processed correctly  by the normalizer\n",
    "row = np.array(train_features)[0:1,:]\n",
    "print(row)\n",
    "\n",
    "norm_row = normalizer(row)\n",
    "print(norm_row.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### See the trained weights\n",
    "\n",
    "Another aspect that we still didn't explore is how to inspect the internal weights of trained Keras layers. In this example, we have a single `Dense()` layer, which contains a $(9, 1)$ weight matrix, since it takes 9 columns as inputs and produces a single estimate as output. Let us see it.\n",
    "\n",
    "The `.layers` field of our model contains a list of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each (non custom) layer has a `weights` field, which contains a list of (typically named) tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.layers[1].weights)\n",
    "print(model.layers[0].weights[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
